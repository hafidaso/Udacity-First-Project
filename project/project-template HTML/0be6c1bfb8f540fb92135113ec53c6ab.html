<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>178e4ac10f7a404b973450a3d3911183</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="predict-bike-sharing-demand-with-autogluon-template"
class="cell markdown">
<h1>Predict Bike Sharing Demand with AutoGluon Template</h1>
</section>
<section id="project-predict-bike-sharing-demand-with-autogluon"
class="cell markdown">
<h2>Project: Predict Bike Sharing Demand with AutoGluon</h2>
<p>This notebook is a template with each step that you need to complete
for the project.</p>
<p>Please fill in your code where there are explicit <code>?</code>
markers in the notebook. You are welcome to add more cells and code as
you see fit.</p>
<p>Once you have completed all the code implementations, please export
your notebook as a HTML file so the reviews can view your code. Make
sure you have all outputs correctly outputted.</p>
<p><code>File-&gt; Export Notebook As... -&gt; Export Notebook as HTML</code></p>
<p>There is a writeup to complete as well after all code implememtation
is done. Please answer all questions and attach the necessary tables and
charts. You can complete the writeup in either markdown or PDF.</p>
<p>Completing the code template and writeup template will cover all of
the rubric points for this project.</p>
<p>The rubric contains "Stand Out Suggestions" for enhancing the project
beyond the minimum requirements. The stand out suggestions are optional.
If you decide to pursue the "stand out suggestions", you can include the
code in this notebook and also discuss the results in the writeup
file.</p>
</section>
<section id="step-1-create-an-account-with-kaggle"
class="cell markdown">
<h2>Step 1: Create an account with Kaggle</h2>
</section>
<section id="create-kaggle-account-and-download-api-key"
class="cell markdown">
<h3>Create Kaggle Account and download API key</h3>
<p>Below is example of steps to get the API username and key. Each
student will have their own username and key.</p>
</section>
<div class="cell markdown">
<ol>
<li>Open account settings. <img
src="vertopal_0be6c1bfb8f540fb92135113ec53c6ab/kaggle1.png"
alt="kaggle1.png" /> <img
src="vertopal_0be6c1bfb8f540fb92135113ec53c6ab/kaggle2.png"
alt="kaggle2.png" /></li>
<li>Scroll down to API and click Create New API Token. <img
src="vertopal_0be6c1bfb8f540fb92135113ec53c6ab/kaggle3.png"
alt="kaggle3.png" /> <img
src="vertopal_0be6c1bfb8f540fb92135113ec53c6ab/kaggle4.png"
alt="kaggle4.png" /></li>
<li>Open up <code>kaggle.json</code> and use the username and key. <img
src="vertopal_0be6c1bfb8f540fb92135113ec53c6ab/kaggle5.png"
alt="kaggle5.png" /></li>
</ol>
</div>
<section
id="step-2-download-the-kaggle-dataset-using-the-kaggle-python-library"
class="cell markdown">
<h2>Step 2: Download the Kaggle dataset using the kaggle python
library</h2>
</section>
<section id="open-up-sagemaker-studio-and-use-starter-template"
class="cell markdown">
<h3>Open up Sagemaker Studio and use starter template</h3>
</section>
<div class="cell markdown">
<ol>
<li>Notebook should be using a <code>ml.t3.medium</code> instance (2
vCPU + 4 GiB)</li>
<li>Notebook should be using kernal:
<code>Python 3 (MXNet 1.8 Python 3.7 CPU Optimized)</code></li>
</ol>
</div>
<section id="install-packages" class="cell markdown">
<h3>Install packages</h3>
</section>
<div class="cell code" data-execution_count="7">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">-</span>U pip</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">-</span>U setuptools wheel</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install <span class="op">-</span>U <span class="st">&quot;mxnet&lt;2.0.0&quot;</span> bokeh<span class="op">==</span><span class="fl">2.0.1</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install autogluon <span class="op">--</span>no<span class="op">-</span>cache<span class="op">-</span><span class="bu">dir</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Without --no-cache-dir, smaller aws instances may have trouble installing</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>Requirement already satisfied: pip in /opt/homebrew/lib/python3.11/site-packages (24.0)
Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.11/site-packages (69.5.1)
Requirement already satisfied: wheel in /opt/homebrew/lib/python3.11/site-packages (0.43.0)
Requirement already satisfied: mxnet&lt;2.0.0 in /opt/homebrew/lib/python3.11/site-packages (1.6.0)
Requirement already satisfied: bokeh==2.0.1 in /opt/homebrew/lib/python3.11/site-packages (2.0.1)
Requirement already satisfied: PyYAML&gt;=3.10 in /opt/homebrew/lib/python3.11/site-packages (from bokeh==2.0.1) (6.0.1)
Requirement already satisfied: python-dateutil&gt;=2.1 in /Users/ymadigital/Library/Python/3.11/lib/python/site-packages (from bokeh==2.0.1) (2.8.2)
Requirement already satisfied: Jinja2&gt;=2.7 in /opt/homebrew/lib/python3.11/site-packages (from bokeh==2.0.1) (3.1.2)
Requirement already satisfied: numpy&gt;=1.11.3 in /opt/homebrew/lib/python3.11/site-packages (from bokeh==2.0.1) (1.25.2)
Requirement already satisfied: pillow&gt;=4.0 in /opt/homebrew/lib/python3.11/site-packages (from bokeh==2.0.1) (10.1.0)
Requirement already satisfied: packaging&gt;=16.8 in /Users/ymadigital/Library/Python/3.11/lib/python/site-packages (from bokeh==2.0.1) (23.1)
Requirement already satisfied: tornado&gt;=5 in /Users/ymadigital/Library/Python/3.11/lib/python/site-packages (from bokeh==2.0.1) (6.3.3)
Requirement already satisfied: typing-extensions&gt;=3.7.4 in /opt/homebrew/lib/python3.11/site-packages (from bokeh==2.0.1) (4.8.0)
Requirement already satisfied: requests&lt;3,&gt;=2.20.0 in /opt/homebrew/lib/python3.11/site-packages (from mxnet&lt;2.0.0) (2.31.0)
Requirement already satisfied: graphviz&lt;0.9.0,&gt;=0.8.1 in /opt/homebrew/lib/python3.11/site-packages (from mxnet&lt;2.0.0) (0.8.4)
Requirement already satisfied: MarkupSafe&gt;=2.0 in /opt/homebrew/lib/python3.11/site-packages (from Jinja2&gt;=2.7-&gt;bokeh==2.0.1) (2.1.3)
Requirement already satisfied: six&gt;=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil&gt;=2.1-&gt;bokeh==2.0.1) (1.16.0)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/homebrew/lib/python3.11/site-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet&lt;2.0.0) (3.3.2)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet&lt;2.0.0) (3.4)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet&lt;2.0.0) (1.26.18)
Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests&lt;3,&gt;=2.20.0-&gt;mxnet&lt;2.0.0) (2023.11.17)
Collecting autogluon
  Downloading autogluon-1.1.0-py3-none-any.whl.metadata (11 kB)
Collecting autogluon.core==1.1.0 (from autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading autogluon.core-1.1.0-py3-none-any.whl.metadata (11 kB)
Collecting autogluon.features==1.1.0 (from autogluon)
  Downloading autogluon.features-1.1.0-py3-none-any.whl.metadata (11 kB)
Collecting autogluon.tabular==1.1.0 (from autogluon.tabular[all]==1.1.0-&gt;autogluon)
  Downloading autogluon.tabular-1.1.0-py3-none-any.whl.metadata (13 kB)
Collecting autogluon.multimodal==1.1.0 (from autogluon)
  Downloading autogluon.multimodal-1.1.0-py3-none-any.whl.metadata (12 kB)
Collecting autogluon.timeseries==1.1.0 (from autogluon.timeseries[all]==1.1.0-&gt;autogluon)
  Downloading autogluon.timeseries-1.1.0-py3-none-any.whl.metadata (12 kB)
Requirement already satisfied: numpy&lt;1.29,&gt;=1.21 in /opt/homebrew/lib/python3.11/site-packages (from autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (1.25.2)
Requirement already satisfied: scipy&lt;1.13,&gt;=1.5.4 in /opt/homebrew/lib/python3.11/site-packages (from autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (1.11.4)
Requirement already satisfied: scikit-learn&lt;1.4.1,&gt;=1.3.0 in /opt/homebrew/lib/python3.11/site-packages (from autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (1.3.2)
Collecting networkx&lt;4,&gt;=3.0 (from autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)
Collecting pandas&lt;2.3.0,&gt;=2.0.0 (from autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading pandas-2.2.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (19 kB)
Requirement already satisfied: tqdm&lt;5,&gt;=4.38 in /opt/homebrew/lib/python3.11/site-packages (from autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (4.64.1)
Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (2.31.0)
Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.11/site-packages (from autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (3.8.2)
Collecting boto3&lt;2,&gt;=1.10 (from autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading boto3-1.34.93-py3-none-any.whl.metadata (6.6 kB)
Collecting autogluon.common==1.1.0 (from autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading autogluon.common-1.1.0-py3-none-any.whl.metadata (11 kB)
Collecting ray&lt;2.11,&gt;=2.10.0 (from ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading ray-2.10.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (13 kB)
Collecting hyperopt&lt;0.2.8,&gt;=0.2.7 (from autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading hyperopt-0.2.7-py2.py3-none-any.whl.metadata (1.7 kB)
Requirement already satisfied: Pillow&lt;11,&gt;=10.0.1 in /opt/homebrew/lib/python3.11/site-packages (from autogluon.multimodal==1.1.0-&gt;autogluon) (10.1.0)
Requirement already satisfied: torch&lt;2.2,&gt;=2.1 in /opt/homebrew/lib/python3.11/site-packages (from autogluon.multimodal==1.1.0-&gt;autogluon) (2.1.2)
Collecting lightning&lt;2.2,&gt;=2.1 (from autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading lightning-2.1.4-py3-none-any.whl.metadata (57 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.2/57.2 kB 723.9 kB/s eta 0:00:00a 0:00:01
ers&lt;4.39.0,&gt;=4.38.0 (from transformers[sentencepiece]&lt;4.39.0,&gt;=4.38.0-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 130.7/130.7 kB 303.0 kB/s eta 0:00:00a 0:00:01
 autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)
Requirement already satisfied: jsonschema&lt;4.22,&gt;=4.18 in /opt/homebrew/lib/python3.11/site-packages (from autogluon.multimodal==1.1.0-&gt;autogluon) (4.20.0)
Collecting seqeval&lt;1.3.0,&gt;=1.2.2 (from autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading seqeval-1.2.2.tar.gz (43 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 3.7 MB/s eta 0:00:00
etadata (setup.py) ...  autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)
Collecting timm&lt;0.10.0,&gt;=0.9.5 (from autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading timm-0.9.16-py3-none-any.whl.metadata (38 kB)
Requirement already satisfied: torchvision&lt;0.17.0,&gt;=0.16.0 in /opt/homebrew/lib/python3.11/site-packages (from autogluon.multimodal==1.1.0-&gt;autogluon) (0.16.2)
Collecting scikit-image&lt;0.21.0,&gt;=0.19.1 (from autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading scikit_image-0.20.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (16 kB)
Collecting text-unidecode&lt;1.4,&gt;=1.3 (from autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)
Collecting torchmetrics&lt;1.3.0,&gt;=1.2.0 (from autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading torchmetrics-1.2.1-py3-none-any.whl.metadata (20 kB)
Collecting nptyping&lt;2.5.0,&gt;=1.4.4 (from autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading nptyping-2.4.1-py3-none-any.whl.metadata (7.7 kB)
Collecting omegaconf&lt;2.3.0,&gt;=2.1.1 (from autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading omegaconf-2.2.3-py3-none-any.whl.metadata (3.9 kB)
Collecting pytorch-metric-learning&lt;2.4,&gt;=1.3.0 (from autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading pytorch_metric_learning-2.3.0-py3-none-any.whl.metadata (17 kB)
Collecting nlpaug&lt;1.2.0,&gt;=1.1.10 (from autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)
Requirement already satisfied: nltk&lt;4.0.0,&gt;=3.4.5 in /opt/homebrew/lib/python3.11/site-packages (from autogluon.multimodal==1.1.0-&gt;autogluon) (3.8.1)
Collecting openmim&lt;0.4.0,&gt;=0.3.7 (from autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading openmim-0.3.9-py2.py3-none-any.whl.metadata (16 kB)
Requirement already satisfied: defusedxml&lt;0.7.2,&gt;=0.7.1 in /opt/homebrew/lib/python3.11/site-packages (from autogluon.multimodal==1.1.0-&gt;autogluon) (0.7.1)
Requirement already satisfied: jinja2&lt;3.2,&gt;=3.0.3 in /opt/homebrew/lib/python3.11/site-packages (from autogluon.multimodal==1.1.0-&gt;autogluon) (3.1.2)
Collecting tensorboard&lt;3,&gt;=2.9 (from autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)
Collecting pytesseract&lt;0.3.11,&gt;=0.3.9 (from autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading pytesseract-0.3.10-py3-none-any.whl.metadata (11 kB)
Collecting nvidia-ml-py3==7.352.0 (from autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)
  Preparing metadata (setup.py) ... age&lt;1.19,&gt;=1.17.0 (from autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)
Requirement already satisfied: xgboost&lt;2.1,&gt;=1.6 in /opt/homebrew/lib/python3.11/site-packages (from autogluon.tabular[all]==1.1.0-&gt;autogluon) (2.0.3)
Collecting fastai&lt;2.8,&gt;=2.3.1 (from autogluon.tabular[all]==1.1.0-&gt;autogluon)
  Downloading fastai-2.7.15-py3-none-any.whl.metadata (9.1 kB)
Collecting lightgbm&lt;4.4,&gt;=3.3 (from autogluon.tabular[all]==1.1.0-&gt;autogluon)
  Downloading lightgbm-4.3.0.tar.gz (1.7 MB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 2.1 MB/s eta 0:00:00a 0:00:01
ents to build wheel ... etadata (pyproject.toml) ... ent already satisfied: joblib&lt;2,&gt;=1.1 in /opt/homebrew/lib/python3.11/site-packages (from autogluon.timeseries==1.1.0-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon) (1.3.2)
Collecting pytorch-lightning&lt;2.2,&gt;=2.1 (from autogluon.timeseries==1.1.0-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon)
  Downloading pytorch_lightning-2.1.4-py3-none-any.whl.metadata (21 kB)
Collecting gluonts&lt;0.14.4,&gt;=0.14.0 (from autogluon.timeseries==1.1.0-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon)
  Downloading gluonts-0.14.3-py3-none-any.whl.metadata (9.5 kB)
Collecting statsforecast&lt;1.5,&gt;=1.4.0 (from autogluon.timeseries==1.1.0-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon)
  Downloading statsforecast-1.4.0-py3-none-any.whl.metadata (19 kB)
Collecting mlforecast&lt;0.10.1,&gt;=0.10.0 (from autogluon.timeseries==1.1.0-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon)
  Downloading mlforecast-0.10.0-py3-none-any.whl.metadata (11 kB)
Collecting utilsforecast&lt;0.0.11,&gt;=0.0.10 (from autogluon.timeseries==1.1.0-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon)
  Downloading utilsforecast-0.0.10-py3-none-any.whl.metadata (7.0 kB)
Collecting orjson~=3.9 (from autogluon.timeseries==1.1.0-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon)
  Downloading orjson-3.10.1-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (49 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.7/49.7 kB 2.8 MB/s eta 0:00:00
um&lt;1.19,&gt;=1.17 (from optimum[onnxruntime]&lt;1.19,&gt;=1.17; extra == &quot;all&quot;-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon)
  Downloading optimum-1.18.1-py3-none-any.whl.metadata (18 kB)
Requirement already satisfied: psutil&lt;6,&gt;=5.7.3 in /Users/ymadigital/Library/Python/3.11/lib/python/site-packages (from autogluon.common==1.1.0-&gt;autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (5.9.5)
Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.11/site-packages (from autogluon.common==1.1.0-&gt;autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (69.5.1)
Requirement already satisfied: packaging&gt;=20.0 in /Users/ymadigital/Library/Python/3.11/lib/python/site-packages (from accelerate&lt;0.22.0,&gt;=0.21.0-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (23.1)
Requirement already satisfied: pyyaml in /opt/homebrew/lib/python3.11/site-packages (from accelerate&lt;0.22.0,&gt;=0.21.0-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (6.0.1)
Collecting botocore&lt;1.35.0,&gt;=1.34.93 (from boto3&lt;2,&gt;=1.10-&gt;autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading botocore-1.34.93-py3-none-any.whl.metadata (5.7 kB)
Requirement already satisfied: jmespath&lt;2.0.0,&gt;=0.7.1 in /opt/homebrew/lib/python3.11/site-packages (from boto3&lt;2,&gt;=1.10-&gt;autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (1.0.1)
Collecting s3transfer&lt;0.11.0,&gt;=0.10.0 (from boto3&lt;2,&gt;=1.10-&gt;autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading s3transfer-0.10.1-py3-none-any.whl.metadata (1.7 kB)
Requirement already satisfied: datasets&gt;=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from evaluate&lt;0.5.0,&gt;=0.4.0-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (2.15.0)
Requirement already satisfied: dill in /opt/homebrew/lib/python3.11/site-packages (from evaluate&lt;0.5.0,&gt;=0.4.0-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (0.3.7)
Requirement already satisfied: xxhash in /opt/homebrew/lib/python3.11/site-packages (from evaluate&lt;0.5.0,&gt;=0.4.0-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (3.4.1)
Requirement already satisfied: multiprocess in /opt/homebrew/lib/python3.11/site-packages (from evaluate&lt;0.5.0,&gt;=0.4.0-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (0.70.15)
Requirement already satisfied: fsspec&gt;=2021.05.0 in /opt/homebrew/lib/python3.11/site-packages (from fsspec[http]&gt;=2021.05.0-&gt;evaluate&lt;0.5.0,&gt;=0.4.0-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (2023.10.0)
Requirement already satisfied: huggingface-hub&gt;=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from evaluate&lt;0.5.0,&gt;=0.4.0-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (0.19.4)
Collecting responses&lt;0.19 (from evaluate&lt;0.5.0,&gt;=0.4.0-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)
Requirement already satisfied: pip in /opt/homebrew/lib/python3.11/site-packages (from fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==1.1.0-&gt;autogluon) (24.0)
Collecting fastdownload&lt;2,&gt;=0.0.5 (from fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==1.1.0-&gt;autogluon)
  Downloading fastdownload-0.0.7-py3-none-any.whl.metadata (5.5 kB)
Collecting fastcore&lt;1.6,&gt;=1.5.29 (from fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==1.1.0-&gt;autogluon)
  Downloading fastcore-1.5.32-py3-none-any.whl.metadata (3.5 kB)
Collecting fastprogress&gt;=0.2.4 (from fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==1.1.0-&gt;autogluon)
  Downloading fastprogress-1.0.3-py3-none-any.whl.metadata (5.6 kB)
Requirement already satisfied: spacy&lt;4 in /opt/homebrew/lib/python3.11/site-packages (from fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==1.1.0-&gt;autogluon) (3.7.2)
Requirement already satisfied: pydantic&lt;3,&gt;=1.7 in /opt/homebrew/lib/python3.11/site-packages (from gluonts&lt;0.14.4,&gt;=0.14.0-&gt;autogluon.timeseries==1.1.0-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon) (2.5.2)
Collecting toolz~=0.10 (from gluonts&lt;0.14.4,&gt;=0.14.0-&gt;autogluon.timeseries==1.1.0-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon)
  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)
Requirement already satisfied: typing-extensions~=4.0 in /opt/homebrew/lib/python3.11/site-packages (from gluonts&lt;0.14.4,&gt;=0.14.0-&gt;autogluon.timeseries==1.1.0-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon) (4.8.0)
Requirement already satisfied: six in /opt/homebrew/lib/python3.11/site-packages (from hyperopt&lt;0.2.8,&gt;=0.2.7-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (1.16.0)
Requirement already satisfied: future in /opt/homebrew/lib/python3.11/site-packages (from hyperopt&lt;0.2.8,&gt;=0.2.7-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (0.18.3)
Collecting cloudpickle (from hyperopt&lt;0.2.8,&gt;=0.2.7-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)
Collecting py4j (from hyperopt&lt;0.2.8,&gt;=0.2.7-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)
Requirement already satisfied: MarkupSafe&gt;=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2&lt;3.2,&gt;=3.0.3-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (2.1.3)
Requirement already satisfied: attrs&gt;=22.2.0 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema&lt;4.22,&gt;=4.18-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (23.1.0)
Requirement already satisfied: jsonschema-specifications&gt;=2023.03.6 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema&lt;4.22,&gt;=4.18-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (2023.12.1)
Requirement already satisfied: referencing&gt;=0.28.4 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema&lt;4.22,&gt;=4.18-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (0.32.0)
Requirement already satisfied: rpds-py&gt;=0.7.1 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema&lt;4.22,&gt;=4.18-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (0.15.2)
Collecting lightning-utilities&lt;2.0,&gt;=0.8.0 (from lightning&lt;2.2,&gt;=2.1-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading lightning_utilities-0.11.2-py3-none-any.whl.metadata (4.7 kB)
Requirement already satisfied: numba in /opt/homebrew/lib/python3.11/site-packages (from mlforecast&lt;0.10.1,&gt;=0.10.0-&gt;autogluon.timeseries==1.1.0-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon) (0.58.1)
Collecting window-ops (from mlforecast&lt;0.10.1,&gt;=0.10.0-&gt;autogluon.timeseries==1.1.0-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon)
  Downloading window_ops-0.0.15-py3-none-any.whl.metadata (6.8 kB)
Collecting gdown&gt;=4.0.0 (from nlpaug&lt;1.2.0,&gt;=1.1.10-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)
Requirement already satisfied: click in /opt/homebrew/lib/python3.11/site-packages (from nltk&lt;4.0.0,&gt;=3.4.5-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (8.1.7)
Requirement already satisfied: regex&gt;=2021.8.3 in /opt/homebrew/lib/python3.11/site-packages (from nltk&lt;4.0.0,&gt;=3.4.5-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (2023.10.3)
Collecting antlr4-python3-runtime==4.9.* (from omegaconf&lt;2.3.0,&gt;=2.1.1-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.0/117.0 kB 2.5 MB/s eta 0:00:00a 0:00:01
etadata (setup.py) ... ent already satisfied: colorama in /opt/homebrew/lib/python3.11/site-packages (from openmim&lt;0.4.0,&gt;=0.3.7-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (0.4.6)
Collecting model-index (from openmim&lt;0.4.0,&gt;=0.3.7-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading model_index-0.1.11-py3-none-any.whl.metadata (3.9 kB)
Collecting opendatalab (from openmim&lt;0.4.0,&gt;=0.3.7-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading opendatalab-0.0.10-py3-none-any.whl.metadata (6.4 kB)
Collecting rich (from openmim&lt;0.4.0,&gt;=0.3.7-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)
Requirement already satisfied: tabulate in /opt/homebrew/lib/python3.11/site-packages (from openmim&lt;0.4.0,&gt;=0.3.7-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (0.9.0)
Collecting coloredlogs (from optimum&lt;1.19,&gt;=1.17-&gt;optimum[onnxruntime]&lt;1.19,&gt;=1.17; extra == &quot;all&quot;-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon)
  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)
Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from optimum&lt;1.19,&gt;=1.17-&gt;optimum[onnxruntime]&lt;1.19,&gt;=1.17; extra == &quot;all&quot;-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon) (1.12)
Collecting onnx (from optimum[onnxruntime]&lt;1.19,&gt;=1.17; extra == &quot;all&quot;-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon)
  Downloading onnx-1.16.0-cp311-cp311-macosx_10_15_universal2.whl.metadata (16 kB)
Collecting onnxruntime&gt;=1.11.0 (from optimum[onnxruntime]&lt;1.19,&gt;=1.17; extra == &quot;all&quot;-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon)
  Downloading onnxruntime-1.17.3-cp311-cp311-macosx_11_0_universal2.whl.metadata (4.4 kB)
Requirement already satisfied: protobuf&gt;=3.20.1 in /opt/homebrew/lib/python3.11/site-packages (from optimum[onnxruntime]&lt;1.19,&gt;=1.17; extra == &quot;all&quot;-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon) (4.23.4)
Requirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/ymadigital/Library/Python/3.11/lib/python/site-packages (from pandas&lt;2.3.0,&gt;=2.0.0-&gt;autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (2.8.2)
Requirement already satisfied: pytz&gt;=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas&lt;2.3.0,&gt;=2.0.0-&gt;autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (2023.3.post1)
Requirement already satisfied: tzdata&gt;=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas&lt;2.3.0,&gt;=2.0.0-&gt;autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (2023.3)
Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from ray&lt;2.11,&gt;=2.10.0-&gt;ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (3.13.1)
Requirement already satisfied: msgpack&lt;2.0.0,&gt;=1.0.0 in /opt/homebrew/lib/python3.11/site-packages (from ray&lt;2.11,&gt;=2.10.0-&gt;ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (1.0.7)
Requirement already satisfied: aiosignal in /opt/homebrew/lib/python3.11/site-packages (from ray&lt;2.11,&gt;=2.10.0-&gt;ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (1.3.1)
Requirement already satisfied: frozenlist in /opt/homebrew/lib/python3.11/site-packages (from ray&lt;2.11,&gt;=2.10.0-&gt;ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (1.4.0)
Requirement already satisfied: aiohttp&gt;=3.7 in /opt/homebrew/lib/python3.11/site-packages (from ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (3.9.0)
Collecting aiohttp-cors (from ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading aiohttp_cors-0.7.0-py3-none-any.whl.metadata (20 kB)
Collecting colorful (from ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)
Collecting py-spy&gt;=0.2.0 (from ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading py_spy-0.3.14-py2.py3-none-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl.metadata (16 kB)
Collecting opencensus (from ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)
Requirement already satisfied: prometheus-client&gt;=0.7.1 in /opt/homebrew/lib/python3.11/site-packages (from ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (0.19.0)
Requirement already satisfied: smart-open in /opt/homebrew/lib/python3.11/site-packages (from ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (6.4.0)
Requirement already satisfied: virtualenv!=20.21.1,&gt;=20.0.24 in /opt/homebrew/lib/python3.11/site-packages (from ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (20.24.7)
Requirement already satisfied: grpcio&gt;=1.42.0 in /opt/homebrew/lib/python3.11/site-packages (from ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (1.59.3)
Collecting tensorboardX&gt;=1.9 (from ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)
Requirement already satisfied: pyarrow&gt;=6.0.1 in /opt/homebrew/lib/python3.11/site-packages (from ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (14.0.1)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/homebrew/lib/python3.11/site-packages (from requests-&gt;autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (3.3.2)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests-&gt;autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (3.4)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests-&gt;autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (1.26.18)
Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests-&gt;autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (2023.11.17)
Collecting imageio&gt;=2.4.1 (from scikit-image&lt;0.21.0,&gt;=0.19.1-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading imageio-2.34.1-py3-none-any.whl.metadata (4.9 kB)
Collecting tifffile&gt;=2019.7.26 (from scikit-image&lt;0.21.0,&gt;=0.19.1-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading tifffile-2024.4.24-py3-none-any.whl.metadata (31 kB)
Collecting PyWavelets&gt;=1.1.1 (from scikit-image&lt;0.21.0,&gt;=0.19.1-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading pywavelets-1.6.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)
Requirement already satisfied: lazy_loader&gt;=0.1 in /opt/homebrew/lib/python3.11/site-packages (from scikit-image&lt;0.21.0,&gt;=0.19.1-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (0.3)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn&lt;1.4.1,&gt;=1.3.0-&gt;autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (3.2.0)
Requirement already satisfied: plotly in /opt/homebrew/lib/python3.11/site-packages (from statsforecast&lt;1.5,&gt;=1.4.0-&gt;autogluon.timeseries==1.1.0-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon) (5.19.0)
Requirement already satisfied: statsmodels&gt;=0.13.2 in /opt/homebrew/lib/python3.11/site-packages (from statsforecast&lt;1.5,&gt;=1.4.0-&gt;autogluon.timeseries==1.1.0-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon) (0.14.1)
Requirement already satisfied: absl-py&gt;=0.4 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard&lt;3,&gt;=2.9-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (2.0.0)
Requirement already satisfied: markdown&gt;=2.6.8 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard&lt;3,&gt;=2.9-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (3.5.1)
Requirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard&lt;3,&gt;=2.9-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (0.7.2)
Requirement already satisfied: werkzeug&gt;=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard&lt;3,&gt;=2.9-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (3.0.1)
Requirement already satisfied: safetensors in /opt/homebrew/lib/python3.11/site-packages (from timm&lt;0.10.0,&gt;=0.9.5-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (0.4.0)
Requirement already satisfied: tokenizers&lt;0.19,&gt;=0.14 in /opt/homebrew/lib/python3.11/site-packages (from transformers&lt;4.39.0,&gt;=4.38.0-&gt;transformers[sentencepiece]&lt;4.39.0,&gt;=4.38.0-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (0.15.0)
Collecting safetensors (from timm&lt;0.10.0,&gt;=0.9.5-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading safetensors-0.4.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)
Requirement already satisfied: sentencepiece!=0.1.92,&gt;=0.1.91 in /opt/homebrew/lib/python3.11/site-packages (from transformers[sentencepiece]&lt;4.39.0,&gt;=4.38.0-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (0.1.99)
Requirement already satisfied: contourpy&gt;=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib-&gt;autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (1.2.0)
Requirement already satisfied: cycler&gt;=0.10 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib-&gt;autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (0.12.1)
Requirement already satisfied: fonttools&gt;=4.22.0 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib-&gt;autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (4.44.0)
Requirement already satisfied: kiwisolver&gt;=1.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib-&gt;autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (1.4.5)
Requirement already satisfied: pyparsing&gt;=2.3.1 in /opt/homebrew/lib/python3.11/site-packages (from matplotlib-&gt;autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (3.1.1)
Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp&gt;=3.7-&gt;ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (6.0.4)
Requirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp&gt;=3.7-&gt;ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (1.9.3)
Requirement already satisfied: pyarrow-hotfix in /opt/homebrew/lib/python3.11/site-packages (from datasets&gt;=2.0.0-&gt;evaluate&lt;0.5.0,&gt;=0.4.0-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (0.6)
Requirement already satisfied: beautifulsoup4 in /opt/homebrew/lib/python3.11/site-packages (from gdown&gt;=4.0.0-&gt;nlpaug&lt;1.2.0,&gt;=1.1.10-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (4.12.2)
Requirement already satisfied: llvmlite&lt;0.42,&gt;=0.41.0dev0 in /opt/homebrew/lib/python3.11/site-packages (from numba-&gt;mlforecast&lt;0.10.1,&gt;=0.10.0-&gt;autogluon.timeseries==1.1.0-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon) (0.41.1)
Collecting flatbuffers (from onnxruntime&gt;=1.11.0-&gt;optimum[onnxruntime]&lt;1.19,&gt;=1.17; extra == &quot;all&quot;-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon)
  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)
Requirement already satisfied: annotated-types&gt;=0.4.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic&lt;3,&gt;=1.7-&gt;gluonts&lt;0.14.4,&gt;=0.14.0-&gt;autogluon.timeseries==1.1.0-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon) (0.6.0)
Requirement already satisfied: pydantic-core==2.14.5 in /opt/homebrew/lib/python3.11/site-packages (from pydantic&lt;3,&gt;=1.7-&gt;gluonts&lt;0.14.4,&gt;=0.14.0-&gt;autogluon.timeseries==1.1.0-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon) (2.14.5)
Requirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.11 in /opt/homebrew/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==1.1.0-&gt;autogluon) (3.0.12)
Requirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==1.1.0-&gt;autogluon) (1.0.5)
Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==1.1.0-&gt;autogluon) (1.0.10)
Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /opt/homebrew/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==1.1.0-&gt;autogluon) (2.0.8)
Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /opt/homebrew/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==1.1.0-&gt;autogluon) (3.0.9)
Requirement already satisfied: thinc&lt;8.3.0,&gt;=8.1.8 in /opt/homebrew/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==1.1.0-&gt;autogluon) (8.2.1)
Requirement already satisfied: wasabi&lt;1.2.0,&gt;=0.9.1 in /opt/homebrew/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==1.1.0-&gt;autogluon) (1.1.2)
Requirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.3 in /opt/homebrew/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==1.1.0-&gt;autogluon) (2.4.8)
Requirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /opt/homebrew/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==1.1.0-&gt;autogluon) (2.0.10)
Requirement already satisfied: weasel&lt;0.4.0,&gt;=0.1.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==1.1.0-&gt;autogluon) (0.3.4)
Requirement already satisfied: typer&lt;0.10.0,&gt;=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==1.1.0-&gt;autogluon) (0.9.0)
Requirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /opt/homebrew/lib/python3.11/site-packages (from spacy&lt;4-&gt;fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==1.1.0-&gt;autogluon) (3.3.0)
Requirement already satisfied: patsy&gt;=0.5.4 in /opt/homebrew/lib/python3.11/site-packages (from statsmodels&gt;=0.13.2-&gt;statsforecast&lt;1.5,&gt;=1.4.0-&gt;autogluon.timeseries==1.1.0-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon) (0.5.6)
Requirement already satisfied: distlib&lt;1,&gt;=0.3.7 in /opt/homebrew/lib/python3.11/site-packages (from virtualenv!=20.21.1,&gt;=20.0.24-&gt;ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (0.3.7)
Requirement already satisfied: platformdirs&lt;5,&gt;=3.9.1 in /Users/ymadigital/Library/Python/3.11/lib/python/site-packages (from virtualenv!=20.21.1,&gt;=20.0.24-&gt;ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (3.10.0)
Collecting humanfriendly&gt;=9.1 (from coloredlogs-&gt;optimum&lt;1.19,&gt;=1.17-&gt;optimum[onnxruntime]&lt;1.19,&gt;=1.17; extra == &quot;all&quot;-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon)
  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)
Collecting ordered-set (from model-index-&gt;openmim&lt;0.4.0,&gt;=0.3.7-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)
Collecting opencensus-context&gt;=0.1.3 (from opencensus-&gt;ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)
Collecting google-api-core&lt;3.0.0,&gt;=1.0.0 (from opencensus-&gt;ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading google_api_core-2.18.0-py3-none-any.whl.metadata (2.7 kB)
Collecting pycryptodome (from opendatalab-&gt;openmim&lt;0.4.0,&gt;=0.3.7-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading pycryptodome-3.20.0-cp35-abi3-macosx_10_9_universal2.whl.metadata (3.4 kB)
Collecting openxlab (from opendatalab-&gt;openmim&lt;0.4.0,&gt;=0.3.7-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading openxlab-0.0.38-py3-none-any.whl.metadata (3.8 kB)
Requirement already satisfied: tenacity&gt;=6.2.0 in /opt/homebrew/lib/python3.11/site-packages (from plotly-&gt;statsforecast&lt;1.5,&gt;=1.4.0-&gt;autogluon.timeseries==1.1.0-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon) (8.2.3)
Collecting markdown-it-py&gt;=2.2.0 (from rich-&gt;openmim&lt;0.4.0,&gt;=0.3.7-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)
Requirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /Users/ymadigital/Library/Python/3.11/lib/python/site-packages (from rich-&gt;openmim&lt;0.4.0,&gt;=0.3.7-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (2.16.1)
Requirement already satisfied: mpmath&gt;=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy-&gt;optimum&lt;1.19,&gt;=1.17-&gt;optimum[onnxruntime]&lt;1.19,&gt;=1.17; extra == &quot;all&quot;-&gt;autogluon.timeseries[all]==1.1.0-&gt;autogluon) (1.3.0)
Collecting googleapis-common-protos&lt;2.0.dev0,&gt;=1.56.2 (from google-api-core&lt;3.0.0,&gt;=1.0.0-&gt;opencensus-&gt;ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl.metadata (1.5 kB)
Collecting proto-plus&lt;2.0.0dev,&gt;=1.22.3 (from google-api-core&lt;3.0.0,&gt;=1.0.0-&gt;opencensus-&gt;ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading proto_plus-1.23.0-py3-none-any.whl.metadata (2.2 kB)
Collecting google-auth&lt;3.0.dev0,&gt;=2.14.1 (from google-api-core&lt;3.0.0,&gt;=1.0.0-&gt;opencensus-&gt;ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading google_auth-2.29.0-py2.py3-none-any.whl.metadata (4.7 kB)
Collecting mdurl~=0.1 (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;openmim&lt;0.4.0,&gt;=0.3.7-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)
Requirement already satisfied: blis&lt;0.8.0,&gt;=0.7.8 in /opt/homebrew/lib/python3.11/site-packages (from thinc&lt;8.3.0,&gt;=8.1.8-&gt;spacy&lt;4-&gt;fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==1.1.0-&gt;autogluon) (0.7.11)
Requirement already satisfied: confection&lt;1.0.0,&gt;=0.0.1 in /opt/homebrew/lib/python3.11/site-packages (from thinc&lt;8.3.0,&gt;=8.1.8-&gt;spacy&lt;4-&gt;fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==1.1.0-&gt;autogluon) (0.1.4)
Requirement already satisfied: cloudpathlib&lt;0.17.0,&gt;=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from weasel&lt;0.4.0,&gt;=0.1.0-&gt;spacy&lt;4-&gt;fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==1.1.0-&gt;autogluon) (0.16.0)
Requirement already satisfied: soupsieve&gt;1.2 in /opt/homebrew/lib/python3.11/site-packages (from beautifulsoup4-&gt;gdown&gt;=4.0.0-&gt;nlpaug&lt;1.2.0,&gt;=1.1.10-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (2.5)
Collecting oss2~=2.17.0 (from openxlab-&gt;opendatalab-&gt;openmim&lt;0.4.0,&gt;=0.3.7-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading oss2-2.17.0.tar.gz (259 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 259.5/259.5 kB 3.3 MB/s eta 0:00:00a 0:00:01
etadata (setup.py) ...  autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading requests-2.28.2-py3-none-any.whl.metadata (4.6 kB)
Collecting rich (from openmim&lt;0.4.0,&gt;=0.3.7-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading rich-13.4.2-py3-none-any.whl.metadata (18 kB)
Collecting setuptools (from autogluon.common==1.1.0-&gt;autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading setuptools-60.2.0-py3-none-any.whl.metadata (5.1 kB)
Collecting tqdm&lt;5,&gt;=4.38 (from autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading tqdm-4.65.2-py3-none-any.whl.metadata (56 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.0/57.0 kB 4.4 MB/s eta 0:00:00
ultiple versions of requests[socks] to determine which version is compatible with other requirements. This could take a while.
Collecting requests[socks] (from gdown&gt;=4.0.0-&gt;nlpaug&lt;1.2.0,&gt;=1.1.10-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading requests-2.30.0-py3-none-any.whl.metadata (4.6 kB)
  Downloading requests-2.29.0-py3-none-any.whl.metadata (4.6 kB)
Requirement already satisfied: PySocks!=1.5.7,&gt;=1.5.6 in /opt/homebrew/lib/python3.11/site-packages (from requests[socks]-&gt;gdown&gt;=4.0.0-&gt;nlpaug&lt;1.2.0,&gt;=1.1.10-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (1.7.1)
Requirement already satisfied: cachetools&lt;6.0,&gt;=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from google-auth&lt;3.0.dev0,&gt;=2.14.1-&gt;google-api-core&lt;3.0.0,&gt;=1.0.0-&gt;opencensus-&gt;ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (4.2.4)
Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /opt/homebrew/lib/python3.11/site-packages (from google-auth&lt;3.0.dev0,&gt;=2.14.1-&gt;google-api-core&lt;3.0.0,&gt;=1.0.0-&gt;opencensus-&gt;ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (0.3.0)
Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /opt/homebrew/lib/python3.11/site-packages (from google-auth&lt;3.0.dev0,&gt;=2.14.1-&gt;google-api-core&lt;3.0.0,&gt;=1.0.0-&gt;opencensus-&gt;ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (4.9)
Collecting crcmod&gt;=1.7 (from oss2~=2.17.0-&gt;openxlab-&gt;opendatalab-&gt;openmim&lt;0.4.0,&gt;=0.3.7-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading crcmod-1.7.tar.gz (89 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.7/89.7 kB 4.9 MB/s eta 0:00:00
etadata (setup.py) ... s&gt;=2.4.1 (from oss2~=2.17.0-&gt;openxlab-&gt;opendatalab-&gt;openmim&lt;0.4.0,&gt;=0.3.7-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading aliyun_python_sdk_kms-2.16.2-py2.py3-none-any.whl.metadata (1.5 kB)
Collecting aliyun-python-sdk-core&gt;=2.13.12 (from oss2~=2.17.0-&gt;openxlab-&gt;opendatalab-&gt;openmim&lt;0.4.0,&gt;=0.3.7-&gt;autogluon.multimodal==1.1.0-&gt;autogluon)
  Downloading aliyun-python-sdk-core-2.15.1.tar.gz (443 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 443.1/443.1 kB 3.1 MB/s eta 0:00:00a 0:00:01
etadata (setup.py) ... espath&lt;2.0.0,&gt;=0.7.1 (from boto3&lt;2,&gt;=1.10-&gt;autogluon.core==1.1.0-&gt;autogluon.core[all]==1.1.0-&gt;autogluon)
  Downloading jmespath-0.10.0-py2.py3-none-any.whl.metadata (8.0 kB)
Requirement already satisfied: cryptography&gt;=2.6.0 in /opt/homebrew/lib/python3.11/site-packages (from aliyun-python-sdk-core&gt;=2.13.12-&gt;oss2~=2.17.0-&gt;openxlab-&gt;opendatalab-&gt;openmim&lt;0.4.0,&gt;=0.3.7-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (41.0.7)
Requirement already satisfied: pyasn1&lt;0.6.0,&gt;=0.4.6 in /opt/homebrew/lib/python3.11/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3.0.dev0,&gt;=2.14.1-&gt;google-api-core&lt;3.0.0,&gt;=1.0.0-&gt;opencensus-&gt;ray[default,tune]&lt;2.11,&gt;=2.10.0; extra == &quot;all&quot;-&gt;autogluon.core[all]==1.1.0-&gt;autogluon) (0.5.1)
Requirement already satisfied: cffi&gt;=1.12 in /opt/homebrew/lib/python3.11/site-packages (from cryptography&gt;=2.6.0-&gt;aliyun-python-sdk-core&gt;=2.13.12-&gt;oss2~=2.17.0-&gt;openxlab-&gt;opendatalab-&gt;openmim&lt;0.4.0,&gt;=0.3.7-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (1.16.0)
Requirement already satisfied: pycparser in /opt/homebrew/lib/python3.11/site-packages (from cffi&gt;=1.12-&gt;cryptography&gt;=2.6.0-&gt;aliyun-python-sdk-core&gt;=2.13.12-&gt;oss2~=2.17.0-&gt;openxlab-&gt;opendatalab-&gt;openmim&lt;0.4.0,&gt;=0.3.7-&gt;autogluon.multimodal==1.1.0-&gt;autogluon) (2.21)
Downloading autogluon-1.1.0-py3-none-any.whl (9.7 kB)
Downloading autogluon.core-1.1.0-py3-none-any.whl (232 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.6/232.6 kB 3.4 MB/s eta 0:00:00a 0:00:01
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.0/63.0 kB 5.0 MB/s eta 0:00:00
ultimodal-1.1.0-py3-none-any.whl (427 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 427.5/427.5 kB 3.3 MB/s eta 0:00:00a 0:00:01
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 308.5/308.5 kB 2.1 MB/s eta 0:00:00a 0:00:01
eseries-1.1.0-py3-none-any.whl (147 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.1/147.1 kB 2.9 MB/s eta 0:00:00 0:00:01
mon-1.1.0-py3-none-any.whl (64 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.3/64.3 kB 3.5 MB/s eta 0:00:00
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 244.2/244.2 kB 3.5 MB/s eta 0:00:00a 0:00:01
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139.3/139.3 kB 4.8 MB/s eta 0:00:00
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 4.2 MB/s eta 0:00:00
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.7/232.7 kB 3.6 MB/s eta 0:00:00a 0:00:01
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 3.2 MB/s eta 0:00:00a 0:00:01
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 3.3 MB/s eta 0:00:00a 0:00:01
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 2.7 MB/s eta 0:00:00a 0:00:01
lforecast-0.10.0-py3-none-any.whl (47 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.6/47.6 kB 4.4 MB/s eta 0:00:00
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 3.2 MB/s eta 0:00:00a 0:00:01
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.5/410.5 kB 3.5 MB/s eta 0:00:00a 0:00:01
egaconf-2.2.3-py3-none-any.whl (79 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.3/79.3 kB 4.4 MB/s eta 0:00:00
im-0.3.9-py2.py3-none-any.whl (52 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.7/52.7 kB 3.7 MB/s eta 0:00:00
um-1.18.1-py3-none-any.whl (410 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.0/410.0 kB 2.4 MB/s eta 0:00:00a 0:00:01
acosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (250 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 250.6/250.6 kB 3.4 MB/s eta 0:00:00a 0:00:01
acosx_11_0_arm64.whl (11.3 MB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/11.3 MB 3.3 MB/s eta 0:00:0000:0100:01
age-1.17.0-py3-none-any.whl (11 kB)
Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)
Downloading pytorch_lightning-2.1.4-py3-none-any.whl (778 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 778.1/778.1 kB 2.2 MB/s eta 0:00:00a 0:00:01
etric_learning-2.3.0-py3-none-any.whl (115 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 2.5 MB/s eta 0:00:00 0:00:01
acosx_11_0_arm64.whl (63.8 MB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.8/63.8 MB 6.7 MB/s eta 0:00:0000:0100:01
age-0.20.0-cp311-cp311-macosx_12_0_arm64.whl (12.6 MB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.6/12.6 MB 6.1 MB/s eta 0:00:0000:0100:01
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.0/92.0 kB 32.1 MB/s eta 0:00:00
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 15.8 MB/s eta 0:00:00a 0:00:01
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.2/78.2 kB 47.2 MB/s eta 0:00:00
m-0.9.16-py3-none-any.whl (2.2 MB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 12.3 MB/s eta 0:00:00a 0:00:01
etrics-1.2.1-py3-none-any.whl (806 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 806.1/806.1 kB 18.9 MB/s eta 0:00:00a 0:00:01
ers-4.38.2-py3-none-any.whl (8.5 MB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.5/8.5 MB 10.5 MB/s eta 0:00:0000:0100:01
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.2/12.2 MB 24.3 MB/s eta 0:00:00a 0:00:01
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.7/67.7 kB 15.2 MB/s eta 0:00:00
ageio-2.34.1-py3-none-any.whl (313 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 313.5/313.5 kB 16.9 MB/s eta 0:00:00
e-1.17.3-cp311-cp311-macosx_11_0_universal2.whl (14.8 MB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.8/14.8 MB 8.2 MB/s eta 0:00:00a 0:00:01
acosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl (3.0 MB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 29.4 MB/s eta 0:00:00a 0:00:01
acosx_11_0_arm64.whl (4.3 MB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/4.3 MB 21.7 MB/s eta 0:00:00a 0:00:01
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.2/82.2 kB 316.1 MB/s eta 0:00:00
acosx_11_0_arm64.whl (410 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.3/410.3 kB 38.7 MB/s eta 0:00:00
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.7/101.7 kB 231.2 MB/s eta 0:00:00
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 225.0/225.0 kB 8.8 MB/s eta 0:00:00
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.1/56.1 kB 211.6 MB/s eta 0:00:00
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 57.2 MB/s eta 0:00:00
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 201.4/201.4 kB 31.6 MB/s eta 0:00:00
odel_index-0.1.11-py3-none-any.whl (34 kB)
Downloading onnx-1.16.0-cp311-cp311-macosx_10_15_universal2.whl (16.5 MB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.5/16.5 MB 16.3 MB/s eta 0:00:0000:0100:01
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.2/128.2 kB 55.2 MB/s eta 0:00:00
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.5/200.5 kB 39.7 MB/s eta 0:00:00
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.3/138.3 kB 347.6 MB/s eta 0:00:00
anfriendly-10.0-py2.py3-none-any.whl (86 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 297.1 MB/s eta 0:00:00
arkdown_it_py-3.0.0-py3-none-any.whl (87 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.5/87.5 kB 297.3 MB/s eta 0:00:00
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.8/302.8 kB 26.5 MB/s eta 0:00:00
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 202.1 MB/s eta 0:00:00
-4.65.2-py3-none-any.whl (77 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 268.4 MB/s eta 0:00:00
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.4/239.4 kB 38.3 MB/s eta 0:00:00
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 953.1/953.1 kB 28.2 MB/s eta 0:00:00
e-3.20.0-cp35-abi3-macosx_10_9_universal2.whl (2.4 MB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 24.5 MB/s eta 0:00:00a 0:00:01
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 189.2/189.2 kB 51.2 MB/s eta 0:00:00
mon_protos-1.63.0-py2.py3-none-any.whl (229 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.1/229.1 kB 37.8 MB/s eta 0:00:00
durl-0.1.2-py3-none-any.whl (10.0 kB)
Downloading proto_plus-1.23.0-py3-none-any.whl (48 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.8/48.8 kB 142.6 MB/s eta 0:00:00
espath-0.10.0-py2.py3-none-any.whl (24 kB)
Downloading aliyun_python_sdk_kms-2.16.2-py2.py3-none-any.whl (94 kB)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.0/94.0 kB 30.9 MB/s eta 0:00:00
l-py3, lightgbm, antlr4-python3-runtime, seqeval, oss2, aliyun-python-sdk-core, crcmod
  Building wheel for nvidia-ml-py3 (setup.py) ... l-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19172 sha256=5deec37732aa47327992113a1b36c1b3ef1f7dd795b1875a78da6eb1bb98a423
  Stored in directory: /private/var/folders/1s/8_yq4qfn4wg43t727cpmhgb40000gn/T/pip-ephem-wheel-cache-lz6aeif8/wheels/47/50/9e/29dc79037d74c3c1bb4a8661fb608e8674b7e4260d6a3f8f51
  Building wheel for lightgbm (pyproject.toml) ... : filename=lightgbm-4.3.0-py3-none-macosx_14_0_arm64.whl size=1417983 sha256=4701db5ac84fa48f744654d9cc050caaed233fd329787f99073c85ca4042d04b
  Stored in directory: /private/var/folders/1s/8_yq4qfn4wg43t727cpmhgb40000gn/T/pip-ephem-wheel-cache-lz6aeif8/wheels/f4/d1/f4/3cd66516e5a05b0879846879dfb06e5a087f453adb405a5999
  Building wheel for antlr4-python3-runtime (setup.py) ... e: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=479684e9cf96d7bfe8f8429fd5f28a5a66b3d8ca75a317eaeeb0164ae5f8b6b9
  Stored in directory: /private/var/folders/1s/8_yq4qfn4wg43t727cpmhgb40000gn/T/pip-ephem-wheel-cache-lz6aeif8/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1
  Building wheel for seqeval (setup.py) ... e=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=1d780b0861205f37f7d8e9763638247ed0e0b147c7714a620b098c9825f0c34b
  Stored in directory: /private/var/folders/1s/8_yq4qfn4wg43t727cpmhgb40000gn/T/pip-ephem-wheel-cache-lz6aeif8/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead
  Building wheel for oss2 (setup.py) ... e=oss2-2.17.0-py3-none-any.whl size=112372 sha256=247f73f826165f60b18251f668ba9d469e910354dab23c45af1f0e89725cbfe5
  Stored in directory: /private/var/folders/1s/8_yq4qfn4wg43t727cpmhgb40000gn/T/pip-ephem-wheel-cache-lz6aeif8/wheels/42/79/aa/3671e313c27de35211d345d7a9d8ccb7dde515cf05edba75df
  Building wheel for aliyun-python-sdk-core (setup.py) ... e=aliyun_python_sdk_core-2.15.1-py3-none-any.whl size=535325 sha256=c10234dc8a63154e30c25a5944ac23d553f8c024968155d99f6554e56f3a1223
  Stored in directory: /private/var/folders/1s/8_yq4qfn4wg43t727cpmhgb40000gn/T/pip-ephem-wheel-cache-lz6aeif8/wheels/a8/3d/4d/0247faf6ab88eb63524d53c86ad8d3e9672f11babca7fd0e53
  Building wheel for crcmod (setup.py) ... od: filename=crcmod-1.7-cp311-cp311-macosx_13_0_arm64.whl size=22108 sha256=3f3ccd3e0c5ec6880939f267b52f9e47f0fe566f19ff04f68bae8d06a19c778e
  Stored in directory: /private/var/folders/1s/8_yq4qfn4wg43t727cpmhgb40000gn/T/pip-ephem-wheel-cache-lz6aeif8/wheels/23/94/7a/8cb7d14597e6395ce969933f01aed9ea8fa5f5b4d4c8a61e99
Successfully built nvidia-ml-py3 lightgbm antlr4-python3-runtime seqeval oss2 aliyun-python-sdk-core crcmod
Installing collected packages: text-unidecode, py4j, py-spy, opencensus-context, nvidia-ml-py3, flatbuffers, crcmod, colorful, antlr4-python3-runtime, tqdm, toolz, tifffile, tensorboardX, setuptools, safetensors, requests, PyWavelets, pytesseract, pycryptodome, proto-plus, pdf2image, orjson, ordered-set, onnx, omegaconf, nptyping, networkx, mdurl, jmespath, imageio, humanfriendly, googleapis-common-protos, fastprogress, fastcore, cloudpickle, window-ops, tensorboard, scikit-image, responses, pandas, model-index, markdown-it-py, lightning-utilities, lightgbm, hyperopt, google-auth, fastdownload, coloredlogs, botocore, utilsforecast, torchmetrics, seqeval, s3transfer, rich, pytorch-metric-learning, onnxruntime, google-api-core, gluonts, gdown, aliyun-python-sdk-core, aiohttp-cors, accelerate, transformers, timm, statsforecast, ray, pytorch-lightning, opencensus, nlpaug, mlforecast, boto3, aliyun-python-sdk-kms, oss2, lightning, evaluate, autogluon.common, optimum, openxlab, fastai, autogluon.features, autogluon.core, opendatalab, autogluon.tabular, openmim, autogluon.timeseries, autogluon.multimodal, autogluon
  Attempting uninstall: tqdm
    Found existing installation: tqdm 4.64.1
    Uninstalling tqdm-4.64.1:
      Successfully uninstalled tqdm-4.64.1
  Attempting uninstall: setuptools
    Found existing installation: setuptools 69.5.1
    Uninstalling setuptools-69.5.1:
      Successfully uninstalled setuptools-69.5.1
  Attempting uninstall: safetensors
    Found existing installation: safetensors 0.4.0
    Uninstalling safetensors-0.4.0:
      Successfully uninstalled safetensors-0.4.0
  Attempting uninstall: requests
    Found existing installation: requests 2.31.0
    Uninstalling requests-2.31.0:
      Successfully uninstalled requests-2.31.0
  Attempting uninstall: networkx
    Found existing installation: networkx 2.8.8
    Uninstalling networkx-2.8.8:
      Successfully uninstalled networkx-2.8.8
  Attempting uninstall: jmespath
    Found existing installation: jmespath 1.0.1
    Uninstalling jmespath-1.0.1:
      Successfully uninstalled jmespath-1.0.1
  Attempting uninstall: tensorboard
    Found existing installation: tensorboard 2.2.2
    Uninstalling tensorboard-2.2.2:
      Successfully uninstalled tensorboard-2.2.2
  Attempting uninstall: pandas
    Found existing installation: pandas 1.5.3
    Uninstalling pandas-1.5.3:
      Successfully uninstalled pandas-1.5.3
  Attempting uninstall: google-auth
    Found existing installation: google-auth 1.35.0
    Uninstalling google-auth-1.35.0:
      Successfully uninstalled google-auth-1.35.0
  Attempting uninstall: transformers
    Found existing installation: transformers 4.35.2
    Uninstalling transformers-4.35.2:
      Successfully uninstalled transformers-4.35.2
ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
simuleval 1.1.4 requires tqdm==4.64.1, but you have tqdm 4.65.2 which is incompatible.
jupyterlab-server 2.25.2 requires requests&gt;=2.31, but you have requests 2.28.2 which is incompatible.
scrapeops-scrapy 0.5.4 requires requests&gt;=2.31.0, but you have requests 2.28.2 which is incompatible.
scrapeops-scrapy 0.5.4 requires urllib3&gt;=2.1, but you have urllib3 1.26.18 which is incompatible.
tts 0.20.6 requires pandas&lt;2.0,&gt;=1.4, but you have pandas 2.2.2 which is incompatible.
yt-dlp 2023.11.16 requires requests&lt;3,&gt;=2.31.0, but you have requests 2.28.2 which is incompatible.
gruut 2.2.3 requires networkx&lt;3.0.0,&gt;=2.5.0, but you have networkx 3.3 which is incompatible.
Successfully installed PyWavelets-1.6.0 accelerate-0.21.0 aiohttp-cors-0.7.0 aliyun-python-sdk-core-2.15.1 aliyun-python-sdk-kms-2.16.2 antlr4-python3-runtime-4.9.3 autogluon-1.1.0 autogluon.common-1.1.0 autogluon.core-1.1.0 autogluon.features-1.1.0 autogluon.multimodal-1.1.0 autogluon.tabular-1.1.0 autogluon.timeseries-1.1.0 boto3-1.34.93 botocore-1.34.93 cloudpickle-3.0.0 coloredlogs-15.0.1 colorful-0.5.6 crcmod-1.7 evaluate-0.4.1 fastai-2.7.15 fastcore-1.5.32 fastdownload-0.0.7 fastprogress-1.0.3 flatbuffers-24.3.25 gdown-5.1.0 gluonts-0.14.3 google-api-core-2.18.0 google-auth-2.29.0 googleapis-common-protos-1.63.0 humanfriendly-10.0 hyperopt-0.2.7 imageio-2.34.1 jmespath-0.10.0 lightgbm-4.3.0 lightning-2.1.4 lightning-utilities-0.11.2 markdown-it-py-3.0.0 mdurl-0.1.2 mlforecast-0.10.0 model-index-0.1.11 networkx-3.3 nlpaug-1.1.11 nptyping-2.4.1 nvidia-ml-py3-7.352.0 omegaconf-2.2.3 onnx-1.16.0 onnxruntime-1.17.3 opencensus-0.11.4 opencensus-context-0.1.3 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.0.38 optimum-1.18.1 ordered-set-4.1.0 orjson-3.10.1 oss2-2.17.0 pandas-2.2.2 pdf2image-1.17.0 proto-plus-1.23.0 py-spy-0.3.14 py4j-0.10.9.7 pycryptodome-3.20.0 pytesseract-0.3.10 pytorch-lightning-2.1.4 pytorch-metric-learning-2.3.0 ray-2.10.0 requests-2.28.2 responses-0.18.0 rich-13.4.2 s3transfer-0.10.1 safetensors-0.4.3 scikit-image-0.20.0 seqeval-1.2.2 setuptools-60.2.0 statsforecast-1.4.0 tensorboard-2.16.2 tensorboardX-2.6.2.2 text-unidecode-1.3 tifffile-2024.4.24 timm-0.9.16 toolz-0.12.1 torchmetrics-1.2.1 tqdm-4.65.2 transformers-4.38.2 utilsforecast-0.0.10 window-ops-0.0.15
</code></pre>
</div>
</div>
<section id="setup-kaggle-api-key" class="cell markdown">
<h3>Setup Kaggle API Key</h3>
</section>
<div class="cell code" data-execution_count="10">
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mkdir <span class="op">-</span>p <span class="op">~/</span>.kaggle</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>touch <span class="op">~/</span>.kaggle<span class="op">/</span>kaggle.json</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>chmod <span class="dv">600</span> <span class="op">~/</span>.kaggle<span class="op">/</span>kaggle.json</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="13">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fill in your user name and key from creating the kaggle account and API token file</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>kaggle_username <span class="op">=</span> <span class="st">&quot;hafidabelayd&quot;</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>kaggle_key <span class="op">=</span> <span class="st">&quot;05fab7c442f85f92564ff8d94497d07e&quot;</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Save API token the kaggle.json file</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Specify the path to the kaggle.json file in the user&#39;s home directory</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>kaggle_json_path <span class="op">=</span> os.path.expanduser(<span class="st">&quot;~/.kaggle/kaggle.json&quot;</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the new API token to the kaggle.json file</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(kaggle_json_path, <span class="st">&quot;w&quot;</span>) <span class="im">as</span> f:</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    json.dump({<span class="st">&quot;username&quot;</span>: kaggle_username, <span class="st">&quot;key&quot;</span>: kaggle_key}, f)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the correct permissions on the kaggle.json file</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>os.chmod(kaggle_json_path, <span class="bn">0o600</span>)</span></code></pre></div>
</div>
<section id="download-and-explore-dataset" class="cell markdown">
<h3>Download and explore dataset</h3>
</section>
<section
id="go-to-the-bike-sharing-demand-competition-and-agree-to-the-terms"
class="cell markdown">
<h3>Go to the bike sharing demand competition and agree to the
terms</h3>
<p><img src="vertopal_0be6c1bfb8f540fb92135113ec53c6ab/kaggle6.png"
alt="kaggle6.png" /></p>
</section>
<div class="cell code" data-execution_count="15">
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Download the dataset, it will be in a .zip file so you&#39;ll need to unzip it as well.</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>kaggle competitions download <span class="op">-</span>c bike<span class="op">-</span>sharing<span class="op">-</span>demand</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># If you already downloaded it you can use the -o command to overwrite the file</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>unzip <span class="op">-</span>o bike<span class="op">-</span>sharing<span class="op">-</span>demand.<span class="bu">zip</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>Downloading bike-sharing-demand.zip to /Users/ymadigital/Downloads/cd0385-project-starter-main/project
100%|█████████████████████████████████████████| 189k/189k [00:00&lt;00:00, 636kB/s]
100%|█████████████████████████████████████████| 189k/189k [00:00&lt;00:00, 634kB/s]
Archive:  bike-sharing-demand.zip
  inflating: sampleSubmission.csv    
  inflating: test.csv                
  inflating: train.csv               
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="16">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> autogluon.tabular <span class="im">import</span> TabularPredictor</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="17">
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the train dataset in pandas by reading the csv</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the parsing of the datetime column so you can use some of the `dt` features in pandas later</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> pd.read_csv(<span class="st">&quot;/Users/ymadigital/Downloads/cd0385-project-starter-main/project/train.csv&quot;</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>train.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="17">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>datetime</th>
      <th>season</th>
      <th>holiday</th>
      <th>workingday</th>
      <th>weather</th>
      <th>temp</th>
      <th>atemp</th>
      <th>humidity</th>
      <th>windspeed</th>
      <th>casual</th>
      <th>registered</th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2011-01-01 00:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.84</td>
      <td>14.395</td>
      <td>81</td>
      <td>0.0</td>
      <td>3</td>
      <td>13</td>
      <td>16</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2011-01-01 01:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.02</td>
      <td>13.635</td>
      <td>80</td>
      <td>0.0</td>
      <td>8</td>
      <td>32</td>
      <td>40</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2011-01-01 02:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.02</td>
      <td>13.635</td>
      <td>80</td>
      <td>0.0</td>
      <td>5</td>
      <td>27</td>
      <td>32</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2011-01-01 03:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.84</td>
      <td>14.395</td>
      <td>75</td>
      <td>0.0</td>
      <td>3</td>
      <td>10</td>
      <td>13</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2011-01-01 04:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.84</td>
      <td>14.395</td>
      <td>75</td>
      <td>0.0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell code">
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple output of the train dataset to view some of the min/max/varition of the dataset features.</span></span></code></pre></div>
</div>
<div class="cell code" data-execution_count="18">
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the test pandas dataframe in pandas by reading the csv, remember to parse the datetime!</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> pd.read_csv(<span class="st">&quot;/Users/ymadigital/Downloads/cd0385-project-starter-main/project/test.csv&quot;</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>test.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="18">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>datetime</th>
      <th>season</th>
      <th>holiday</th>
      <th>workingday</th>
      <th>weather</th>
      <th>temp</th>
      <th>atemp</th>
      <th>humidity</th>
      <th>windspeed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2011-01-20 00:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>10.66</td>
      <td>11.365</td>
      <td>56</td>
      <td>26.0027</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2011-01-20 01:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>10.66</td>
      <td>13.635</td>
      <td>56</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2011-01-20 02:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>10.66</td>
      <td>13.635</td>
      <td>56</td>
      <td>0.0000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2011-01-20 03:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>10.66</td>
      <td>12.880</td>
      <td>56</td>
      <td>11.0014</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2011-01-20 04:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>10.66</td>
      <td>12.880</td>
      <td>56</td>
      <td>11.0014</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell code" data-execution_count="19">
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Same thing as train and test dataset</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>submission <span class="op">=</span> pd.read_csv(<span class="st">&quot;/Users/ymadigital/Downloads/cd0385-project-starter-main/project/sampleSubmission.csv&quot;</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>submission.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="19">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>datetime</th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2011-01-20 00:00:00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2011-01-20 01:00:00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2011-01-20 02:00:00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2011-01-20 03:00:00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2011-01-20 04:00:00</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<section id="step-3-train-a-model-using-autogluons-tabular-prediction"
class="cell markdown">
<h2>Step 3: Train a model using AutoGluon’s Tabular Prediction</h2>
</section>
<div class="cell markdown">
<p>Requirements:</p>
<ul>
<li>We are prediting <code>count</code>, so it is the label we are
setting.</li>
<li>Ignore <code>casual</code> and <code>registered</code> columns as
they are also not present in the test dataset.</li>
<li>Use the <code>root_mean_squared_error</code> as the metric to use
for evaluation.</li>
<li>Set a time limit of 10 minutes (600 seconds).</li>
<li>Use the preset <code>best_quality</code> to focus on creating the
best model.</li>
</ul>
</div>
<div class="cell code" data-execution_count="24">
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>predictor <span class="op">=</span> TabularPredictor(label<span class="op">=</span><span class="st">&quot;count&quot;</span>, problem_type<span class="op">=</span><span class="st">&quot;regression&quot;</span>, eval_metric<span class="op">=</span><span class="st">&quot;root_mean_squared_error&quot;</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>predictor.fit(train_data<span class="op">=</span>train.drop(columns<span class="op">=</span>[<span class="st">&quot;casual&quot;</span>, <span class="st">&quot;registered&quot;</span>]), time_limit<span class="op">=</span><span class="dv">600</span>, presets<span class="op">=</span><span class="st">&quot;best_quality&quot;</span>)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>No path specified. Models will be saved in: &quot;AutogluonModels/ag-20240428_174128&quot;
Presets specified: [&#39;best_quality&#39;]
Setting dynamic_stacking from &#39;auto&#39; to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)
Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1
Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.
Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.
Sub-fit(s) time limit is: 600 seconds.
Starting holdout-based sub-fit for dynamic stacking. Context path is: AutogluonModels/ag-20240428_174128/ds_sub_fit/sub_fit_ho.
Running the sub-fit in a ray process to avoid memory leakage.
Spend 165 seconds for the sub-fit(s) during dynamic stacking.
Time left for full fit of AutoGluon: 435 seconds.
Starting full fit now with num_stack_levels 1.
Beginning AutoGluon training ... Time limit = 435s
AutoGluon will save models to &quot;AutogluonModels/ag-20240428_174128&quot;
=================== System Info ===================
AutoGluon Version:  1.1.0
Python Version:     3.11.5
Operating System:   Darwin
Platform Machine:   arm64
Platform Version:   Darwin Kernel Version 23.3.0: Wed Dec 20 21:30:44 PST 2023; root:xnu-10002.81.5~7/RELEASE_ARM64_T6000
CPU Count:          8
Memory Avail:       4.78 GB / 16.00 GB (29.8%)
Disk Space Avail:   47.36 GB / 460.43 GB (10.3%)
===================================================
Train Data Rows:    10886
Train Data Columns: 9
Label Column:       count
Problem Type:       regression
Preprocessing data ...
Using Feature Generators to preprocess the data ...
Fitting AutoMLPipelineFeatureGenerator...
	Available Memory:                    4891.86 MB
	Train Data (Original)  Memory Usage: 1.45 MB (0.0% of available memory)
	Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.
	Stage 1 Generators:
		Fitting AsTypeFeatureGenerator...
			Note: Converting 2 features to boolean dtype as they only contain 2 unique values.
	Stage 2 Generators:
		Fitting FillNaFeatureGenerator...
	Stage 3 Generators:
		Fitting IdentityFeatureGenerator...
		Fitting DatetimeFeatureGenerator...
	Stage 4 Generators:
		Fitting DropUniqueFeatureGenerator...
	Stage 5 Generators:
		Fitting DropDuplicatesFeatureGenerator...
	Types of features in original data (raw dtype, special dtypes):
		(&#39;float&#39;, [])                      : 3 | [&#39;temp&#39;, &#39;atemp&#39;, &#39;windspeed&#39;]
		(&#39;int&#39;, [])                        : 5 | [&#39;season&#39;, &#39;holiday&#39;, &#39;workingday&#39;, &#39;weather&#39;, &#39;humidity&#39;]
		(&#39;object&#39;, [&#39;datetime_as_object&#39;]) : 1 | [&#39;datetime&#39;]
	Types of features in processed data (raw dtype, special dtypes):
		(&#39;float&#39;, [])                : 3 | [&#39;temp&#39;, &#39;atemp&#39;, &#39;windspeed&#39;]
		(&#39;int&#39;, [])                  : 3 | [&#39;season&#39;, &#39;weather&#39;, &#39;humidity&#39;]
		(&#39;int&#39;, [&#39;bool&#39;])            : 2 | [&#39;holiday&#39;, &#39;workingday&#39;]
		(&#39;int&#39;, [&#39;datetime_as_int&#39;]) : 5 | [&#39;datetime&#39;, &#39;datetime.year&#39;, &#39;datetime.month&#39;, &#39;datetime.day&#39;, &#39;datetime.dayofweek&#39;]
	0.1s = Fit runtime
	9 features in original data used to generate 13 features in processed data.
	Train Data (Processed) Memory Usage: 0.93 MB (0.0% of available memory)
Data preprocessing and feature engineering runtime = 0.07s ...
AutoGluon will gauge predictive performance using evaluation metric: &#39;root_mean_squared_error&#39;
	This metric&#39;s sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	To change this, specify the eval_metric parameter of Predictor()
Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.
User-specified model hyperparameters to be fit:
{
	&#39;NN_TORCH&#39;: [{}, {&#39;activation&#39;: &#39;elu&#39;, &#39;dropout_prob&#39;: 0.10077639529843717, &#39;hidden_size&#39;: 108, &#39;learning_rate&#39;: 0.002735937344002146, &#39;num_layers&#39;: 4, &#39;use_batchnorm&#39;: True, &#39;weight_decay&#39;: 1.356433327634438e-12, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;_r79&#39;, &#39;priority&#39;: -2}}, {&#39;activation&#39;: &#39;elu&#39;, &#39;dropout_prob&#39;: 0.11897478034205347, &#39;hidden_size&#39;: 213, &#39;learning_rate&#39;: 0.0010474382260641949, &#39;num_layers&#39;: 4, &#39;use_batchnorm&#39;: False, &#39;weight_decay&#39;: 5.594471067786272e-10, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;_r22&#39;, &#39;priority&#39;: -7}}],
	&#39;GBM&#39;: [{&#39;extra_trees&#39;: True, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;XT&#39;}}, {}, &#39;GBMLarge&#39;],
	&#39;CAT&#39;: [{}, {&#39;depth&#39;: 6, &#39;grow_policy&#39;: &#39;SymmetricTree&#39;, &#39;l2_leaf_reg&#39;: 2.1542798306067823, &#39;learning_rate&#39;: 0.06864209415792857, &#39;max_ctr_complexity&#39;: 4, &#39;one_hot_max_size&#39;: 10, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;_r177&#39;, &#39;priority&#39;: -1}}, {&#39;depth&#39;: 8, &#39;grow_policy&#39;: &#39;Depthwise&#39;, &#39;l2_leaf_reg&#39;: 2.7997999596449104, &#39;learning_rate&#39;: 0.031375015734637225, &#39;max_ctr_complexity&#39;: 2, &#39;one_hot_max_size&#39;: 3, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;_r9&#39;, &#39;priority&#39;: -5}}],
	&#39;XGB&#39;: [{}, {&#39;colsample_bytree&#39;: 0.6917311125174739, &#39;enable_categorical&#39;: False, &#39;learning_rate&#39;: 0.018063876087523967, &#39;max_depth&#39;: 10, &#39;min_child_weight&#39;: 0.6028633586934382, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;_r33&#39;, &#39;priority&#39;: -8}}, {&#39;colsample_bytree&#39;: 0.6628423832084077, &#39;enable_categorical&#39;: False, &#39;learning_rate&#39;: 0.08775715546881824, &#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 0.6294123374222513, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;_r89&#39;, &#39;priority&#39;: -16}}],
	&#39;FASTAI&#39;: [{}, {&#39;bs&#39;: 256, &#39;emb_drop&#39;: 0.5411770367537934, &#39;epochs&#39;: 43, &#39;layers&#39;: [800, 400], &#39;lr&#39;: 0.01519848858318159, &#39;ps&#39;: 0.23782946566604385, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;_r191&#39;, &#39;priority&#39;: -4}}, {&#39;bs&#39;: 2048, &#39;emb_drop&#39;: 0.05070411322605811, &#39;epochs&#39;: 29, &#39;layers&#39;: [200, 100], &#39;lr&#39;: 0.08974235041576624, &#39;ps&#39;: 0.10393466140748028, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;_r102&#39;, &#39;priority&#39;: -11}}],
	&#39;RF&#39;: [{&#39;criterion&#39;: &#39;gini&#39;, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;Gini&#39;, &#39;problem_types&#39;: [&#39;binary&#39;, &#39;multiclass&#39;]}}, {&#39;criterion&#39;: &#39;entropy&#39;, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;Entr&#39;, &#39;problem_types&#39;: [&#39;binary&#39;, &#39;multiclass&#39;]}}, {&#39;criterion&#39;: &#39;squared_error&#39;, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;MSE&#39;, &#39;problem_types&#39;: [&#39;regression&#39;, &#39;quantile&#39;]}}],
	&#39;XT&#39;: [{&#39;criterion&#39;: &#39;gini&#39;, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;Gini&#39;, &#39;problem_types&#39;: [&#39;binary&#39;, &#39;multiclass&#39;]}}, {&#39;criterion&#39;: &#39;entropy&#39;, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;Entr&#39;, &#39;problem_types&#39;: [&#39;binary&#39;, &#39;multiclass&#39;]}}, {&#39;criterion&#39;: &#39;squared_error&#39;, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;MSE&#39;, &#39;problem_types&#39;: [&#39;regression&#39;, &#39;quantile&#39;]}}],
	&#39;KNN&#39;: [{&#39;weights&#39;: &#39;uniform&#39;, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;Unif&#39;}}, {&#39;weights&#39;: &#39;distance&#39;, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;Dist&#39;}}],
}
AutoGluon will fit 2 stack levels (L1 to L2) ...
Fitting 108 L1 models ...
Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 289.88s of the 434.93s of remaining time.
	-101.5882	 = Validation score   (-root_mean_squared_error)
	0.01s	 = Training   runtime
	0.02s	 = Validation runtime
Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 289.81s of the 434.85s of remaining time.
	-84.1464	 = Validation score   (-root_mean_squared_error)
	0.01s	 = Training   runtime
	0.02s	 = Validation runtime
Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 289.76s of the 434.8s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.17%)
	-131.4609	 = Validation score   (-root_mean_squared_error)
	10.38s	 = Training   runtime
	4.57s	 = Validation runtime
Fitting model: LightGBM_BAG_L1 ... Training model for up to 275.36s of the 420.41s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.15%)
	-131.0542	 = Validation score   (-root_mean_squared_error)
	1.66s	 = Training   runtime
	0.99s	 = Validation runtime
Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 271.5s of the 416.54s of remaining time.
	-116.5439	 = Validation score   (-root_mean_squared_error)
	2.52s	 = Training   runtime
	0.31s	 = Validation runtime
Fitting model: CatBoost_BAG_L1 ... Training model for up to 268.46s of the 413.5s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.21%)
	Warning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.
		ray::_ray_fit() (pid=22987, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=22987, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 266.48s of the 411.53s of remaining time.
	-124.5878	 = Validation score   (-root_mean_squared_error)
	0.94s	 = Training   runtime
	0.31s	 = Validation runtime
Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 265.0s of the 410.05s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.21%)
2024-04-28 18:44:42,460	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:44:42,468	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:44:42,469	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:44:42,471	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:44:42,473	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:44:42,474	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:44:42,475	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
	-135.9455	 = Validation score   (-root_mean_squared_error)
	8.84s	 = Training   runtime
	0.09s	 = Validation runtime
Fitting model: XGBoost_BAG_L1 ... Training model for up to 254.3s of the 399.34s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.29%)
	-131.8939	 = Validation score   (-root_mean_squared_error)
	1.54s	 = Training   runtime
	0.24s	 = Validation runtime
Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 249.53s of the 394.58s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.10%)
	-138.3607	 = Validation score   (-root_mean_squared_error)
	40.23s	 = Training   runtime
	0.11s	 = Validation runtime
Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 207.31s of the 352.36s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.30%)
	-130.1323	 = Validation score   (-root_mean_squared_error)
	2.05s	 = Training   runtime
	0.73s	 = Validation runtime
Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 202.71s of the 347.75s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)
	Warning: Exception caused CatBoost_r177_BAG_L1 to fail during training (ImportError)... Skipping this model.
		ray::_ray_fit() (pid=23366, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=23366, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 200.46s of the 345.5s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.11%)
2024-04-28 18:45:48,678	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:45:48,681	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:45:48,682	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:45:48,683	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:45:48,684	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:45:48,686	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:45:48,687	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
	-140.3146	 = Validation score   (-root_mean_squared_error)
	36.03s	 = Training   runtime
	0.09s	 = Validation runtime
Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 162.49s of the 307.54s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.18%)
	-129.8629	 = Validation score   (-root_mean_squared_error)
	4.88s	 = Training   runtime
	4.23s	 = Validation runtime
Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 154.26s of the 299.3s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.21%)
	-134.7179	 = Validation score   (-root_mean_squared_error)
	44.45s	 = Training   runtime
	0.19s	 = Validation runtime
Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 108.09s of the 253.13s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.88%)
	Warning: Exception caused CatBoost_r9_BAG_L1 to fail during training (ImportError)... Skipping this model.
		ray::_ray_fit() (pid=23833, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=23833, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 104.42s of the 249.46s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.19%)
2024-04-28 18:47:25,255	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): ray::_ray_fit() (pid=23830, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=23830, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
2024-04-28 18:47:25,258	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): ray::_ray_fit() (pid=23835, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=23835, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
2024-04-28 18:47:25,259	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): ray::_ray_fit() (pid=23836, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=23836, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
2024-04-28 18:47:25,260	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): ray::_ray_fit() (pid=23831, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=23831, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
2024-04-28 18:47:25,262	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): ray::_ray_fit() (pid=23832, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=23832, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
2024-04-28 18:47:25,263	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
	-134.5292	 = Validation score   (-root_mean_squared_error)
	10.99s	 = Training   runtime
	11.39s	 = Validation runtime
Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 89.22s of the 234.26s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.14%)
	-139.1747	 = Validation score   (-root_mean_squared_error)
	52.55s	 = Training   runtime
	0.16s	 = Validation runtime
Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 34.26s of the 179.3s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.01%)
	-129.5673	 = Validation score   (-root_mean_squared_error)
	3.91s	 = Training   runtime
	0.6s	 = Validation runtime
Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 27.69s of the 172.74s of remaining time.
	-123.6154	 = Validation score   (-root_mean_squared_error)
	0.75s	 = Training   runtime
	0.31s	 = Validation runtime
Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 26.42s of the 171.46s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.15%)
	Warning: Exception caused CatBoost_r137_BAG_L1 to fail during training (ImportError)... Skipping this model.
		ray::_ray_fit() (pid=24275, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=24275, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 24.2s of the 169.25s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)
2024-04-28 18:48:45,468	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:48:45,499	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:48:45,509	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:48:45,514	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:48:45,518	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:48:45,522	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:48:45,526	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
	-138.1456	 = Validation score   (-root_mean_squared_error)
	8.56s	 = Training   runtime
	0.12s	 = Validation runtime
Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 13.7s of the 158.74s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.79%)
	Warning: Exception caused CatBoost_r13_BAG_L1 to fail during training (ImportError)... Skipping this model.
		ray::_ray_fit() (pid=24370, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=24370, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 9.22s of the 154.27s of remaining time.
	-116.795	 = Validation score   (-root_mean_squared_error)
	2.0s	 = Training   runtime
	0.31s	 = Validation runtime
Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 6.65s of the 151.69s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.37%)
2024-04-28 18:49:00,548	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:49:00,553	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:49:00,563	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:49:00,611	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:49:00,614	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:49:00,615	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:49:00,616	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
	-130.3842	 = Validation score   (-root_mean_squared_error)
	7.47s	 = Training   runtime
	11.14s	 = Validation runtime
Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 139.75s of remaining time.
	Ensemble Weights: {&#39;KNeighborsDist_BAG_L1&#39;: 1.0}
	-84.1464	 = Validation score   (-root_mean_squared_error)
	0.03s	 = Training   runtime
	0.0s	 = Validation runtime
Fitting 106 L2 models ...
Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 139.7s of the 139.63s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.35%)
	-60.4864	 = Validation score   (-root_mean_squared_error)
	8.17s	 = Training   runtime
	5.3s	 = Validation runtime
Fitting model: LightGBM_BAG_L2 ... Training model for up to 128.59s of the 128.51s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.33%)
	-55.1907	 = Validation score   (-root_mean_squared_error)
	2.03s	 = Training   runtime
	0.46s	 = Validation runtime
Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 122.7s of the 122.63s of remaining time.
	-53.5781	 = Validation score   (-root_mean_squared_error)
	12.43s	 = Training   runtime
	0.38s	 = Validation runtime
Fitting model: CatBoost_BAG_L2 ... Training model for up to 109.62s of the 109.55s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.45%)
	Warning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.
		ray::_ray_fit() (pid=24632, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=24632, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 107.13s of the 107.06s of remaining time.
	-54.4565	 = Validation score   (-root_mean_squared_error)
	2.58s	 = Training   runtime
	0.38s	 = Validation runtime
Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 103.9s of the 103.82s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.38%)
2024-04-28 18:49:47,713	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:49:47,714	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:49:47,715	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:49:47,720	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): ray::_ray_fit() (pid=24636, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=24636, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
	-51.6381	 = Validation score   (-root_mean_squared_error)
	9.42s	 = Training   runtime
	0.12s	 = Validation runtime
Fitting model: XGBoost_BAG_L2 ... Training model for up to 92.56s of the 92.49s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.62%)
	-55.2956	 = Validation score   (-root_mean_squared_error)
	2.99s	 = Training   runtime
	0.07s	 = Validation runtime
Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 86.93s of the 86.86s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)
	-55.6522	 = Validation score   (-root_mean_squared_error)
	28.03s	 = Training   runtime
	0.09s	 = Validation runtime
Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 56.99s of the 56.91s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.64%)
	-55.6882	 = Validation score   (-root_mean_squared_error)
	4.14s	 = Training   runtime
	0.5s	 = Validation runtime
Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 50.73s of the 50.66s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.44%)
	Warning: Exception caused CatBoost_r177_BAG_L2 to fail during training (ImportError)... Skipping this model.
		ray::_ray_fit() (pid=24955, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=24955, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 48.81s of the 48.73s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.19%)
2024-04-28 18:50:45,876	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:50:45,879	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:50:45,880	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:50:45,881	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:50:45,882	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:50:45,883	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 18:50:45,884	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
	-52.0099	 = Validation score   (-root_mean_squared_error)
	40.59s	 = Training   runtime
	0.15s	 = Validation runtime
Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 6.28s of the 6.2s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.40%)
	-55.7988	 = Validation score   (-root_mean_squared_error)
	5.9s	 = Training   runtime
	6.22s	 = Validation runtime
Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -8.58s of remaining time.
	Ensemble Weights: {&#39;NeuralNetFastAI_BAG_L2&#39;: 0.333, &#39;NeuralNetTorch_r79_BAG_L2&#39;: 0.333, &#39;RandomForestMSE_BAG_L2&#39;: 0.2, &#39;ExtraTreesMSE_BAG_L2&#39;: 0.067, &#39;NeuralNetTorch_BAG_L2&#39;: 0.067}
	-50.0515	 = Validation score   (-root_mean_squared_error)
	0.05s	 = Training   runtime
	0.0s	 = Validation runtime
AutoGluon training complete, total runtime = 443.65s ... Best model: &quot;WeightedEnsemble_L3&quot;
TabularPredictor saved. To load, use: predictor = TabularPredictor.load(&quot;AutogluonModels/ag-20240428_174128&quot;)
</code></pre>
</div>
<div class="output execute_result" data-execution_count="24">
<pre><code>&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x285358b50&gt;</code></pre>
</div>
</div>
<section
id="review-autogluons-training-run-with-ranking-of-models-that-did-the-best"
class="cell markdown">
<h3>Review AutoGluon's training run with ranking of models that did the
best.</h3>
</section>
<div class="cell code" data-execution_count="25">
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>predictor.fit_summary()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>*** Summary of fit() ***
Estimated performance of each model:
                          model   score_val              eval_metric  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order
0           WeightedEnsemble_L3  -50.051484  root_mean_squared_error      37.053862  332.859654                0.000278           0.050217            3       True         32
1        NeuralNetFastAI_BAG_L2  -51.638098  root_mean_squared_error      36.055013  249.174486                0.115703           9.424196            2       True         26
2     NeuralNetTorch_r79_BAG_L2  -52.009929  root_mean_squared_error      36.092029  280.341284                0.152719          40.590994            2       True         30
3        RandomForestMSE_BAG_L2  -53.578125  root_mean_squared_error      36.318207  252.175677                0.378897          12.425387            2       True         24
4          ExtraTreesMSE_BAG_L2  -54.456478  root_mean_squared_error      36.319005  242.333970                0.379695           2.583680            2       True         25
5               LightGBM_BAG_L2  -55.190664  root_mean_squared_error      36.403526  241.778615                0.464216           2.028325            2       True         23
6                XGBoost_BAG_L2  -55.295586  root_mean_squared_error      36.005630  242.736152                0.066320           2.985862            2       True         27
7         NeuralNetTorch_BAG_L2  -55.652235  root_mean_squared_error      36.026571  267.785180                0.087261          28.034890            2       True         28
8          LightGBMLarge_BAG_L2  -55.688152  root_mean_squared_error      36.443893  243.889118                0.504583           4.138828            2       True         29
9          LightGBM_r131_BAG_L2  -55.798825  root_mean_squared_error      42.159325  245.653419                6.220015           5.903129            2       True         31
10            LightGBMXT_BAG_L2  -60.486378  root_mean_squared_error      41.243752  247.921954                5.304442           8.171664            2       True         22
11        KNeighborsDist_BAG_L1  -84.146423  root_mean_squared_error       0.016056    0.010608                0.016056           0.010608            1       True          2
12          WeightedEnsemble_L2  -84.146423  root_mean_squared_error       0.016408    0.041622                0.000352           0.031014            2       True         21
13        KNeighborsUnif_BAG_L1 -101.588176  root_mean_squared_error       0.016640    0.011411                0.016640           0.011411            1       True          1
14       RandomForestMSE_BAG_L1 -116.543896  root_mean_squared_error       0.313704    2.515439                0.313704           2.515439            1       True          5
15     RandomForest_r195_BAG_L1 -116.794965  root_mean_squared_error       0.310778    1.997757                0.310778           1.997757            1       True         19
16        ExtraTrees_r42_BAG_L1 -123.615395  root_mean_squared_error       0.309332    0.749629                0.309332           0.749629            1       True         17
17         ExtraTreesMSE_BAG_L1 -124.587823  root_mean_squared_error       0.313365    0.939744                0.313365           0.939744            1       True          6
18           XGBoost_r33_BAG_L1 -129.567325  root_mean_squared_error       0.598168    3.905872                0.598168           3.905872            1       True         16
19         LightGBM_r131_BAG_L1 -129.862938  root_mean_squared_error       4.234677    4.884851                4.234677           4.884851            1       True         12
20         LightGBMLarge_BAG_L1 -130.132290  root_mean_squared_error       0.731242    2.046686                0.731242           2.046686            1       True         10
21         LightGBM_r188_BAG_L1 -130.384240  root_mean_squared_error      11.140976    7.466395               11.140976           7.466395            1       True         20
22              LightGBM_BAG_L1 -131.054162  root_mean_squared_error       0.991803    1.657133                0.991803           1.657133            1       True          4
23            LightGBMXT_BAG_L1 -131.460909  root_mean_squared_error       4.567910   10.380445                4.567910          10.380445            1       True          3
24               XGBoost_BAG_L1 -131.893935  root_mean_squared_error       0.238807    1.536159                0.238807           1.536159            1       True          8
25          LightGBM_r96_BAG_L1 -134.529206  root_mean_squared_error      11.390560   10.985263               11.390560          10.985263            1       True         14
26  NeuralNetFastAI_r191_BAG_L1 -134.717893  root_mean_squared_error       0.185012   44.447444                0.185012          44.447444            1       True         13
27       NeuralNetFastAI_BAG_L1 -135.945469  root_mean_squared_error       0.091512    8.842776                0.091512           8.842776            1       True          7
28  NeuralNetFastAI_r102_BAG_L1 -138.145579  root_mean_squared_error       0.124373    8.563601                0.124373           8.563601            1       True         18
29        NeuralNetTorch_BAG_L1 -138.360743  root_mean_squared_error       0.109538   40.229308                0.109538          40.229308            1       True          9
30    NeuralNetTorch_r22_BAG_L1 -139.174689  root_mean_squared_error       0.162762   52.553388                0.162762          52.553388            1       True         15
31    NeuralNetTorch_r79_BAG_L1 -140.314613  root_mean_squared_error       0.092095   36.026380                0.092095          36.026380            1       True         11
Number of models trained: 32
Types of models trained:
{&#39;StackerEnsembleModel_KNN&#39;, &#39;StackerEnsembleModel_XGBoost&#39;, &#39;StackerEnsembleModel_RF&#39;, &#39;StackerEnsembleModel_TabularNeuralNetTorch&#39;, &#39;StackerEnsembleModel_XT&#39;, &#39;StackerEnsembleModel_LGB&#39;, &#39;WeightedEnsembleModel&#39;, &#39;StackerEnsembleModel_NNFastAiTabular&#39;}
Bagging used: True  (with 8 folds)
Multi-layer stack-ensembling used: True  (with 3 levels)
Feature Metadata (Processed):
(raw dtype, special dtypes):
(&#39;float&#39;, [])                : 3 | [&#39;temp&#39;, &#39;atemp&#39;, &#39;windspeed&#39;]
(&#39;int&#39;, [])                  : 3 | [&#39;season&#39;, &#39;weather&#39;, &#39;humidity&#39;]
(&#39;int&#39;, [&#39;bool&#39;])            : 2 | [&#39;holiday&#39;, &#39;workingday&#39;]
(&#39;int&#39;, [&#39;datetime_as_int&#39;]) : 5 | [&#39;datetime&#39;, &#39;datetime.year&#39;, &#39;datetime.month&#39;, &#39;datetime.day&#39;, &#39;datetime.dayofweek&#39;]
*** End of fit() summary ***
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>/opt/homebrew/lib/python3.11/site-packages/autogluon/core/utils/plots.py:169: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: &quot;pip install bokeh==2.0.1&quot;
  warnings.warn(&#39;AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: &quot;pip install bokeh==2.0.1&quot;&#39;)
</code></pre>
</div>
<div class="output execute_result" data-execution_count="25">
<pre><code>{&#39;model_types&#39;: {&#39;KNeighborsUnif_BAG_L1&#39;: &#39;StackerEnsembleModel_KNN&#39;,
  &#39;KNeighborsDist_BAG_L1&#39;: &#39;StackerEnsembleModel_KNN&#39;,
  &#39;LightGBMXT_BAG_L1&#39;: &#39;StackerEnsembleModel_LGB&#39;,
  &#39;LightGBM_BAG_L1&#39;: &#39;StackerEnsembleModel_LGB&#39;,
  &#39;RandomForestMSE_BAG_L1&#39;: &#39;StackerEnsembleModel_RF&#39;,
  &#39;ExtraTreesMSE_BAG_L1&#39;: &#39;StackerEnsembleModel_XT&#39;,
  &#39;NeuralNetFastAI_BAG_L1&#39;: &#39;StackerEnsembleModel_NNFastAiTabular&#39;,
  &#39;XGBoost_BAG_L1&#39;: &#39;StackerEnsembleModel_XGBoost&#39;,
  &#39;NeuralNetTorch_BAG_L1&#39;: &#39;StackerEnsembleModel_TabularNeuralNetTorch&#39;,
  &#39;LightGBMLarge_BAG_L1&#39;: &#39;StackerEnsembleModel_LGB&#39;,
  &#39;NeuralNetTorch_r79_BAG_L1&#39;: &#39;StackerEnsembleModel_TabularNeuralNetTorch&#39;,
  &#39;LightGBM_r131_BAG_L1&#39;: &#39;StackerEnsembleModel_LGB&#39;,
  &#39;NeuralNetFastAI_r191_BAG_L1&#39;: &#39;StackerEnsembleModel_NNFastAiTabular&#39;,
  &#39;LightGBM_r96_BAG_L1&#39;: &#39;StackerEnsembleModel_LGB&#39;,
  &#39;NeuralNetTorch_r22_BAG_L1&#39;: &#39;StackerEnsembleModel_TabularNeuralNetTorch&#39;,
  &#39;XGBoost_r33_BAG_L1&#39;: &#39;StackerEnsembleModel_XGBoost&#39;,
  &#39;ExtraTrees_r42_BAG_L1&#39;: &#39;StackerEnsembleModel_XT&#39;,
  &#39;NeuralNetFastAI_r102_BAG_L1&#39;: &#39;StackerEnsembleModel_NNFastAiTabular&#39;,
  &#39;RandomForest_r195_BAG_L1&#39;: &#39;StackerEnsembleModel_RF&#39;,
  &#39;LightGBM_r188_BAG_L1&#39;: &#39;StackerEnsembleModel_LGB&#39;,
  &#39;WeightedEnsemble_L2&#39;: &#39;WeightedEnsembleModel&#39;,
  &#39;LightGBMXT_BAG_L2&#39;: &#39;StackerEnsembleModel_LGB&#39;,
  &#39;LightGBM_BAG_L2&#39;: &#39;StackerEnsembleModel_LGB&#39;,
  &#39;RandomForestMSE_BAG_L2&#39;: &#39;StackerEnsembleModel_RF&#39;,
  &#39;ExtraTreesMSE_BAG_L2&#39;: &#39;StackerEnsembleModel_XT&#39;,
  &#39;NeuralNetFastAI_BAG_L2&#39;: &#39;StackerEnsembleModel_NNFastAiTabular&#39;,
  &#39;XGBoost_BAG_L2&#39;: &#39;StackerEnsembleModel_XGBoost&#39;,
  &#39;NeuralNetTorch_BAG_L2&#39;: &#39;StackerEnsembleModel_TabularNeuralNetTorch&#39;,
  &#39;LightGBMLarge_BAG_L2&#39;: &#39;StackerEnsembleModel_LGB&#39;,
  &#39;NeuralNetTorch_r79_BAG_L2&#39;: &#39;StackerEnsembleModel_TabularNeuralNetTorch&#39;,
  &#39;LightGBM_r131_BAG_L2&#39;: &#39;StackerEnsembleModel_LGB&#39;,
  &#39;WeightedEnsemble_L3&#39;: &#39;WeightedEnsembleModel&#39;},
 &#39;model_performance&#39;: {&#39;KNeighborsUnif_BAG_L1&#39;: -101.58817625927213,
  &#39;KNeighborsDist_BAG_L1&#39;: -84.14642264302962,
  &#39;LightGBMXT_BAG_L1&#39;: -131.46090891834504,
  &#39;LightGBM_BAG_L1&#39;: -131.054161598899,
  &#39;RandomForestMSE_BAG_L1&#39;: -116.54389626190594,
  &#39;ExtraTreesMSE_BAG_L1&#39;: -124.58782321399028,
  &#39;NeuralNetFastAI_BAG_L1&#39;: -135.94546905391869,
  &#39;XGBoost_BAG_L1&#39;: -131.89393473529245,
  &#39;NeuralNetTorch_BAG_L1&#39;: -138.3607427426411,
  &#39;LightGBMLarge_BAG_L1&#39;: -130.13228993716103,
  &#39;NeuralNetTorch_r79_BAG_L1&#39;: -140.31461268775865,
  &#39;LightGBM_r131_BAG_L1&#39;: -129.86293820154293,
  &#39;NeuralNetFastAI_r191_BAG_L1&#39;: -134.7178925604412,
  &#39;LightGBM_r96_BAG_L1&#39;: -134.5292055395193,
  &#39;NeuralNetTorch_r22_BAG_L1&#39;: -139.17468897071913,
  &#39;XGBoost_r33_BAG_L1&#39;: -129.56732524632403,
  &#39;ExtraTrees_r42_BAG_L1&#39;: -123.61539457004514,
  &#39;NeuralNetFastAI_r102_BAG_L1&#39;: -138.14557870820497,
  &#39;RandomForest_r195_BAG_L1&#39;: -116.79496538999467,
  &#39;LightGBM_r188_BAG_L1&#39;: -130.3842399824061,
  &#39;WeightedEnsemble_L2&#39;: -84.14642264302962,
  &#39;LightGBMXT_BAG_L2&#39;: -60.48637816876633,
  &#39;LightGBM_BAG_L2&#39;: -55.19066401035075,
  &#39;RandomForestMSE_BAG_L2&#39;: -53.57812508638611,
  &#39;ExtraTreesMSE_BAG_L2&#39;: -54.45647816084044,
  &#39;NeuralNetFastAI_BAG_L2&#39;: -51.63809817154404,
  &#39;XGBoost_BAG_L2&#39;: -55.29558637253587,
  &#39;NeuralNetTorch_BAG_L2&#39;: -55.65223508461118,
  &#39;LightGBMLarge_BAG_L2&#39;: -55.688152290058106,
  &#39;NeuralNetTorch_r79_BAG_L2&#39;: -52.00992910112898,
  &#39;LightGBM_r131_BAG_L2&#39;: -55.798825269640126,
  &#39;WeightedEnsemble_L3&#39;: -50.051483627493404},
 &#39;model_best&#39;: &#39;WeightedEnsemble_L3&#39;,
 &#39;model_paths&#39;: {&#39;KNeighborsUnif_BAG_L1&#39;: [&#39;KNeighborsUnif_BAG_L1&#39;],
  &#39;KNeighborsDist_BAG_L1&#39;: [&#39;KNeighborsDist_BAG_L1&#39;],
  &#39;LightGBMXT_BAG_L1&#39;: [&#39;LightGBMXT_BAG_L1&#39;],
  &#39;LightGBM_BAG_L1&#39;: [&#39;LightGBM_BAG_L1&#39;],
  &#39;RandomForestMSE_BAG_L1&#39;: [&#39;RandomForestMSE_BAG_L1&#39;],
  &#39;ExtraTreesMSE_BAG_L1&#39;: [&#39;ExtraTreesMSE_BAG_L1&#39;],
  &#39;NeuralNetFastAI_BAG_L1&#39;: [&#39;NeuralNetFastAI_BAG_L1&#39;],
  &#39;XGBoost_BAG_L1&#39;: [&#39;XGBoost_BAG_L1&#39;],
  &#39;NeuralNetTorch_BAG_L1&#39;: [&#39;NeuralNetTorch_BAG_L1&#39;],
  &#39;LightGBMLarge_BAG_L1&#39;: [&#39;LightGBMLarge_BAG_L1&#39;],
  &#39;NeuralNetTorch_r79_BAG_L1&#39;: [&#39;NeuralNetTorch_r79_BAG_L1&#39;],
  &#39;LightGBM_r131_BAG_L1&#39;: [&#39;LightGBM_r131_BAG_L1&#39;],
  &#39;NeuralNetFastAI_r191_BAG_L1&#39;: [&#39;NeuralNetFastAI_r191_BAG_L1&#39;],
  &#39;LightGBM_r96_BAG_L1&#39;: [&#39;LightGBM_r96_BAG_L1&#39;],
  &#39;NeuralNetTorch_r22_BAG_L1&#39;: [&#39;NeuralNetTorch_r22_BAG_L1&#39;],
  &#39;XGBoost_r33_BAG_L1&#39;: [&#39;XGBoost_r33_BAG_L1&#39;],
  &#39;ExtraTrees_r42_BAG_L1&#39;: [&#39;ExtraTrees_r42_BAG_L1&#39;],
  &#39;NeuralNetFastAI_r102_BAG_L1&#39;: [&#39;NeuralNetFastAI_r102_BAG_L1&#39;],
  &#39;RandomForest_r195_BAG_L1&#39;: [&#39;RandomForest_r195_BAG_L1&#39;],
  &#39;LightGBM_r188_BAG_L1&#39;: [&#39;LightGBM_r188_BAG_L1&#39;],
  &#39;WeightedEnsemble_L2&#39;: [&#39;WeightedEnsemble_L2&#39;],
  &#39;LightGBMXT_BAG_L2&#39;: [&#39;LightGBMXT_BAG_L2&#39;],
  &#39;LightGBM_BAG_L2&#39;: [&#39;LightGBM_BAG_L2&#39;],
  &#39;RandomForestMSE_BAG_L2&#39;: [&#39;RandomForestMSE_BAG_L2&#39;],
  &#39;ExtraTreesMSE_BAG_L2&#39;: [&#39;ExtraTreesMSE_BAG_L2&#39;],
  &#39;NeuralNetFastAI_BAG_L2&#39;: [&#39;NeuralNetFastAI_BAG_L2&#39;],
  &#39;XGBoost_BAG_L2&#39;: [&#39;XGBoost_BAG_L2&#39;],
  &#39;NeuralNetTorch_BAG_L2&#39;: [&#39;NeuralNetTorch_BAG_L2&#39;],
  &#39;LightGBMLarge_BAG_L2&#39;: [&#39;LightGBMLarge_BAG_L2&#39;],
  &#39;NeuralNetTorch_r79_BAG_L2&#39;: [&#39;NeuralNetTorch_r79_BAG_L2&#39;],
  &#39;LightGBM_r131_BAG_L2&#39;: [&#39;LightGBM_r131_BAG_L2&#39;],
  &#39;WeightedEnsemble_L3&#39;: [&#39;WeightedEnsemble_L3&#39;]},
 &#39;model_fit_times&#39;: {&#39;KNeighborsUnif_BAG_L1&#39;: 0.011410951614379883,
  &#39;KNeighborsDist_BAG_L1&#39;: 0.01060795783996582,
  &#39;LightGBMXT_BAG_L1&#39;: 10.3804452419281,
  &#39;LightGBM_BAG_L1&#39;: 1.6571331024169922,
  &#39;RandomForestMSE_BAG_L1&#39;: 2.515439033508301,
  &#39;ExtraTreesMSE_BAG_L1&#39;: 0.9397439956665039,
  &#39;NeuralNetFastAI_BAG_L1&#39;: 8.84277606010437,
  &#39;XGBoost_BAG_L1&#39;: 1.5361590385437012,
  &#39;NeuralNetTorch_BAG_L1&#39;: 40.229307889938354,
  &#39;LightGBMLarge_BAG_L1&#39;: 2.0466861724853516,
  &#39;NeuralNetTorch_r79_BAG_L1&#39;: 36.02638006210327,
  &#39;LightGBM_r131_BAG_L1&#39;: 4.884850978851318,
  &#39;NeuralNetFastAI_r191_BAG_L1&#39;: 44.44744396209717,
  &#39;LightGBM_r96_BAG_L1&#39;: 10.985263109207153,
  &#39;NeuralNetTorch_r22_BAG_L1&#39;: 52.5533881187439,
  &#39;XGBoost_r33_BAG_L1&#39;: 3.905872344970703,
  &#39;ExtraTrees_r42_BAG_L1&#39;: 0.749629020690918,
  &#39;NeuralNetFastAI_r102_BAG_L1&#39;: 8.563601016998291,
  &#39;RandomForest_r195_BAG_L1&#39;: 1.9977569580078125,
  &#39;LightGBM_r188_BAG_L1&#39;: 7.466394901275635,
  &#39;WeightedEnsemble_L2&#39;: 0.03101372718811035,
  &#39;LightGBMXT_BAG_L2&#39;: 8.171663761138916,
  &#39;LightGBM_BAG_L2&#39;: 2.028325080871582,
  &#39;RandomForestMSE_BAG_L2&#39;: 12.425387144088745,
  &#39;ExtraTreesMSE_BAG_L2&#39;: 2.5836801528930664,
  &#39;NeuralNetFastAI_BAG_L2&#39;: 9.424196243286133,
  &#39;XGBoost_BAG_L2&#39;: 2.9858620166778564,
  &#39;NeuralNetTorch_BAG_L2&#39;: 28.034889936447144,
  &#39;LightGBMLarge_BAG_L2&#39;: 4.1388280391693115,
  &#39;NeuralNetTorch_r79_BAG_L2&#39;: 40.590993881225586,
  &#39;LightGBM_r131_BAG_L2&#39;: 5.9031291007995605,
  &#39;WeightedEnsemble_L3&#39;: 0.0502169132232666},
 &#39;model_pred_times&#39;: {&#39;KNeighborsUnif_BAG_L1&#39;: 0.01663994789123535,
  &#39;KNeighborsDist_BAG_L1&#39;: 0.016056060791015625,
  &#39;LightGBMXT_BAG_L1&#39;: 4.5679099559783936,
  &#39;LightGBM_BAG_L1&#39;: 0.9918029308319092,
  &#39;RandomForestMSE_BAG_L1&#39;: 0.3137040138244629,
  &#39;ExtraTreesMSE_BAG_L1&#39;: 0.31336498260498047,
  &#39;NeuralNetFastAI_BAG_L1&#39;: 0.09151244163513184,
  &#39;XGBoost_BAG_L1&#39;: 0.23880672454833984,
  &#39;NeuralNetTorch_BAG_L1&#39;: 0.10953760147094727,
  &#39;LightGBMLarge_BAG_L1&#39;: 0.7312421798706055,
  &#39;NeuralNetTorch_r79_BAG_L1&#39;: 0.09209537506103516,
  &#39;LightGBM_r131_BAG_L1&#39;: 4.234677314758301,
  &#39;NeuralNetFastAI_r191_BAG_L1&#39;: 0.1850121021270752,
  &#39;LightGBM_r96_BAG_L1&#39;: 11.390559673309326,
  &#39;NeuralNetTorch_r22_BAG_L1&#39;: 0.16276168823242188,
  &#39;XGBoost_r33_BAG_L1&#39;: 0.598167896270752,
  &#39;ExtraTrees_r42_BAG_L1&#39;: 0.30933189392089844,
  &#39;NeuralNetFastAI_r102_BAG_L1&#39;: 0.12437319755554199,
  &#39;RandomForest_r195_BAG_L1&#39;: 0.3107781410217285,
  &#39;LightGBM_r188_BAG_L1&#39;: 11.140975952148438,
  &#39;WeightedEnsemble_L2&#39;: 0.00035190582275390625,
  &#39;LightGBMXT_BAG_L2&#39;: 5.304441928863525,
  &#39;LightGBM_BAG_L2&#39;: 0.4642159938812256,
  &#39;RandomForestMSE_BAG_L2&#39;: 0.37889719009399414,
  &#39;ExtraTreesMSE_BAG_L2&#39;: 0.37969493865966797,
  &#39;NeuralNetFastAI_BAG_L2&#39;: 0.11570262908935547,
  &#39;XGBoost_BAG_L2&#39;: 0.06632041931152344,
  &#39;NeuralNetTorch_BAG_L2&#39;: 0.08726096153259277,
  &#39;LightGBMLarge_BAG_L2&#39;: 0.5045833587646484,
  &#39;NeuralNetTorch_r79_BAG_L2&#39;: 0.15271854400634766,
  &#39;LightGBM_r131_BAG_L2&#39;: 6.220014810562134,
  &#39;WeightedEnsemble_L3&#39;: 0.0002779960632324219},
 &#39;num_bag_folds&#39;: 8,
 &#39;max_stack_level&#39;: 3,
 &#39;model_hyperparams&#39;: {&#39;KNeighborsUnif_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True,
   &#39;use_child_oof&#39;: True},
  &#39;KNeighborsDist_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True,
   &#39;use_child_oof&#39;: True},
  &#39;LightGBMXT_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;LightGBM_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;RandomForestMSE_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True,
   &#39;use_child_oof&#39;: True},
  &#39;ExtraTreesMSE_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True,
   &#39;use_child_oof&#39;: True},
  &#39;NeuralNetFastAI_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;XGBoost_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;NeuralNetTorch_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;LightGBMLarge_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;NeuralNetTorch_r79_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;LightGBM_r131_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;NeuralNetFastAI_r191_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;LightGBM_r96_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;NeuralNetTorch_r22_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;XGBoost_r33_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;ExtraTrees_r42_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True,
   &#39;use_child_oof&#39;: True},
  &#39;NeuralNetFastAI_r102_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;RandomForest_r195_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True,
   &#39;use_child_oof&#39;: True},
  &#39;LightGBM_r188_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;WeightedEnsemble_L2&#39;: {&#39;use_orig_features&#39;: False,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;LightGBMXT_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;LightGBM_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;RandomForestMSE_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True,
   &#39;use_child_oof&#39;: True},
  &#39;ExtraTreesMSE_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True,
   &#39;use_child_oof&#39;: True},
  &#39;NeuralNetFastAI_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;XGBoost_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;NeuralNetTorch_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;LightGBMLarge_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;NeuralNetTorch_r79_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;LightGBM_r131_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;WeightedEnsemble_L3&#39;: {&#39;use_orig_features&#39;: False,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True}},
 &#39;leaderboard&#39;:                           model   score_val              eval_metric  \
 0           WeightedEnsemble_L3  -50.051484  root_mean_squared_error   
 1        NeuralNetFastAI_BAG_L2  -51.638098  root_mean_squared_error   
 2     NeuralNetTorch_r79_BAG_L2  -52.009929  root_mean_squared_error   
 3        RandomForestMSE_BAG_L2  -53.578125  root_mean_squared_error   
 4          ExtraTreesMSE_BAG_L2  -54.456478  root_mean_squared_error   
 5               LightGBM_BAG_L2  -55.190664  root_mean_squared_error   
 6                XGBoost_BAG_L2  -55.295586  root_mean_squared_error   
 7         NeuralNetTorch_BAG_L2  -55.652235  root_mean_squared_error   
 8          LightGBMLarge_BAG_L2  -55.688152  root_mean_squared_error   
 9          LightGBM_r131_BAG_L2  -55.798825  root_mean_squared_error   
 10            LightGBMXT_BAG_L2  -60.486378  root_mean_squared_error   
 11        KNeighborsDist_BAG_L1  -84.146423  root_mean_squared_error   
 12          WeightedEnsemble_L2  -84.146423  root_mean_squared_error   
 13        KNeighborsUnif_BAG_L1 -101.588176  root_mean_squared_error   
 14       RandomForestMSE_BAG_L1 -116.543896  root_mean_squared_error   
 15     RandomForest_r195_BAG_L1 -116.794965  root_mean_squared_error   
 16        ExtraTrees_r42_BAG_L1 -123.615395  root_mean_squared_error   
 17         ExtraTreesMSE_BAG_L1 -124.587823  root_mean_squared_error   
 18           XGBoost_r33_BAG_L1 -129.567325  root_mean_squared_error   
 19         LightGBM_r131_BAG_L1 -129.862938  root_mean_squared_error   
 20         LightGBMLarge_BAG_L1 -130.132290  root_mean_squared_error   
 21         LightGBM_r188_BAG_L1 -130.384240  root_mean_squared_error   
 22              LightGBM_BAG_L1 -131.054162  root_mean_squared_error   
 23            LightGBMXT_BAG_L1 -131.460909  root_mean_squared_error   
 24               XGBoost_BAG_L1 -131.893935  root_mean_squared_error   
 25          LightGBM_r96_BAG_L1 -134.529206  root_mean_squared_error   
 26  NeuralNetFastAI_r191_BAG_L1 -134.717893  root_mean_squared_error   
 27       NeuralNetFastAI_BAG_L1 -135.945469  root_mean_squared_error   
 28  NeuralNetFastAI_r102_BAG_L1 -138.145579  root_mean_squared_error   
 29        NeuralNetTorch_BAG_L1 -138.360743  root_mean_squared_error   
 30    NeuralNetTorch_r22_BAG_L1 -139.174689  root_mean_squared_error   
 31    NeuralNetTorch_r79_BAG_L1 -140.314613  root_mean_squared_error   
 
     pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  \
 0       37.053862  332.859654                0.000278           0.050217   
 1       36.055013  249.174486                0.115703           9.424196   
 2       36.092029  280.341284                0.152719          40.590994   
 3       36.318207  252.175677                0.378897          12.425387   
 4       36.319005  242.333970                0.379695           2.583680   
 5       36.403526  241.778615                0.464216           2.028325   
 6       36.005630  242.736152                0.066320           2.985862   
 7       36.026571  267.785180                0.087261          28.034890   
 8       36.443893  243.889118                0.504583           4.138828   
 9       42.159325  245.653419                6.220015           5.903129   
 10      41.243752  247.921954                5.304442           8.171664   
 11       0.016056    0.010608                0.016056           0.010608   
 12       0.016408    0.041622                0.000352           0.031014   
 13       0.016640    0.011411                0.016640           0.011411   
 14       0.313704    2.515439                0.313704           2.515439   
 15       0.310778    1.997757                0.310778           1.997757   
 16       0.309332    0.749629                0.309332           0.749629   
 17       0.313365    0.939744                0.313365           0.939744   
 18       0.598168    3.905872                0.598168           3.905872   
 19       4.234677    4.884851                4.234677           4.884851   
 20       0.731242    2.046686                0.731242           2.046686   
 21      11.140976    7.466395               11.140976           7.466395   
 22       0.991803    1.657133                0.991803           1.657133   
 23       4.567910   10.380445                4.567910          10.380445   
 24       0.238807    1.536159                0.238807           1.536159   
 25      11.390560   10.985263               11.390560          10.985263   
 26       0.185012   44.447444                0.185012          44.447444   
 27       0.091512    8.842776                0.091512           8.842776   
 28       0.124373    8.563601                0.124373           8.563601   
 29       0.109538   40.229308                0.109538          40.229308   
 30       0.162762   52.553388                0.162762          52.553388   
 31       0.092095   36.026380                0.092095          36.026380   
 
     stack_level  can_infer  fit_order  
 0             3       True         32  
 1             2       True         26  
 2             2       True         30  
 3             2       True         24  
 4             2       True         25  
 5             2       True         23  
 6             2       True         27  
 7             2       True         28  
 8             2       True         29  
 9             2       True         31  
 10            2       True         22  
 11            1       True          2  
 12            2       True         21  
 13            1       True          1  
 14            1       True          5  
 15            1       True         19  
 16            1       True         17  
 17            1       True          6  
 18            1       True         16  
 19            1       True         12  
 20            1       True         10  
 21            1       True         20  
 22            1       True          4  
 23            1       True          3  
 24            1       True          8  
 25            1       True         14  
 26            1       True         13  
 27            1       True          7  
 28            1       True         18  
 29            1       True          9  
 30            1       True         15  
 31            1       True         11  }</code></pre>
</div>
</div>
<section id="create-predictions-from-test-dataset"
class="cell markdown">
<h3>Create predictions from test dataset</h3>
</section>
<div class="cell code" data-execution_count="26">
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> predictor.predict(test)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> {<span class="st">&#39;datetime&#39;</span>: test[<span class="st">&#39;datetime&#39;</span>], <span class="st">&#39;Pred_count&#39;</span>: predictions}</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>predictions)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>predictions.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="26">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>datetime</th>
      <th>Pred_count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2011-01-20 00:00:00</td>
      <td>27.299713</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2011-01-20 01:00:00</td>
      <td>40.786640</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2011-01-20 02:00:00</td>
      <td>46.426559</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2011-01-20 03:00:00</td>
      <td>50.377052</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2011-01-20 04:00:00</td>
      <td>52.754913</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<section
id="note-kaggle-will-reject-the-submission-if-we-dont-set-everything-to-be--0"
class="cell markdown">
<h4>NOTE: Kaggle will reject the submission if we don't set everything
to be &gt; 0.</h4>
</section>
<div class="cell code" data-execution_count="27">
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Describe the `predictions` series to see if there are any negative values</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>predictions.describe()</span></code></pre></div>
<div class="output execute_result" data-execution_count="27">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pred_count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>6493.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>99.430008</td>
    </tr>
    <tr>
      <th>std</th>
      <td>89.633232</td>
    </tr>
    <tr>
      <th>min</th>
      <td>3.349782</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>18.217186</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>63.441143</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>171.790588</td>
    </tr>
    <tr>
      <th>max</th>
      <td>353.889343</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell code" data-execution_count="38">
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># How many negative values do we have?</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>negative_counts <span class="op">=</span> predictions.groupby(predictions[<span class="st">&#39;Pred_count&#39;</span>])</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> minus(value):</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (value <span class="op">&lt;</span> <span class="dv">0</span>).<span class="bu">sum</span>()</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(negative_counts[<span class="st">&#39;Pred_count&#39;</span>].agg([(<span class="st">&#39;negative_counts&#39;</span>, minus)]))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>            negative_counts
Pred_count                 
3.349782                  0
3.584929                  0
3.585921                  0
3.636454                  0
3.651541                  0
...                     ...
352.617554                0
353.099640                0
353.335693                0
353.416168                0
353.889343                0

[6493 rows x 1 columns]
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="39">
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set them to zero</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>predictions[<span class="st">&#39;Pred_count&#39;</span>] <span class="op">=</span> predictions[<span class="st">&#39;Pred_count&#39;</span>].clip(lower<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="40">
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>predictions.describe()</span></code></pre></div>
<div class="output execute_result" data-execution_count="40">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pred_count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>6493.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>99.430008</td>
    </tr>
    <tr>
      <th>std</th>
      <td>89.633232</td>
    </tr>
    <tr>
      <th>min</th>
      <td>3.349782</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>18.217186</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>63.441143</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>171.790588</td>
    </tr>
    <tr>
      <th>max</th>
      <td>353.889343</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell code" data-execution_count="41">
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>predictions.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="41">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>datetime</th>
      <th>Pred_count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2011-01-20 00:00:00</td>
      <td>27.299713</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2011-01-20 01:00:00</td>
      <td>40.786640</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2011-01-20 02:00:00</td>
      <td>46.426559</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2011-01-20 03:00:00</td>
      <td>50.377052</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2011-01-20 04:00:00</td>
      <td>52.754913</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<section id="set-predictions-to-submission-dataframe-save-and-submit"
class="cell markdown">
<h3>Set predictions to submission dataframe, save, and submit</h3>
</section>
<div class="cell code" data-execution_count="42">
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>submission[<span class="st">&quot;count&quot;</span>] <span class="op">=</span> predictions[<span class="st">&quot;Pred_count&quot;</span>]</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>submission.to_csv(<span class="st">&quot;submission.csv&quot;</span>, index<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="43">
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>kaggle competitions submit <span class="op">-</span>c bike<span class="op">-</span>sharing<span class="op">-</span>demand <span class="op">-</span>f submission.csv <span class="op">-</span>m <span class="st">&quot;first raw submission&quot;</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>100%|█████████████████████████████████████████| 188k/188k [00:00&lt;00:00, 220kB/s]
Successfully submitted to Bike Sharing Demand</code></pre>
</div>
</div>
<section
id="view-submission-via-the-command-line-or-in-the-web-browser-under-the-competitions-page---my-submissions"
class="cell markdown">
<h4>View submission via the command line or in the web browser under the
competition's page - <code>My Submissions</code></h4>
</section>
<div class="cell code" data-execution_count="44">
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>kaggle competitions submissions <span class="op">-</span>c bike<span class="op">-</span>sharing<span class="op">-</span>demand <span class="op">|</span> tail <span class="op">-</span>n <span class="op">+</span><span class="dv">1</span> <span class="op">|</span> head <span class="op">-</span>n <span class="dv">6</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>fileName        date                 description           status    publicScore  privateScore  
--------------  -------------------  --------------------  --------  -----------  ------------  
submission.csv  2024-04-28 18:04:20  first raw submission  complete  1.83131      1.83131       
</code></pre>
</div>
</div>
<section id="initial-score-of-183131" class="cell markdown">
<h4>Initial score of <code>1.83131</code></h4>
</section>
<section
id="step-4-exploratory-data-analysis-and-creating-an-additional-feature"
class="cell markdown">
<h2>Step 4: Exploratory Data Analysis and Creating an additional
feature</h2>
<ul>
<li>Any additional feature will do, but a great suggestion would be to
separate out the datetime into hour, day, or month parts.</li>
</ul>
</section>
<div class="cell code" data-execution_count="45">
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a histogram of all features to show the distribution of each one relative to the data. This is part of the exploritory data analysis</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>train.hist(bins<span class="op">=</span><span class="dv">50</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">15</span>))</span></code></pre></div>
<div class="output execute_result" data-execution_count="45">
<pre><code>array([[&lt;Axes: title={&#39;center&#39;: &#39;season&#39;}&gt;,
        &lt;Axes: title={&#39;center&#39;: &#39;holiday&#39;}&gt;,
        &lt;Axes: title={&#39;center&#39;: &#39;workingday&#39;}&gt;],
       [&lt;Axes: title={&#39;center&#39;: &#39;weather&#39;}&gt;,
        &lt;Axes: title={&#39;center&#39;: &#39;temp&#39;}&gt;,
        &lt;Axes: title={&#39;center&#39;: &#39;atemp&#39;}&gt;],
       [&lt;Axes: title={&#39;center&#39;: &#39;humidity&#39;}&gt;,
        &lt;Axes: title={&#39;center&#39;: &#39;windspeed&#39;}&gt;,
        &lt;Axes: title={&#39;center&#39;: &#39;casual&#39;}&gt;],
       [&lt;Axes: title={&#39;center&#39;: &#39;registered&#39;}&gt;,
        &lt;Axes: title={&#39;center&#39;: &#39;count&#39;}&gt;, &lt;Axes: &gt;]], dtype=object)</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0be6c1bfb8f540fb92135113ec53c6ab/8797386a2284366c19ee139960d576167866b0e2.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="47">
<div class="sourceCode" id="cb33"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a new feature</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert &#39;datetime&#39; column to datetime format</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>train[<span class="st">&#39;datetime&#39;</span>] <span class="op">=</span> pd.to_datetime(train[<span class="st">&#39;datetime&#39;</span>])</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>test[<span class="st">&#39;datetime&#39;</span>] <span class="op">=</span> pd.to_datetime(test[<span class="st">&#39;datetime&#39;</span>])</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract year, month, day, and hour from &#39;datetime&#39; column</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>train[<span class="st">&#39;year&#39;</span>] <span class="op">=</span> train[<span class="st">&#39;datetime&#39;</span>].dt.year</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>train[<span class="st">&#39;month&#39;</span>] <span class="op">=</span> train[<span class="st">&#39;datetime&#39;</span>].dt.month</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>train[<span class="st">&#39;day&#39;</span>] <span class="op">=</span> train[<span class="st">&#39;datetime&#39;</span>].dt.day</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>train[<span class="st">&#39;hour&#39;</span>] <span class="op">=</span> train[<span class="st">&#39;datetime&#39;</span>].dt.hour</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>test[<span class="st">&#39;year&#39;</span>] <span class="op">=</span> test[<span class="st">&#39;datetime&#39;</span>].dt.year</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>test[<span class="st">&#39;month&#39;</span>] <span class="op">=</span> test[<span class="st">&#39;datetime&#39;</span>].dt.month</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>test[<span class="st">&#39;day&#39;</span>] <span class="op">=</span> test[<span class="st">&#39;datetime&#39;</span>].dt.day</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>test[<span class="st">&#39;hour&#39;</span>] <span class="op">=</span> test[<span class="st">&#39;datetime&#39;</span>].dt.hour</span></code></pre></div>
</div>
<section
id="make-category-types-for-these-so-models-know-they-are-not-just-numbers"
class="cell markdown">
<h2>Make category types for these so models know they are not just
numbers</h2>
<ul>
<li>AutoGluon originally sees these as ints, but in reality they are int
representations of a category.</li>
<li>Setting the dtype to category will classify these as categories in
AutoGluon.</li>
</ul>
</section>
<div class="cell code" data-execution_count="48">
<div class="sourceCode" id="cb34"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>train[<span class="st">&quot;season&quot;</span>] <span class="op">=</span> train[<span class="st">&quot;season&quot;</span>].astype(<span class="st">&quot;category&quot;</span>)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>train[<span class="st">&quot;weather&quot;</span>] <span class="op">=</span> train[<span class="st">&quot;weather&quot;</span>].astype(<span class="st">&quot;category&quot;</span>)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>test[<span class="st">&quot;season&quot;</span>] <span class="op">=</span> test[<span class="st">&quot;season&quot;</span>].astype(<span class="st">&quot;category&quot;</span>)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>test[<span class="st">&quot;weather&quot;</span>] <span class="op">=</span> test[<span class="st">&quot;weather&quot;</span>].astype(<span class="st">&quot;category&quot;</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="49">
<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View are new feature</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>train.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="49">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>datetime</th>
      <th>season</th>
      <th>holiday</th>
      <th>workingday</th>
      <th>weather</th>
      <th>temp</th>
      <th>atemp</th>
      <th>humidity</th>
      <th>windspeed</th>
      <th>casual</th>
      <th>registered</th>
      <th>count</th>
      <th>year</th>
      <th>month</th>
      <th>day</th>
      <th>hour</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2011-01-01 00:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.84</td>
      <td>14.395</td>
      <td>81</td>
      <td>0.0</td>
      <td>3</td>
      <td>13</td>
      <td>16</td>
      <td>2011</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2011-01-01 01:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.02</td>
      <td>13.635</td>
      <td>80</td>
      <td>0.0</td>
      <td>8</td>
      <td>32</td>
      <td>40</td>
      <td>2011</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2011-01-01 02:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.02</td>
      <td>13.635</td>
      <td>80</td>
      <td>0.0</td>
      <td>5</td>
      <td>27</td>
      <td>32</td>
      <td>2011</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2011-01-01 03:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.84</td>
      <td>14.395</td>
      <td>75</td>
      <td>0.0</td>
      <td>3</td>
      <td>10</td>
      <td>13</td>
      <td>2011</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2011-01-01 04:00:00</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>9.84</td>
      <td>14.395</td>
      <td>75</td>
      <td>0.0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>2011</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell code" data-execution_count="50">
<div class="sourceCode" id="cb36"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View histogram of all features again now with the hour feature</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>train.hist(bins<span class="op">=</span><span class="dv">50</span>, figsize<span class="op">=</span>(<span class="dv">20</span>, <span class="dv">15</span>))</span></code></pre></div>
<div class="output execute_result" data-execution_count="50">
<pre><code>array([[&lt;Axes: title={&#39;center&#39;: &#39;datetime&#39;}&gt;,
        &lt;Axes: title={&#39;center&#39;: &#39;holiday&#39;}&gt;,
        &lt;Axes: title={&#39;center&#39;: &#39;workingday&#39;}&gt;,
        &lt;Axes: title={&#39;center&#39;: &#39;temp&#39;}&gt;],
       [&lt;Axes: title={&#39;center&#39;: &#39;atemp&#39;}&gt;,
        &lt;Axes: title={&#39;center&#39;: &#39;humidity&#39;}&gt;,
        &lt;Axes: title={&#39;center&#39;: &#39;windspeed&#39;}&gt;,
        &lt;Axes: title={&#39;center&#39;: &#39;casual&#39;}&gt;],
       [&lt;Axes: title={&#39;center&#39;: &#39;registered&#39;}&gt;,
        &lt;Axes: title={&#39;center&#39;: &#39;count&#39;}&gt;,
        &lt;Axes: title={&#39;center&#39;: &#39;year&#39;}&gt;,
        &lt;Axes: title={&#39;center&#39;: &#39;month&#39;}&gt;],
       [&lt;Axes: title={&#39;center&#39;: &#39;day&#39;}&gt;,
        &lt;Axes: title={&#39;center&#39;: &#39;hour&#39;}&gt;, &lt;Axes: &gt;, &lt;Axes: &gt;]],
      dtype=object)</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_0be6c1bfb8f540fb92135113ec53c6ab/f7f235fca2673ad2993486d4e18dc54cd199eec9.png" /></p>
</div>
</div>
<section
id="step-5-rerun-the-model-with-the-same-settings-as-before-just-with-more-features"
class="cell markdown">
<h2>Step 5: Rerun the model with the same settings as before, just with
more features</h2>
</section>
<div class="cell code" data-execution_count="52">
<div class="sourceCode" id="cb38"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>predictor_new_features <span class="op">=</span> TabularPredictor(label<span class="op">=</span><span class="st">&quot;count&quot;</span>, problem_type<span class="op">=</span><span class="st">&quot;regression&quot;</span>, eval_metric<span class="op">=</span><span class="st">&quot;rmse&quot;</span>).fit(train_data<span class="op">=</span>train.drop(columns<span class="op">=</span>[<span class="st">&quot;casual&quot;</span>, <span class="st">&quot;registered&quot;</span>], axis<span class="op">=</span><span class="dv">1</span>), time_limit<span class="op">=</span><span class="dv">600</span>, presets<span class="op">=</span><span class="st">&quot;best_quality&quot;</span>)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>No path specified. Models will be saved in: &quot;AutogluonModels/ag-20240428_180959&quot;
Presets specified: [&#39;best_quality&#39;]
Setting dynamic_stacking from &#39;auto&#39; to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)
Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1
Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.
Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.
Sub-fit(s) time limit is: 600 seconds.
Starting holdout-based sub-fit for dynamic stacking. Context path is: AutogluonModels/ag-20240428_180959/ds_sub_fit/sub_fit_ho.
Running the sub-fit in a ray process to avoid memory leakage.
Spend 169 seconds for the sub-fit(s) during dynamic stacking.
Time left for full fit of AutoGluon: 431 seconds.
Starting full fit now with num_stack_levels 1.
Beginning AutoGluon training ... Time limit = 431s
AutoGluon will save models to &quot;AutogluonModels/ag-20240428_180959&quot;
=================== System Info ===================
AutoGluon Version:  1.1.0
Python Version:     3.11.5
Operating System:   Darwin
Platform Machine:   arm64
Platform Version:   Darwin Kernel Version 23.3.0: Wed Dec 20 21:30:44 PST 2023; root:xnu-10002.81.5~7/RELEASE_ARM64_T6000
CPU Count:          8
Memory Avail:       4.82 GB / 16.00 GB (30.1%)
Disk Space Avail:   45.83 GB / 460.43 GB (10.0%)
===================================================
Train Data Rows:    10886
Train Data Columns: 13
Label Column:       count
Problem Type:       regression
Preprocessing data ...
Using Feature Generators to preprocess the data ...
Fitting AutoMLPipelineFeatureGenerator...
	Available Memory:                    4930.92 MB
	Train Data (Original)  Memory Usage: 0.77 MB (0.0% of available memory)
	Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.
	Stage 1 Generators:
		Fitting AsTypeFeatureGenerator...
			Note: Converting 3 features to boolean dtype as they only contain 2 unique values.
	Stage 2 Generators:
		Fitting FillNaFeatureGenerator...
	Stage 3 Generators:
		Fitting IdentityFeatureGenerator...
		Fitting CategoryFeatureGenerator...
			Fitting CategoryMemoryMinimizeFeatureGenerator...
		Fitting DatetimeFeatureGenerator...
	Stage 4 Generators:
		Fitting DropUniqueFeatureGenerator...
	Stage 5 Generators:
		Fitting DropDuplicatesFeatureGenerator...
	Types of features in original data (raw dtype, special dtypes):
		(&#39;category&#39;, []) : 2 | [&#39;season&#39;, &#39;weather&#39;]
		(&#39;datetime&#39;, []) : 1 | [&#39;datetime&#39;]
		(&#39;float&#39;, [])    : 3 | [&#39;temp&#39;, &#39;atemp&#39;, &#39;windspeed&#39;]
		(&#39;int&#39;, [])      : 7 | [&#39;holiday&#39;, &#39;workingday&#39;, &#39;humidity&#39;, &#39;year&#39;, &#39;month&#39;, ...]
	Types of features in processed data (raw dtype, special dtypes):
		(&#39;category&#39;, [])             : 2 | [&#39;season&#39;, &#39;weather&#39;]
		(&#39;float&#39;, [])                : 3 | [&#39;temp&#39;, &#39;atemp&#39;, &#39;windspeed&#39;]
		(&#39;int&#39;, [])                  : 4 | [&#39;humidity&#39;, &#39;month&#39;, &#39;day&#39;, &#39;hour&#39;]
		(&#39;int&#39;, [&#39;bool&#39;])            : 3 | [&#39;holiday&#39;, &#39;workingday&#39;, &#39;year&#39;]
		(&#39;int&#39;, [&#39;datetime_as_int&#39;]) : 3 | [&#39;datetime&#39;, &#39;datetime.year&#39;, &#39;datetime.dayofweek&#39;]
	0.2s = Fit runtime
	13 features in original data used to generate 15 features in processed data.
	Train Data (Processed) Memory Usage: 0.76 MB (0.0% of available memory)
Data preprocessing and feature engineering runtime = 0.19s ...
AutoGluon will gauge predictive performance using evaluation metric: &#39;root_mean_squared_error&#39;
	This metric&#39;s sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	To change this, specify the eval_metric parameter of Predictor()
Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.
User-specified model hyperparameters to be fit:
{
	&#39;NN_TORCH&#39;: [{}, {&#39;activation&#39;: &#39;elu&#39;, &#39;dropout_prob&#39;: 0.10077639529843717, &#39;hidden_size&#39;: 108, &#39;learning_rate&#39;: 0.002735937344002146, &#39;num_layers&#39;: 4, &#39;use_batchnorm&#39;: True, &#39;weight_decay&#39;: 1.356433327634438e-12, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;_r79&#39;, &#39;priority&#39;: -2}}, {&#39;activation&#39;: &#39;elu&#39;, &#39;dropout_prob&#39;: 0.11897478034205347, &#39;hidden_size&#39;: 213, &#39;learning_rate&#39;: 0.0010474382260641949, &#39;num_layers&#39;: 4, &#39;use_batchnorm&#39;: False, &#39;weight_decay&#39;: 5.594471067786272e-10, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;_r22&#39;, &#39;priority&#39;: -7}}],
	&#39;GBM&#39;: [{&#39;extra_trees&#39;: True, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;XT&#39;}}, {}, &#39;GBMLarge&#39;],
	&#39;CAT&#39;: [{}, {&#39;depth&#39;: 6, &#39;grow_policy&#39;: &#39;SymmetricTree&#39;, &#39;l2_leaf_reg&#39;: 2.1542798306067823, &#39;learning_rate&#39;: 0.06864209415792857, &#39;max_ctr_complexity&#39;: 4, &#39;one_hot_max_size&#39;: 10, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;_r177&#39;, &#39;priority&#39;: -1}}, {&#39;depth&#39;: 8, &#39;grow_policy&#39;: &#39;Depthwise&#39;, &#39;l2_leaf_reg&#39;: 2.7997999596449104, &#39;learning_rate&#39;: 0.031375015734637225, &#39;max_ctr_complexity&#39;: 2, &#39;one_hot_max_size&#39;: 3, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;_r9&#39;, &#39;priority&#39;: -5}}],
	&#39;XGB&#39;: [{}, {&#39;colsample_bytree&#39;: 0.6917311125174739, &#39;enable_categorical&#39;: False, &#39;learning_rate&#39;: 0.018063876087523967, &#39;max_depth&#39;: 10, &#39;min_child_weight&#39;: 0.6028633586934382, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;_r33&#39;, &#39;priority&#39;: -8}}, {&#39;colsample_bytree&#39;: 0.6628423832084077, &#39;enable_categorical&#39;: False, &#39;learning_rate&#39;: 0.08775715546881824, &#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 0.6294123374222513, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;_r89&#39;, &#39;priority&#39;: -16}}],
	&#39;FASTAI&#39;: [{}, {&#39;bs&#39;: 256, &#39;emb_drop&#39;: 0.5411770367537934, &#39;epochs&#39;: 43, &#39;layers&#39;: [800, 400], &#39;lr&#39;: 0.01519848858318159, &#39;ps&#39;: 0.23782946566604385, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;_r191&#39;, &#39;priority&#39;: -4}}, {&#39;bs&#39;: 2048, &#39;emb_drop&#39;: 0.05070411322605811, &#39;epochs&#39;: 29, &#39;layers&#39;: [200, 100], &#39;lr&#39;: 0.08974235041576624, &#39;ps&#39;: 0.10393466140748028, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;_r102&#39;, &#39;priority&#39;: -11}}],
	&#39;RF&#39;: [{&#39;criterion&#39;: &#39;gini&#39;, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;Gini&#39;, &#39;problem_types&#39;: [&#39;binary&#39;, &#39;multiclass&#39;]}}, {&#39;criterion&#39;: &#39;entropy&#39;, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;Entr&#39;, &#39;problem_types&#39;: [&#39;binary&#39;, &#39;multiclass&#39;]}}, {&#39;criterion&#39;: &#39;squared_error&#39;, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;MSE&#39;, &#39;problem_types&#39;: [&#39;regression&#39;, &#39;quantile&#39;]}}],
	&#39;XT&#39;: [{&#39;criterion&#39;: &#39;gini&#39;, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;Gini&#39;, &#39;problem_types&#39;: [&#39;binary&#39;, &#39;multiclass&#39;]}}, {&#39;criterion&#39;: &#39;entropy&#39;, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;Entr&#39;, &#39;problem_types&#39;: [&#39;binary&#39;, &#39;multiclass&#39;]}}, {&#39;criterion&#39;: &#39;squared_error&#39;, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;MSE&#39;, &#39;problem_types&#39;: [&#39;regression&#39;, &#39;quantile&#39;]}}],
	&#39;KNN&#39;: [{&#39;weights&#39;: &#39;uniform&#39;, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;Unif&#39;}}, {&#39;weights&#39;: &#39;distance&#39;, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;Dist&#39;}}],
}
AutoGluon will fit 2 stack levels (L1 to L2) ...
Fitting 108 L1 models ...
Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 287.13s of the 430.8s of remaining time.
	-101.5882	 = Validation score   (-root_mean_squared_error)
	0.01s	 = Training   runtime
	0.02s	 = Validation runtime
Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 287.08s of the 430.75s of remaining time.
	-84.1464	 = Validation score   (-root_mean_squared_error)
	0.01s	 = Training   runtime
	0.02s	 = Validation runtime
Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 287.03s of the 430.7s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.15%)
	-34.4539	 = Validation score   (-root_mean_squared_error)
	7.47s	 = Training   runtime
	8.2s	 = Validation runtime
Fitting model: LightGBM_BAG_L1 ... Training model for up to 275.73s of the 419.39s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.14%)
	-33.9173	 = Validation score   (-root_mean_squared_error)
	3.4s	 = Training   runtime
	1.59s	 = Validation runtime
Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 270.07s of the 413.73s of remaining time.
	-38.4351	 = Validation score   (-root_mean_squared_error)
	2.44s	 = Training   runtime
	0.33s	 = Validation runtime
Fitting model: CatBoost_BAG_L1 ... Training model for up to 267.12s of the 410.79s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.21%)
	Warning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.
		ray::_ray_fit() (pid=30669, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=30669, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 265.25s of the 408.92s of remaining time.
	-38.0939	 = Validation score   (-root_mean_squared_error)
	1.04s	 = Training   runtime
	0.34s	 = Validation runtime
Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 263.67s of the 407.33s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.17%)
2024-04-28 19:13:17,281	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:13:17,317	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:13:17,318	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:13:17,320	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:13:17,321	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:13:17,323	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:13:17,324	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
	-45.0436	 = Validation score   (-root_mean_squared_error)
	8.64s	 = Training   runtime
	0.15s	 = Validation runtime
Fitting model: XGBoost_BAG_L1 ... Training model for up to 253.72s of the 397.39s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.27%)
	-34.6029	 = Validation score   (-root_mean_squared_error)
	4.49s	 = Training   runtime
	1.05s	 = Validation runtime
Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 246.68s of the 390.34s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.08%)
	-38.1838	 = Validation score   (-root_mean_squared_error)
	38.23s	 = Training   runtime
	0.05s	 = Validation runtime
Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 206.8s of the 350.47s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.30%)
	-33.6464	 = Validation score   (-root_mean_squared_error)
	6.99s	 = Training   runtime
	1.67s	 = Validation runtime
Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 195.99s of the 339.66s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.20%)
	Warning: Exception caused CatBoost_r177_BAG_L1 to fail during training (ImportError)... Skipping this model.
		ray::_ray_fit() (pid=31011, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=31011, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 194.18s of the 337.85s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.09%)
2024-04-28 19:14:28,394	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): ray::_ray_fit() (pid=31012, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=31012, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
2024-04-28 19:14:28,395	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:14:28,397	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:14:28,399	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:14:28,401	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): ray::_ray_fit() (pid=31015, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=31015, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
2024-04-28 19:14:28,403	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
	-50.077	 = Validation score   (-root_mean_squared_error)
	51.97s	 = Training   runtime
	0.09s	 = Validation runtime
Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 140.54s of the 284.21s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.16%)
	-33.2396	 = Validation score   (-root_mean_squared_error)
	17.84s	 = Training   runtime
	23.9s	 = Validation runtime
Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 113.35s of the 257.02s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.15%)
	-42.5226	 = Validation score   (-root_mean_squared_error)
	24.98s	 = Training   runtime
	0.19s	 = Validation runtime
Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 85.34s of the 229.01s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.59%)
	Warning: Exception caused CatBoost_r9_BAG_L1 to fail during training (ImportError)... Skipping this model.
		ray::_ray_fit() (pid=31513, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=31513, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 82.43s of the 226.1s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.10%)
2024-04-28 19:16:19,816	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:16:19,817	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): ray::_ray_fit() (pid=31515, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=31515, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
2024-04-28 19:16:19,818	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): ray::_ray_fit() (pid=31517, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=31517, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
2024-04-28 19:16:19,819	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:16:19,820	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:16:19,821	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:16:19,822	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): ray::_ray_fit() (pid=31518, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=31518, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
	-37.962	 = Validation score   (-root_mean_squared_error)
	10.34s	 = Training   runtime
	19.54s	 = Validation runtime
Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 65.22s of the 208.89s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.10%)
	-40.0487	 = Validation score   (-root_mean_squared_error)
	55.17s	 = Training   runtime
	0.32s	 = Validation runtime
Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 7.48s of the 151.14s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=1.21%)
	-38.2307	 = Validation score   (-root_mean_squared_error)
	6.48s	 = Training   runtime
	6.64s	 = Validation runtime
Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 140.46s of remaining time.
	Ensemble Weights: {&#39;LightGBMLarge_BAG_L1&#39;: 0.24, &#39;LightGBMXT_BAG_L1&#39;: 0.2, &#39;XGBoost_BAG_L1&#39;: 0.16, &#39;NeuralNetTorch_BAG_L1&#39;: 0.16, &#39;LightGBM_BAG_L1&#39;: 0.08, &#39;LightGBM_r131_BAG_L1&#39;: 0.08, &#39;KNeighborsDist_BAG_L1&#39;: 0.04, &#39;NeuralNetTorch_r22_BAG_L1&#39;: 0.04}
	-31.6707	 = Validation score   (-root_mean_squared_error)
	0.16s	 = Training   runtime
	0.0s	 = Validation runtime
Fitting 106 L2 models ...
Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 140.27s of the 140.06s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.33%)
	-30.9828	 = Validation score   (-root_mean_squared_error)
	2.08s	 = Training   runtime
	0.5s	 = Validation runtime
Fitting model: LightGBM_BAG_L2 ... Training model for up to 136.17s of the 135.97s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.28%)
	-30.3354	 = Validation score   (-root_mean_squared_error)
	1.87s	 = Training   runtime
	0.28s	 = Validation runtime
Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 132.58s of the 132.37s of remaining time.
	-31.3803	 = Validation score   (-root_mean_squared_error)
	9.79s	 = Training   runtime
	0.37s	 = Validation runtime
Fitting model: CatBoost_BAG_L2 ... Training model for up to 122.18s of the 121.98s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.44%)
	Warning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.
		ray::_ray_fit() (pid=32052, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=32052, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 120.04s of the 119.83s of remaining time.
	-31.4526	 = Validation score   (-root_mean_squared_error)
	2.25s	 = Training   runtime
	0.37s	 = Validation runtime
Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 117.14s of the 116.94s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.35%)
2024-04-28 19:18:06,062	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:18:06,064	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:18:06,065	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:18:06,067	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:18:06,070	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:18:06,079	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:18:06,082	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
	-29.5321	 = Validation score   (-root_mean_squared_error)
	8.38s	 = Training   runtime
	0.11s	 = Validation runtime
Fitting model: XGBoost_BAG_L2 ... Training model for up to 107.06s of the 106.85s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.60%)
	-30.9816	 = Validation score   (-root_mean_squared_error)
	2.72s	 = Training   runtime
	0.56s	 = Validation runtime
Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 102.01s of the 101.81s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.16%)
	-31.4263	 = Validation score   (-root_mean_squared_error)
	14.88s	 = Training   runtime
	0.1s	 = Validation runtime
Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 85.57s of the 85.37s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.64%)
	-31.091	 = Validation score   (-root_mean_squared_error)
	3.24s	 = Training   runtime
	0.42s	 = Validation runtime
Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 80.35s of the 80.14s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.40%)
	Warning: Exception caused CatBoost_r177_BAG_L2 to fail during training (ImportError)... Skipping this model.
		ray::_ray_fit() (pid=32319, ip=127.0.0.1)
ModuleNotFoundError: No module named &#39;catboost&#39;

During handling of the above exception, another exception occurred:

ray::_ray_fit() (pid=32319, ip=127.0.0.1)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py&quot;, line 404, in _ray_fit
    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 855, in fit
    out = self._fit(**kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/tabular/models/catboost/catboost_model.py&quot;, line 95, in _fit
    try_import_catboost()
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/common/utils/try_import.py&quot;, line 71, in try_import_catboost
    raise ImportError()
ImportError
Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 78.17s of the 77.96s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.15%)
2024-04-28 19:18:48,144	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:18:48,146	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:18:48,147	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:18:48,149	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:18:48,150	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:18:48,151	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
2024-04-28 19:18:48,152	ERROR worker.py:406 -- Unhandled error (suppress with &#39;RAY_IGNORE_UNHANDLED_ERRORS=1&#39;): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.
	-28.9917	 = Validation score   (-root_mean_squared_error)
	44.5s	 = Training   runtime
	0.14s	 = Validation runtime
Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 31.95s of the 31.74s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.35%)
	-30.5716	 = Validation score   (-root_mean_squared_error)
	3.37s	 = Training   runtime
	0.91s	 = Validation runtime
Fitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 26.34s of the 26.13s of remaining time.
	Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.31%)
	-28.7554	 = Validation score   (-root_mean_squared_error)
	20.99s	 = Training   runtime
	0.17s	 = Validation runtime
Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 3.21s of remaining time.
	Ensemble Weights: {&#39;NeuralNetFastAI_r191_BAG_L2&#39;: 0.471, &#39;NeuralNetTorch_r79_BAG_L2&#39;: 0.412, &#39;LightGBM_BAG_L2&#39;: 0.118}
	-28.1637	 = Validation score   (-root_mean_squared_error)
	0.06s	 = Training   runtime
	0.0s	 = Validation runtime
AutoGluon training complete, total runtime = 427.87s ... Best model: &quot;WeightedEnsemble_L3&quot;
TabularPredictor saved. To load, use: predictor = TabularPredictor.load(&quot;AutogluonModels/ag-20240428_180959&quot;)
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="53">
<div class="sourceCode" id="cb40"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>predictor_new_features.fit_summary()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>*** Summary of fit() ***
Estimated performance of each model:
                          model   score_val              eval_metric  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order
0           WeightedEnsemble_L3  -28.163684  root_mean_squared_error      64.682833  306.917837                0.000347           0.063044            3       True         29
1   NeuralNetFastAI_r191_BAG_L2  -28.755394  root_mean_squared_error      64.258761  260.482944                0.171508          20.987218            2       True         28
2     NeuralNetTorch_r79_BAG_L2  -28.991694  root_mean_squared_error      64.228266  283.998588                0.141013          44.502862            2       True         26
3        NeuralNetFastAI_BAG_L2  -29.532066  root_mean_squared_error      64.198199  247.878518                0.110946           8.382792            2       True         22
4               LightGBM_BAG_L2  -30.335405  root_mean_squared_error      64.369966  241.364714                0.282713           1.868988            2       True         19
5          LightGBM_r131_BAG_L2  -30.571619  root_mean_squared_error      65.000892  242.865588                0.913639           3.369862            2       True         27
6                XGBoost_BAG_L2  -30.981561  root_mean_squared_error      64.642256  242.217222                0.555002           2.721496            2       True         23
7             LightGBMXT_BAG_L2  -30.982791  root_mean_squared_error      64.585937  241.578472                0.498683           2.082746            2       True         18
8          LightGBMLarge_BAG_L2  -31.091021  root_mean_squared_error      64.510900  242.733773                0.423647           3.238047            2       True         25
9        RandomForestMSE_BAG_L2  -31.380349  root_mean_squared_error      64.456970  249.281442                0.369717           9.785716            2       True         20
10        NeuralNetTorch_BAG_L2  -31.426278  root_mean_squared_error      64.185942  254.380053                0.098689          14.884327            2       True         24
11         ExtraTreesMSE_BAG_L2  -31.452630  root_mean_squared_error      64.458685  241.744830                0.371432           2.249104            2       True         21
12          WeightedEnsemble_L2  -31.670650  root_mean_squared_error      36.790868  133.752577                0.000642           0.159596            2       True         17
13         LightGBM_r131_BAG_L1  -33.239616  root_mean_squared_error      23.904147   17.844153               23.904147          17.844153            1       True         12
14         LightGBMLarge_BAG_L1  -33.646444  root_mean_squared_error       1.670936    6.994130                1.670936           6.994130            1       True         10
15              LightGBM_BAG_L1  -33.917339  root_mean_squared_error       1.589185    3.402898                1.589185           3.402898            1       True          4
16            LightGBMXT_BAG_L1  -34.453884  root_mean_squared_error       8.196058    7.466105                8.196058           7.466105            1       True          3
17               XGBoost_BAG_L1  -34.602917  root_mean_squared_error       1.047377    4.485637                1.047377           4.485637            1       True          8
18          LightGBM_r96_BAG_L1  -37.962039  root_mean_squared_error      19.537646   10.340168               19.537646          10.340168            1       True         14
19         ExtraTreesMSE_BAG_L1  -38.093932  root_mean_squared_error       0.343005    1.037588                0.343005           1.037588            1       True          6
20        NeuralNetTorch_BAG_L1  -38.183763  root_mean_squared_error       0.048533   38.226841                0.048533          38.226841            1       True          9
21           XGBoost_r33_BAG_L1  -38.230654  root_mean_squared_error       6.643663    6.479961                6.643663           6.479961            1       True         16
22       RandomForestMSE_BAG_L1  -38.435101  root_mean_squared_error       0.325903    2.440858                0.325903           2.440858            1       True          5
23    NeuralNetTorch_r22_BAG_L1  -40.048682  root_mean_squared_error       0.317375   55.165059                0.317375          55.165059            1       True         15
24  NeuralNetFastAI_r191_BAG_L1  -42.522596  root_mean_squared_error       0.190433   24.977403                0.190433          24.977403            1       True         13
25       NeuralNetFastAI_BAG_L1  -45.043552  root_mean_squared_error       0.151246    8.641363                0.151246           8.641363            1       True          7
26    NeuralNetTorch_r79_BAG_L1  -50.076982  root_mean_squared_error       0.089319   51.974439                0.089319          51.974439            1       True         11
27        KNeighborsDist_BAG_L1  -84.146423  root_mean_squared_error       0.016615    0.008158                0.016615           0.008158            1       True          2
28        KNeighborsUnif_BAG_L1 -101.588176  root_mean_squared_error       0.015812    0.010965                0.015812           0.010965            1       True          1
Number of models trained: 29
Types of models trained:
{&#39;StackerEnsembleModel_KNN&#39;, &#39;StackerEnsembleModel_XGBoost&#39;, &#39;StackerEnsembleModel_RF&#39;, &#39;StackerEnsembleModel_TabularNeuralNetTorch&#39;, &#39;StackerEnsembleModel_XT&#39;, &#39;StackerEnsembleModel_LGB&#39;, &#39;WeightedEnsembleModel&#39;, &#39;StackerEnsembleModel_NNFastAiTabular&#39;}
Bagging used: True  (with 8 folds)
Multi-layer stack-ensembling used: True  (with 3 levels)
Feature Metadata (Processed):
(raw dtype, special dtypes):
(&#39;category&#39;, [])             : 2 | [&#39;season&#39;, &#39;weather&#39;]
(&#39;float&#39;, [])                : 3 | [&#39;temp&#39;, &#39;atemp&#39;, &#39;windspeed&#39;]
(&#39;int&#39;, [])                  : 4 | [&#39;humidity&#39;, &#39;month&#39;, &#39;day&#39;, &#39;hour&#39;]
(&#39;int&#39;, [&#39;bool&#39;])            : 3 | [&#39;holiday&#39;, &#39;workingday&#39;, &#39;year&#39;]
(&#39;int&#39;, [&#39;datetime_as_int&#39;]) : 3 | [&#39;datetime&#39;, &#39;datetime.year&#39;, &#39;datetime.dayofweek&#39;]
*** End of fit() summary ***
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>/opt/homebrew/lib/python3.11/site-packages/autogluon/core/utils/plots.py:169: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: &quot;pip install bokeh==2.0.1&quot;
  warnings.warn(&#39;AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: &quot;pip install bokeh==2.0.1&quot;&#39;)
</code></pre>
</div>
<div class="output execute_result" data-execution_count="53">
<pre><code>{&#39;model_types&#39;: {&#39;KNeighborsUnif_BAG_L1&#39;: &#39;StackerEnsembleModel_KNN&#39;,
  &#39;KNeighborsDist_BAG_L1&#39;: &#39;StackerEnsembleModel_KNN&#39;,
  &#39;LightGBMXT_BAG_L1&#39;: &#39;StackerEnsembleModel_LGB&#39;,
  &#39;LightGBM_BAG_L1&#39;: &#39;StackerEnsembleModel_LGB&#39;,
  &#39;RandomForestMSE_BAG_L1&#39;: &#39;StackerEnsembleModel_RF&#39;,
  &#39;ExtraTreesMSE_BAG_L1&#39;: &#39;StackerEnsembleModel_XT&#39;,
  &#39;NeuralNetFastAI_BAG_L1&#39;: &#39;StackerEnsembleModel_NNFastAiTabular&#39;,
  &#39;XGBoost_BAG_L1&#39;: &#39;StackerEnsembleModel_XGBoost&#39;,
  &#39;NeuralNetTorch_BAG_L1&#39;: &#39;StackerEnsembleModel_TabularNeuralNetTorch&#39;,
  &#39;LightGBMLarge_BAG_L1&#39;: &#39;StackerEnsembleModel_LGB&#39;,
  &#39;NeuralNetTorch_r79_BAG_L1&#39;: &#39;StackerEnsembleModel_TabularNeuralNetTorch&#39;,
  &#39;LightGBM_r131_BAG_L1&#39;: &#39;StackerEnsembleModel_LGB&#39;,
  &#39;NeuralNetFastAI_r191_BAG_L1&#39;: &#39;StackerEnsembleModel_NNFastAiTabular&#39;,
  &#39;LightGBM_r96_BAG_L1&#39;: &#39;StackerEnsembleModel_LGB&#39;,
  &#39;NeuralNetTorch_r22_BAG_L1&#39;: &#39;StackerEnsembleModel_TabularNeuralNetTorch&#39;,
  &#39;XGBoost_r33_BAG_L1&#39;: &#39;StackerEnsembleModel_XGBoost&#39;,
  &#39;WeightedEnsemble_L2&#39;: &#39;WeightedEnsembleModel&#39;,
  &#39;LightGBMXT_BAG_L2&#39;: &#39;StackerEnsembleModel_LGB&#39;,
  &#39;LightGBM_BAG_L2&#39;: &#39;StackerEnsembleModel_LGB&#39;,
  &#39;RandomForestMSE_BAG_L2&#39;: &#39;StackerEnsembleModel_RF&#39;,
  &#39;ExtraTreesMSE_BAG_L2&#39;: &#39;StackerEnsembleModel_XT&#39;,
  &#39;NeuralNetFastAI_BAG_L2&#39;: &#39;StackerEnsembleModel_NNFastAiTabular&#39;,
  &#39;XGBoost_BAG_L2&#39;: &#39;StackerEnsembleModel_XGBoost&#39;,
  &#39;NeuralNetTorch_BAG_L2&#39;: &#39;StackerEnsembleModel_TabularNeuralNetTorch&#39;,
  &#39;LightGBMLarge_BAG_L2&#39;: &#39;StackerEnsembleModel_LGB&#39;,
  &#39;NeuralNetTorch_r79_BAG_L2&#39;: &#39;StackerEnsembleModel_TabularNeuralNetTorch&#39;,
  &#39;LightGBM_r131_BAG_L2&#39;: &#39;StackerEnsembleModel_LGB&#39;,
  &#39;NeuralNetFastAI_r191_BAG_L2&#39;: &#39;StackerEnsembleModel_NNFastAiTabular&#39;,
  &#39;WeightedEnsemble_L3&#39;: &#39;WeightedEnsembleModel&#39;},
 &#39;model_performance&#39;: {&#39;KNeighborsUnif_BAG_L1&#39;: -101.58817625927213,
  &#39;KNeighborsDist_BAG_L1&#39;: -84.14642264302962,
  &#39;LightGBMXT_BAG_L1&#39;: -34.453884062670745,
  &#39;LightGBM_BAG_L1&#39;: -33.91733862651761,
  &#39;RandomForestMSE_BAG_L1&#39;: -38.43510081827286,
  &#39;ExtraTreesMSE_BAG_L1&#39;: -38.09393221409416,
  &#39;NeuralNetFastAI_BAG_L1&#39;: -45.04355236383513,
  &#39;XGBoost_BAG_L1&#39;: -34.60291738693257,
  &#39;NeuralNetTorch_BAG_L1&#39;: -38.18376283094465,
  &#39;LightGBMLarge_BAG_L1&#39;: -33.64644400536692,
  &#39;NeuralNetTorch_r79_BAG_L1&#39;: -50.07698215128641,
  &#39;LightGBM_r131_BAG_L1&#39;: -33.239615707340015,
  &#39;NeuralNetFastAI_r191_BAG_L1&#39;: -42.522595532796736,
  &#39;LightGBM_r96_BAG_L1&#39;: -37.9620385739336,
  &#39;NeuralNetTorch_r22_BAG_L1&#39;: -40.04868155578324,
  &#39;XGBoost_r33_BAG_L1&#39;: -38.23065430144657,
  &#39;WeightedEnsemble_L2&#39;: -31.670650007966117,
  &#39;LightGBMXT_BAG_L2&#39;: -30.982790992721814,
  &#39;LightGBM_BAG_L2&#39;: -30.33540496242712,
  &#39;RandomForestMSE_BAG_L2&#39;: -31.380348703819877,
  &#39;ExtraTreesMSE_BAG_L2&#39;: -31.452630101641095,
  &#39;NeuralNetFastAI_BAG_L2&#39;: -29.532065782052694,
  &#39;XGBoost_BAG_L2&#39;: -30.981561080899393,
  &#39;NeuralNetTorch_BAG_L2&#39;: -31.426277562519203,
  &#39;LightGBMLarge_BAG_L2&#39;: -31.0910211914063,
  &#39;NeuralNetTorch_r79_BAG_L2&#39;: -28.991693693558517,
  &#39;LightGBM_r131_BAG_L2&#39;: -30.57161864641645,
  &#39;NeuralNetFastAI_r191_BAG_L2&#39;: -28.755394280761113,
  &#39;WeightedEnsemble_L3&#39;: -28.163683526368118},
 &#39;model_best&#39;: &#39;WeightedEnsemble_L3&#39;,
 &#39;model_paths&#39;: {&#39;KNeighborsUnif_BAG_L1&#39;: [&#39;KNeighborsUnif_BAG_L1&#39;],
  &#39;KNeighborsDist_BAG_L1&#39;: [&#39;KNeighborsDist_BAG_L1&#39;],
  &#39;LightGBMXT_BAG_L1&#39;: [&#39;LightGBMXT_BAG_L1&#39;],
  &#39;LightGBM_BAG_L1&#39;: [&#39;LightGBM_BAG_L1&#39;],
  &#39;RandomForestMSE_BAG_L1&#39;: [&#39;RandomForestMSE_BAG_L1&#39;],
  &#39;ExtraTreesMSE_BAG_L1&#39;: [&#39;ExtraTreesMSE_BAG_L1&#39;],
  &#39;NeuralNetFastAI_BAG_L1&#39;: [&#39;NeuralNetFastAI_BAG_L1&#39;],
  &#39;XGBoost_BAG_L1&#39;: [&#39;XGBoost_BAG_L1&#39;],
  &#39;NeuralNetTorch_BAG_L1&#39;: [&#39;NeuralNetTorch_BAG_L1&#39;],
  &#39;LightGBMLarge_BAG_L1&#39;: [&#39;LightGBMLarge_BAG_L1&#39;],
  &#39;NeuralNetTorch_r79_BAG_L1&#39;: [&#39;NeuralNetTorch_r79_BAG_L1&#39;],
  &#39;LightGBM_r131_BAG_L1&#39;: [&#39;LightGBM_r131_BAG_L1&#39;],
  &#39;NeuralNetFastAI_r191_BAG_L1&#39;: [&#39;NeuralNetFastAI_r191_BAG_L1&#39;],
  &#39;LightGBM_r96_BAG_L1&#39;: [&#39;LightGBM_r96_BAG_L1&#39;],
  &#39;NeuralNetTorch_r22_BAG_L1&#39;: [&#39;NeuralNetTorch_r22_BAG_L1&#39;],
  &#39;XGBoost_r33_BAG_L1&#39;: [&#39;XGBoost_r33_BAG_L1&#39;],
  &#39;WeightedEnsemble_L2&#39;: [&#39;WeightedEnsemble_L2&#39;],
  &#39;LightGBMXT_BAG_L2&#39;: [&#39;LightGBMXT_BAG_L2&#39;],
  &#39;LightGBM_BAG_L2&#39;: [&#39;LightGBM_BAG_L2&#39;],
  &#39;RandomForestMSE_BAG_L2&#39;: [&#39;RandomForestMSE_BAG_L2&#39;],
  &#39;ExtraTreesMSE_BAG_L2&#39;: [&#39;ExtraTreesMSE_BAG_L2&#39;],
  &#39;NeuralNetFastAI_BAG_L2&#39;: [&#39;NeuralNetFastAI_BAG_L2&#39;],
  &#39;XGBoost_BAG_L2&#39;: [&#39;XGBoost_BAG_L2&#39;],
  &#39;NeuralNetTorch_BAG_L2&#39;: [&#39;NeuralNetTorch_BAG_L2&#39;],
  &#39;LightGBMLarge_BAG_L2&#39;: [&#39;LightGBMLarge_BAG_L2&#39;],
  &#39;NeuralNetTorch_r79_BAG_L2&#39;: [&#39;NeuralNetTorch_r79_BAG_L2&#39;],
  &#39;LightGBM_r131_BAG_L2&#39;: [&#39;LightGBM_r131_BAG_L2&#39;],
  &#39;NeuralNetFastAI_r191_BAG_L2&#39;: [&#39;NeuralNetFastAI_r191_BAG_L2&#39;],
  &#39;WeightedEnsemble_L3&#39;: [&#39;WeightedEnsemble_L3&#39;]},
 &#39;model_fit_times&#39;: {&#39;KNeighborsUnif_BAG_L1&#39;: 0.01096487045288086,
  &#39;KNeighborsDist_BAG_L1&#39;: 0.008157968521118164,
  &#39;LightGBMXT_BAG_L1&#39;: 7.466105222702026,
  &#39;LightGBM_BAG_L1&#39;: 3.402898073196411,
  &#39;RandomForestMSE_BAG_L1&#39;: 2.4408578872680664,
  &#39;ExtraTreesMSE_BAG_L1&#39;: 1.0375878810882568,
  &#39;NeuralNetFastAI_BAG_L1&#39;: 8.641363143920898,
  &#39;XGBoost_BAG_L1&#39;: 4.485636949539185,
  &#39;NeuralNetTorch_BAG_L1&#39;: 38.22684073448181,
  &#39;LightGBMLarge_BAG_L1&#39;: 6.9941301345825195,
  &#39;NeuralNetTorch_r79_BAG_L1&#39;: 51.97443890571594,
  &#39;LightGBM_r131_BAG_L1&#39;: 17.84415316581726,
  &#39;NeuralNetFastAI_r191_BAG_L1&#39;: 24.977403163909912,
  &#39;LightGBM_r96_BAG_L1&#39;: 10.340168237686157,
  &#39;NeuralNetTorch_r22_BAG_L1&#39;: 55.165058851242065,
  &#39;XGBoost_r33_BAG_L1&#39;: 6.479960680007935,
  &#39;WeightedEnsemble_L2&#39;: 0.15959620475769043,
  &#39;LightGBMXT_BAG_L2&#39;: 2.0827462673187256,
  &#39;LightGBM_BAG_L2&#39;: 1.868987798690796,
  &#39;RandomForestMSE_BAG_L2&#39;: 9.785715818405151,
  &#39;ExtraTreesMSE_BAG_L2&#39;: 2.2491037845611572,
  &#39;NeuralNetFastAI_BAG_L2&#39;: 8.382792234420776,
  &#39;XGBoost_BAG_L2&#39;: 2.721496105194092,
  &#39;NeuralNetTorch_BAG_L2&#39;: 14.884326934814453,
  &#39;LightGBMLarge_BAG_L2&#39;: 3.2380471229553223,
  &#39;NeuralNetTorch_r79_BAG_L2&#39;: 44.502861976623535,
  &#39;LightGBM_r131_BAG_L2&#39;: 3.3698618412017822,
  &#39;NeuralNetFastAI_r191_BAG_L2&#39;: 20.987217903137207,
  &#39;WeightedEnsemble_L3&#39;: 0.06304383277893066},
 &#39;model_pred_times&#39;: {&#39;KNeighborsUnif_BAG_L1&#39;: 0.015811920166015625,
  &#39;KNeighborsDist_BAG_L1&#39;: 0.016614913940429688,
  &#39;LightGBMXT_BAG_L1&#39;: 8.19605827331543,
  &#39;LightGBM_BAG_L1&#39;: 1.5891845226287842,
  &#39;RandomForestMSE_BAG_L1&#39;: 0.32590293884277344,
  &#39;ExtraTreesMSE_BAG_L1&#39;: 0.3430049419403076,
  &#39;NeuralNetFastAI_BAG_L1&#39;: 0.1512460708618164,
  &#39;XGBoost_BAG_L1&#39;: 1.047377109527588,
  &#39;NeuralNetTorch_BAG_L1&#39;: 0.048532724380493164,
  &#39;LightGBMLarge_BAG_L1&#39;: 1.670936107635498,
  &#39;NeuralNetTorch_r79_BAG_L1&#39;: 0.08931922912597656,
  &#39;LightGBM_r131_BAG_L1&#39;: 23.904147386550903,
  &#39;NeuralNetFastAI_r191_BAG_L1&#39;: 0.19043254852294922,
  &#39;LightGBM_r96_BAG_L1&#39;: 19.537646293640137,
  &#39;NeuralNetTorch_r22_BAG_L1&#39;: 0.31737494468688965,
  &#39;XGBoost_r33_BAG_L1&#39;: 6.643663167953491,
  &#39;WeightedEnsemble_L2&#39;: 0.0006420612335205078,
  &#39;LightGBMXT_BAG_L2&#39;: 0.49868345260620117,
  &#39;LightGBM_BAG_L2&#39;: 0.2827129364013672,
  &#39;RandomForestMSE_BAG_L2&#39;: 0.3697171211242676,
  &#39;ExtraTreesMSE_BAG_L2&#39;: 0.371431827545166,
  &#39;NeuralNetFastAI_BAG_L2&#39;: 0.1109461784362793,
  &#39;XGBoost_BAG_L2&#39;: 0.5550024509429932,
  &#39;NeuralNetTorch_BAG_L2&#39;: 0.09868884086608887,
  &#39;LightGBMLarge_BAG_L2&#39;: 0.4236471652984619,
  &#39;NeuralNetTorch_r79_BAG_L2&#39;: 0.14101266860961914,
  &#39;LightGBM_r131_BAG_L2&#39;: 0.9136385917663574,
  &#39;NeuralNetFastAI_r191_BAG_L2&#39;: 0.1715075969696045,
  &#39;WeightedEnsemble_L3&#39;: 0.00034689903259277344},
 &#39;num_bag_folds&#39;: 8,
 &#39;max_stack_level&#39;: 3,
 &#39;model_hyperparams&#39;: {&#39;KNeighborsUnif_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True,
   &#39;use_child_oof&#39;: True},
  &#39;KNeighborsDist_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True,
   &#39;use_child_oof&#39;: True},
  &#39;LightGBMXT_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;LightGBM_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;RandomForestMSE_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True,
   &#39;use_child_oof&#39;: True},
  &#39;ExtraTreesMSE_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True,
   &#39;use_child_oof&#39;: True},
  &#39;NeuralNetFastAI_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;XGBoost_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;NeuralNetTorch_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;LightGBMLarge_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;NeuralNetTorch_r79_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;LightGBM_r131_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;NeuralNetFastAI_r191_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;LightGBM_r96_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;NeuralNetTorch_r22_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;XGBoost_r33_BAG_L1&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;WeightedEnsemble_L2&#39;: {&#39;use_orig_features&#39;: False,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;LightGBMXT_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;LightGBM_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;RandomForestMSE_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True,
   &#39;use_child_oof&#39;: True},
  &#39;ExtraTreesMSE_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True,
   &#39;use_child_oof&#39;: True},
  &#39;NeuralNetFastAI_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;XGBoost_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;NeuralNetTorch_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;LightGBMLarge_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;NeuralNetTorch_r79_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;LightGBM_r131_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;NeuralNetFastAI_r191_BAG_L2&#39;: {&#39;use_orig_features&#39;: True,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True},
  &#39;WeightedEnsemble_L3&#39;: {&#39;use_orig_features&#39;: False,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True}},
 &#39;leaderboard&#39;:                           model   score_val              eval_metric  \
 0           WeightedEnsemble_L3  -28.163684  root_mean_squared_error   
 1   NeuralNetFastAI_r191_BAG_L2  -28.755394  root_mean_squared_error   
 2     NeuralNetTorch_r79_BAG_L2  -28.991694  root_mean_squared_error   
 3        NeuralNetFastAI_BAG_L2  -29.532066  root_mean_squared_error   
 4               LightGBM_BAG_L2  -30.335405  root_mean_squared_error   
 5          LightGBM_r131_BAG_L2  -30.571619  root_mean_squared_error   
 6                XGBoost_BAG_L2  -30.981561  root_mean_squared_error   
 7             LightGBMXT_BAG_L2  -30.982791  root_mean_squared_error   
 8          LightGBMLarge_BAG_L2  -31.091021  root_mean_squared_error   
 9        RandomForestMSE_BAG_L2  -31.380349  root_mean_squared_error   
 10        NeuralNetTorch_BAG_L2  -31.426278  root_mean_squared_error   
 11         ExtraTreesMSE_BAG_L2  -31.452630  root_mean_squared_error   
 12          WeightedEnsemble_L2  -31.670650  root_mean_squared_error   
 13         LightGBM_r131_BAG_L1  -33.239616  root_mean_squared_error   
 14         LightGBMLarge_BAG_L1  -33.646444  root_mean_squared_error   
 15              LightGBM_BAG_L1  -33.917339  root_mean_squared_error   
 16            LightGBMXT_BAG_L1  -34.453884  root_mean_squared_error   
 17               XGBoost_BAG_L1  -34.602917  root_mean_squared_error   
 18          LightGBM_r96_BAG_L1  -37.962039  root_mean_squared_error   
 19         ExtraTreesMSE_BAG_L1  -38.093932  root_mean_squared_error   
 20        NeuralNetTorch_BAG_L1  -38.183763  root_mean_squared_error   
 21           XGBoost_r33_BAG_L1  -38.230654  root_mean_squared_error   
 22       RandomForestMSE_BAG_L1  -38.435101  root_mean_squared_error   
 23    NeuralNetTorch_r22_BAG_L1  -40.048682  root_mean_squared_error   
 24  NeuralNetFastAI_r191_BAG_L1  -42.522596  root_mean_squared_error   
 25       NeuralNetFastAI_BAG_L1  -45.043552  root_mean_squared_error   
 26    NeuralNetTorch_r79_BAG_L1  -50.076982  root_mean_squared_error   
 27        KNeighborsDist_BAG_L1  -84.146423  root_mean_squared_error   
 28        KNeighborsUnif_BAG_L1 -101.588176  root_mean_squared_error   
 
     pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  \
 0       64.682833  306.917837                0.000347           0.063044   
 1       64.258761  260.482944                0.171508          20.987218   
 2       64.228266  283.998588                0.141013          44.502862   
 3       64.198199  247.878518                0.110946           8.382792   
 4       64.369966  241.364714                0.282713           1.868988   
 5       65.000892  242.865588                0.913639           3.369862   
 6       64.642256  242.217222                0.555002           2.721496   
 7       64.585937  241.578472                0.498683           2.082746   
 8       64.510900  242.733773                0.423647           3.238047   
 9       64.456970  249.281442                0.369717           9.785716   
 10      64.185942  254.380053                0.098689          14.884327   
 11      64.458685  241.744830                0.371432           2.249104   
 12      36.790868  133.752577                0.000642           0.159596   
 13      23.904147   17.844153               23.904147          17.844153   
 14       1.670936    6.994130                1.670936           6.994130   
 15       1.589185    3.402898                1.589185           3.402898   
 16       8.196058    7.466105                8.196058           7.466105   
 17       1.047377    4.485637                1.047377           4.485637   
 18      19.537646   10.340168               19.537646          10.340168   
 19       0.343005    1.037588                0.343005           1.037588   
 20       0.048533   38.226841                0.048533          38.226841   
 21       6.643663    6.479961                6.643663           6.479961   
 22       0.325903    2.440858                0.325903           2.440858   
 23       0.317375   55.165059                0.317375          55.165059   
 24       0.190433   24.977403                0.190433          24.977403   
 25       0.151246    8.641363                0.151246           8.641363   
 26       0.089319   51.974439                0.089319          51.974439   
 27       0.016615    0.008158                0.016615           0.008158   
 28       0.015812    0.010965                0.015812           0.010965   
 
     stack_level  can_infer  fit_order  
 0             3       True         29  
 1             2       True         28  
 2             2       True         26  
 3             2       True         22  
 4             2       True         19  
 5             2       True         27  
 6             2       True         23  
 7             2       True         18  
 8             2       True         25  
 9             2       True         20  
 10            2       True         24  
 11            2       True         21  
 12            2       True         17  
 13            1       True         12  
 14            1       True         10  
 15            1       True          4  
 16            1       True          3  
 17            1       True          8  
 18            1       True         14  
 19            1       True          6  
 20            1       True          9  
 21            1       True         16  
 22            1       True          5  
 23            1       True         15  
 24            1       True         13  
 25            1       True          7  
 26            1       True         11  
 27            1       True          2  
 28            1       True          1  }</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="54">
<div class="sourceCode" id="cb44"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>predictions_new_features <span class="op">=</span> predictor_new_features.predict(test)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>predictions_new_features <span class="op">=</span> {<span class="st">&#39;datetime&#39;</span>: test[<span class="st">&#39;datetime&#39;</span>], <span class="st">&#39;Pred_count&#39;</span>: predictions_new_features}</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>predictions_new_features <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>predictions_new_features)</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>predictions_new_features.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="54">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>datetime</th>
      <th>Pred_count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2011-01-20 00:00:00</td>
      <td>17.277929</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2011-01-20 01:00:00</td>
      <td>8.864119</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2011-01-20 02:00:00</td>
      <td>7.685024</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2011-01-20 03:00:00</td>
      <td>8.712017</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2011-01-20 04:00:00</td>
      <td>8.588231</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell code" data-execution_count="55">
<div class="sourceCode" id="cb45"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Remember to set all negative values to zero</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>predictions_new_features[<span class="st">&#39;Pred_count&#39;</span>] <span class="op">=</span> predictions_new_features[<span class="st">&#39;Pred_count&#39;</span>].clip(lower<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="56">
<div class="sourceCode" id="cb46"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>predictions_new_features.describe()</span></code></pre></div>
<div class="output execute_result" data-execution_count="56">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>datetime</th>
      <th>Pred_count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>6493</td>
      <td>6493.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>2012-01-13 09:27:47.765285632</td>
      <td>124.659843</td>
    </tr>
    <tr>
      <th>min</th>
      <td>2011-01-20 00:00:00</td>
      <td>1.119439</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>2011-07-22 15:00:00</td>
      <td>48.840836</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>2012-01-20 23:00:00</td>
      <td>96.039490</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>2012-07-20 17:00:00</td>
      <td>166.971588</td>
    </tr>
    <tr>
      <th>max</th>
      <td>2012-12-31 23:00:00</td>
      <td>726.968750</td>
    </tr>
    <tr>
      <th>std</th>
      <td>NaN</td>
      <td>106.113327</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell code" data-execution_count="58">
<div class="sourceCode" id="cb47"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Same thing as train and test dataset</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>submission_new_features <span class="op">=</span> pd.read_csv(<span class="st">&#39;sampleSubmission.csv&#39;</span>, parse_dates <span class="op">=</span> [<span class="st">&#39;datetime&#39;</span>])</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>submission_new_features.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="58">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>datetime</th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2011-01-20 00:00:00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2011-01-20 01:00:00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2011-01-20 02:00:00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2011-01-20 03:00:00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2011-01-20 04:00:00</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell code" data-execution_count="59">
<div class="sourceCode" id="cb48"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Same submitting predictions</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>submission_new_features[<span class="st">&quot;count&quot;</span>] <span class="op">=</span> predictions_new_features[<span class="st">&quot;Pred_count&quot;</span>]</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>submission_new_features.to_csv(<span class="st">&quot;submission_new_features.csv&quot;</span>, index<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="60">
<div class="sourceCode" id="cb49"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>kaggle competitions submit <span class="op">-</span>c bike<span class="op">-</span>sharing<span class="op">-</span>demand <span class="op">-</span>f submission_new_features.csv <span class="op">-</span>m <span class="st">&quot;new features&quot;</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>100%|█████████████████████████████████████████| 188k/188k [00:00&lt;00:00, 246kB/s]
Successfully submitted to Bike Sharing Demand</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="61">
<div class="sourceCode" id="cb51"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>kaggle competitions submissions <span class="op">-</span>c bike<span class="op">-</span>sharing<span class="op">-</span>demand <span class="op">|</span> tail <span class="op">-</span>n <span class="op">+</span><span class="dv">1</span> <span class="op">|</span> head <span class="op">-</span>n <span class="dv">6</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>fileName                     date                 description           status    publicScore  privateScore  
---------------------------  -------------------  --------------------  --------  -----------  ------------  
submission_new_features.csv  2024-04-28 18:23:06  new features          complete  0.85666      0.85666       
submission.csv               2024-04-28 18:04:20  first raw submission  complete  1.83131      1.83131       
</code></pre>
</div>
</div>
<section id="new-score-of-085666" class="cell markdown">
<h4>New Score of <code>0.85666</code></h4>
</section>
<section id="step-6-hyper-parameter-optimization" class="cell markdown">
<h2>Step 6: Hyper parameter optimization</h2>
<ul>
<li>There are many options for hyper parameter optimization.</li>
<li>Options are to change the AutoGluon higher level parameters or the
individual model hyperparameters.</li>
<li>The hyperparameters of the models themselves that are in AutoGluon.
Those need the <code>hyperparameter</code> and
<code>hyperparameter_tune_kwargs</code> arguments.</li>
</ul>
</section>
<div class="cell code" data-execution_count="62">
<div class="sourceCode" id="cb53"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Requirements: (Same settings as initial run) For AutoGluon&#39;s Tabular Predictions</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>eval_metric <span class="op">=</span> <span class="st">&#39;root_mean_squared_error&#39;</span> </span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>label <span class="op">=</span> <span class="st">&#39;count&#39;</span></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>ignored_columns <span class="op">=</span> [<span class="st">&quot;casual&quot;</span>, <span class="st">&quot;registered&quot;</span>]   </span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> train                           </span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>time_limit <span class="op">=</span> <span class="dv">600</span>                             </span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>presets <span class="op">=</span> <span class="st">&quot;optimize_for_deployment&quot;</span>                             </span></code></pre></div>
</div>
<div class="cell code" data-execution_count="79">
<div class="sourceCode" id="cb54"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> autogluon.common <span class="im">as</span> ag</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> autogluon.common.space <span class="im">import</span> Real, Categorical, Int</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>nn_options <span class="op">=</span> {<span class="st">&#39;num_epochs&#39;</span>: <span class="dv">5</span>, </span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>              <span class="st">&#39;learning_rate&#39;</span>: ag.space.Real(<span class="fl">1e-4</span>, <span class="fl">1e-2</span>, default<span class="op">=</span><span class="fl">5e-4</span>, log<span class="op">=</span><span class="va">True</span>),  </span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># learning rate used in training (real-valued hyperparameter, default = 0.1)</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>              <span class="st">&#39;activation&#39;</span>: ag.space.Categorical(<span class="st">&#39;relu&#39;</span>, <span class="st">&#39;softrelu&#39;</span>, <span class="st">&#39;tanh&#39;</span>),  </span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># activation function used in the neural network (categorical hyperparameter, default = &#39;softrelu&#39;)</span></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>              <span class="st">&#39;dropout_prob&#39;</span>: ag.space.Real(<span class="fl">0.0</span>, <span class="fl">0.5</span>, default<span class="op">=</span><span class="fl">0.1</span>)}</span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a>gbm_options <span class="op">=</span> [{<span class="st">&#39;extra_trees&#39;</span>: <span class="va">True</span>, </span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a>                <span class="st">&#39;num_boost_round&#39;</span>: ag.space.Int(lower<span class="op">=</span><span class="dv">100</span>, upper<span class="op">=</span><span class="dv">500</span>, default<span class="op">=</span><span class="dv">100</span>),</span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a>                <span class="st">&#39;num_leaves&#39;</span>: ag.space.Int(lower<span class="op">=</span><span class="dv">25</span>, upper<span class="op">=</span><span class="dv">64</span>, default<span class="op">=</span><span class="dv">36</span>),</span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a>                <span class="st">&#39;ag_args&#39;</span>: {<span class="st">&#39;name_suffix&#39;</span>: <span class="st">&#39;XT&#39;</span>}}, {}, <span class="st">&#39;GBMLarge&#39;</span>]</span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a>hyperparameters <span class="op">=</span> {  </span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a>                   <span class="st">&#39;GBM&#39;</span>: gbm_options,</span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>                   <span class="st">&#39;NN_TORCH&#39;</span>: nn_options, </span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a>                  }  </span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a>num_trials <span class="op">=</span> <span class="dv">20</span>            <span class="co"># number of hyperparameter tuning trials (default = 20)</span></span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a>search_strategy <span class="op">=</span> <span class="st">&#39;auto&#39;</span>  <span class="co"># to tune hyperparameters using multiple search strategies (default = &#39;auto&#39;)</span></span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> <span class="st">&#39;local&#39;</span>       <span class="co"># to run the hyperparameter tuning jobs in parallel across multiple machines (default = &#39;local&#39;)</span></span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a>hyperparameter_tune_kwargs <span class="op">=</span> { </span>
<span id="cb54-27"><a href="#cb54-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;num_trials&#39;</span>: num_trials,</span>
<span id="cb54-28"><a href="#cb54-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;scheduler&#39;</span> : scheduler,</span>
<span id="cb54-29"><a href="#cb54-29" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;searcher&#39;</span>: search_strategy,</span>
<span id="cb54-30"><a href="#cb54-30" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb54-31"><a href="#cb54-31" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<div class="output stream stderr">
<pre><code>No path specified. Models will be saved in: &quot;AutogluonModels/ag-20240428_184432&quot;
Presets specified: [&#39;optimize_for_deployment&#39;]
Warning: hyperparameter tuning is currently experimental and may cause the process to hang.
Beginning AutoGluon training ... Time limit = 600s
AutoGluon will save models to &quot;AutogluonModels/ag-20240428_184432&quot;
=================== System Info ===================
AutoGluon Version:  1.1.0
Python Version:     3.11.5
Operating System:   Darwin
Platform Machine:   arm64
Platform Version:   Darwin Kernel Version 23.3.0: Wed Dec 20 21:30:44 PST 2023; root:xnu-10002.81.5~7/RELEASE_ARM64_T6000
CPU Count:          8
Memory Avail:       4.97 GB / 16.00 GB (31.1%)
Disk Space Avail:   43.67 GB / 460.43 GB (9.5%)
===================================================
Train Data Rows:    10886
Train Data Columns: 15
Label Column:       count
Problem Type:       regression
Preprocessing data ...
Using Feature Generators to preprocess the data ...
Dropping user-specified ignored columns: [&#39;casual&#39;, &#39;registered&#39;]
Fitting AutoMLPipelineFeatureGenerator...
	Available Memory:                    5077.24 MB
	Train Data (Original)  Memory Usage: 0.77 MB (0.0% of available memory)
	Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.
	Stage 1 Generators:
		Fitting AsTypeFeatureGenerator...
			Note: Converting 3 features to boolean dtype as they only contain 2 unique values.
	Stage 2 Generators:
		Fitting FillNaFeatureGenerator...
	Stage 3 Generators:
		Fitting IdentityFeatureGenerator...
		Fitting CategoryFeatureGenerator...
			Fitting CategoryMemoryMinimizeFeatureGenerator...
		Fitting DatetimeFeatureGenerator...
	Stage 4 Generators:
		Fitting DropUniqueFeatureGenerator...
	Stage 5 Generators:
		Fitting DropDuplicatesFeatureGenerator...
	Types of features in original data (raw dtype, special dtypes):
		(&#39;category&#39;, []) : 2 | [&#39;season&#39;, &#39;weather&#39;]
		(&#39;datetime&#39;, []) : 1 | [&#39;datetime&#39;]
		(&#39;float&#39;, [])    : 3 | [&#39;temp&#39;, &#39;atemp&#39;, &#39;windspeed&#39;]
		(&#39;int&#39;, [])      : 7 | [&#39;holiday&#39;, &#39;workingday&#39;, &#39;humidity&#39;, &#39;year&#39;, &#39;month&#39;, ...]
	Types of features in processed data (raw dtype, special dtypes):
		(&#39;category&#39;, [])             : 2 | [&#39;season&#39;, &#39;weather&#39;]
		(&#39;float&#39;, [])                : 3 | [&#39;temp&#39;, &#39;atemp&#39;, &#39;windspeed&#39;]
		(&#39;int&#39;, [])                  : 4 | [&#39;humidity&#39;, &#39;month&#39;, &#39;day&#39;, &#39;hour&#39;]
		(&#39;int&#39;, [&#39;bool&#39;])            : 3 | [&#39;holiday&#39;, &#39;workingday&#39;, &#39;year&#39;]
		(&#39;int&#39;, [&#39;datetime_as_int&#39;]) : 3 | [&#39;datetime&#39;, &#39;datetime.year&#39;, &#39;datetime.dayofweek&#39;]
	0.2s = Fit runtime
	13 features in original data used to generate 15 features in processed data.
	Train Data (Processed) Memory Usage: 0.76 MB (0.0% of available memory)
Data preprocessing and feature engineering runtime = 0.2s ...
AutoGluon will gauge predictive performance using evaluation metric: &#39;root_mean_squared_error&#39;
	This metric&#39;s sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	To change this, specify the eval_metric parameter of Predictor()
Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 8708, Val Rows: 2178
User-specified model hyperparameters to be fit:
{
	&#39;GBM&#39;: [{&#39;extra_trees&#39;: True, &#39;num_boost_round&#39;: Int: lower=100, upper=500, &#39;num_leaves&#39;: Int: lower=25, upper=64, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;XT&#39;}}, {}, &#39;GBMLarge&#39;],
	&#39;NN_TORCH&#39;: {&#39;num_epochs&#39;: 5, &#39;learning_rate&#39;: Real: lower=0.0001, upper=0.01, &#39;activation&#39;: Categorical[&#39;relu&#39;, &#39;softrelu&#39;, &#39;tanh&#39;], &#39;dropout_prob&#39;: Real: lower=0.0, upper=0.5},
}
Fitting 4 L1 models ...
Hyperparameter tuning model: LightGBMXT ... Tuning model for up to 134.96s of the 599.8s of remaining time.
</code></pre>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb56"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;564d5009d6d8436f8028d68b915fc524&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output stream stderr">
<pre><code>Fitted model: LightGBMXT/T1 ...
	-78.818	 = Validation score   (-root_mean_squared_error)
	1.64s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T2 ...
	-46.2068	 = Validation score   (-root_mean_squared_error)
	3.94s	 = Training   runtime
	0.02s	 = Validation runtime
Fitted model: LightGBMXT/T3 ...
	-47.7462	 = Validation score   (-root_mean_squared_error)
	5.9s	 = Training   runtime
	0.02s	 = Validation runtime
Fitted model: LightGBMXT/T4 ...
	-40.4993	 = Validation score   (-root_mean_squared_error)
	6.83s	 = Training   runtime
	0.02s	 = Validation runtime
Fitted model: LightGBMXT/T5 ...
	-63.01	 = Validation score   (-root_mean_squared_error)
	4.54s	 = Training   runtime
	0.02s	 = Validation runtime
Fitted model: LightGBMXT/T6 ...
	-105.7793	 = Validation score   (-root_mean_squared_error)
	3.43s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T7 ...
	-45.387	 = Validation score   (-root_mean_squared_error)
	3.94s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T8 ...
	-49.3335	 = Validation score   (-root_mean_squared_error)
	12.72s	 = Training   runtime
	0.03s	 = Validation runtime
Fitted model: LightGBMXT/T9 ...
	-125.1796	 = Validation score   (-root_mean_squared_error)
	1.94s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T10 ...
	-73.2246	 = Validation score   (-root_mean_squared_error)
	9.47s	 = Training   runtime
	0.02s	 = Validation runtime
Fitted model: LightGBMXT/T11 ...
	-100.4798	 = Validation score   (-root_mean_squared_error)
	9.03s	 = Training   runtime
	0.02s	 = Validation runtime
Fitted model: LightGBMXT/T12 ...
	-83.8827	 = Validation score   (-root_mean_squared_error)
	3.68s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T13 ...
	-63.1415	 = Validation score   (-root_mean_squared_error)
	3.1s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T14 ...
	-110.3516	 = Validation score   (-root_mean_squared_error)
	2.02s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T15 ...
	-93.1544	 = Validation score   (-root_mean_squared_error)
	2.15s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T16 ...
	-113.0309	 = Validation score   (-root_mean_squared_error)
	3.53s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T17 ...
	-91.5956	 = Validation score   (-root_mean_squared_error)
	5.0s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T18 ...
	-69.3348	 = Validation score   (-root_mean_squared_error)
	6.03s	 = Training   runtime
	0.02s	 = Validation runtime
Fitted model: LightGBMXT/T19 ...
	-94.4336	 = Validation score   (-root_mean_squared_error)
	2.37s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T20 ...
	-45.519	 = Validation score   (-root_mean_squared_error)
	4.56s	 = Training   runtime
	0.02s	 = Validation runtime
Hyperparameter tuning model: LightGBM ... Tuning model for up to 134.96s of the 503.19s of remaining time.
</code></pre>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb58"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;f32d35699b884fd0bf0c824e525ed610&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output stream stdout">
<pre><code>[1000]	valid_set&#39;s rmse: 36.7309
[1000]	valid_set&#39;s rmse: 36.6478
[1000]	valid_set&#39;s rmse: 37.9359
[2000]	valid_set&#39;s rmse: 36.5501
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>	Ran out of time, early stopping on iteration 2924. Best iteration is:
	[2923]	valid_set&#39;s rmse: 36.1999
	Stopping HPO to satisfy time limit...
Fitted model: LightGBM/T1 ...
	-36.5883	 = Validation score   (-root_mean_squared_error)
	19.71s	 = Training   runtime
	0.07s	 = Validation runtime
Fitted model: LightGBM/T2 ...
	-35.5328	 = Validation score   (-root_mean_squared_error)
	16.55s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBM/T3 ...
	-36.5418	 = Validation score   (-root_mean_squared_error)
	15.7s	 = Training   runtime
	0.04s	 = Validation runtime
Fitted model: LightGBM/T4 ...
	-36.1999	 = Validation score   (-root_mean_squared_error)
	77.93s	 = Training   runtime
	0.19s	 = Validation runtime
Hyperparameter tuning model: NeuralNetTorch ... Tuning model for up to 134.96s of the 372.53s of remaining time.
Warning: Exception caused NeuralNetTorch to fail during hyperparameter tuning... Skipping this model.
Traceback (most recent call last):
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py&quot;, line 2237, in _train_single_full
    hpo_models, hpo_results = model.hyperparameter_tune(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 1474, in hyperparameter_tune
    return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 1537, in _hyperparameter_tune
    hpo_executor.execute(
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/hpo/executors.py&quot;, line 408, in execute
    analysis = run(
               ^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/hpo/ray_hpo.py&quot;, line 267, in run
    tuner = tune.Tuner(
            ^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/ray/tune/tuner.py&quot;, line 166, in __init__
    self._local_tuner = TunerInternal(**kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/ray/tune/impl/tuner_internal.py&quot;, line 139, in __init__
    storage = StorageContext(
              ^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/ray/train/_internal/storage.py&quot;, line 441, in __init__
    self.storage_filesystem, self.storage_fs_path = get_fs_and_path(
                                                    ^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/ray/train/_internal/storage.py&quot;, line 306, in get_fs_and_path
    return pyarrow.fs.FileSystem.from_uri(storage_path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;pyarrow/_fs.pyx&quot;, line 471, in pyarrow._fs.FileSystem.from_uri
  File &quot;pyarrow/error.pxi&quot;, line 154, in pyarrow.lib.pyarrow_internal_check_status
  File &quot;pyarrow/error.pxi&quot;, line 91, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: URI has empty scheme: &#39;AutogluonModels/ag-20240428_184432/models&#39;
URI has empty scheme: &#39;AutogluonModels/ag-20240428_184432/models&#39;
Fitting model: LightGBMLarge ... Training model for up to 134.96s of the 371.7s of remaining time.
	-34.8778	 = Validation score   (-root_mean_squared_error)
	19.64s	 = Training   runtime
	0.03s	 = Validation runtime
Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 351.21s of remaining time.
	Ensemble Weights: {&#39;LightGBMLarge&#39;: 0.583, &#39;LightGBM/T2&#39;: 0.167, &#39;LightGBM/T1&#39;: 0.125, &#39;LightGBM/T3&#39;: 0.125}
	-34.5569	 = Validation score   (-root_mean_squared_error)
	0.0s	 = Training   runtime
	0.0s	 = Validation runtime
AutoGluon training complete, total runtime = 248.81s ... Best model: &quot;WeightedEnsemble_L2&quot;
Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`
Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...
	Models trained in this way will have the suffix &quot;_FULL&quot; and have NaN validation score.
	This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.
	To learn more, refer to the `.refit_full` method docstring which explains how &quot;_FULL&quot; models differ from normal models.
Fitting 1 L1 models ...
Fitting model: LightGBM/T1_FULL ...
	8.25s	 = Training   runtime
Fitting 1 L1 models ...
Fitting model: LightGBM/T2_FULL ...
	5.5s	 = Training   runtime
Fitting 1 L1 models ...
Fitting model: LightGBM/T3_FULL ...
	7.44s	 = Training   runtime
Fitting 1 L1 models ...
Fitting model: LightGBMLarge_FULL ...
	15.93s	 = Training   runtime
Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...
	Ensemble Weights: {&#39;LightGBMLarge&#39;: 0.583, &#39;LightGBM/T2&#39;: 0.167, &#39;LightGBM/T1&#39;: 0.125, &#39;LightGBM/T3&#39;: 0.125}
	0.0s	 = Training   runtime
Refit complete, total runtime = 37.65s ... Best model: &quot;WeightedEnsemble_L2&quot;
Deleting model LightGBMXT/T1. All files under AutogluonModels/ag-20240428_184432/models/LightGBMXT/T1 will be removed.
Deleting model LightGBMXT/T2. All files under AutogluonModels/ag-20240428_184432/models/LightGBMXT/T2 will be removed.
Deleting model LightGBMXT/T3. All files under AutogluonModels/ag-20240428_184432/models/LightGBMXT/T3 will be removed.
Deleting model LightGBMXT/T4. All files under AutogluonModels/ag-20240428_184432/models/LightGBMXT/T4 will be removed.
Deleting model LightGBMXT/T5. All files under AutogluonModels/ag-20240428_184432/models/LightGBMXT/T5 will be removed.
Deleting model LightGBMXT/T6. All files under AutogluonModels/ag-20240428_184432/models/LightGBMXT/T6 will be removed.
Deleting model LightGBMXT/T7. All files under AutogluonModels/ag-20240428_184432/models/LightGBMXT/T7 will be removed.
Deleting model LightGBMXT/T8. All files under AutogluonModels/ag-20240428_184432/models/LightGBMXT/T8 will be removed.
Deleting model LightGBMXT/T9. All files under AutogluonModels/ag-20240428_184432/models/LightGBMXT/T9 will be removed.
Deleting model LightGBMXT/T10. All files under AutogluonModels/ag-20240428_184432/models/LightGBMXT/T10 will be removed.
Deleting model LightGBMXT/T11. All files under AutogluonModels/ag-20240428_184432/models/LightGBMXT/T11 will be removed.
Deleting model LightGBMXT/T12. All files under AutogluonModels/ag-20240428_184432/models/LightGBMXT/T12 will be removed.
Deleting model LightGBMXT/T13. All files under AutogluonModels/ag-20240428_184432/models/LightGBMXT/T13 will be removed.
Deleting model LightGBMXT/T14. All files under AutogluonModels/ag-20240428_184432/models/LightGBMXT/T14 will be removed.
Deleting model LightGBMXT/T15. All files under AutogluonModels/ag-20240428_184432/models/LightGBMXT/T15 will be removed.
Deleting model LightGBMXT/T16. All files under AutogluonModels/ag-20240428_184432/models/LightGBMXT/T16 will be removed.
Deleting model LightGBMXT/T17. All files under AutogluonModels/ag-20240428_184432/models/LightGBMXT/T17 will be removed.
Deleting model LightGBMXT/T18. All files under AutogluonModels/ag-20240428_184432/models/LightGBMXT/T18 will be removed.
Deleting model LightGBMXT/T19. All files under AutogluonModels/ag-20240428_184432/models/LightGBMXT/T19 will be removed.
Deleting model LightGBMXT/T20. All files under AutogluonModels/ag-20240428_184432/models/LightGBMXT/T20 will be removed.
Deleting model LightGBM/T4. All files under AutogluonModels/ag-20240428_184432/models/LightGBM/T4 will be removed.
Deleting model LightGBM/T1_FULL. All files under AutogluonModels/ag-20240428_184432/models/LightGBM/T1_FULL will be removed.
Deleting model LightGBM/T2_FULL. All files under AutogluonModels/ag-20240428_184432/models/LightGBM/T2_FULL will be removed.
Deleting model LightGBM/T3_FULL. All files under AutogluonModels/ag-20240428_184432/models/LightGBM/T3_FULL will be removed.
Deleting model LightGBMLarge_FULL. All files under AutogluonModels/ag-20240428_184432/models/LightGBMLarge_FULL will be removed.
Deleting model WeightedEnsemble_L2_FULL. All files under AutogluonModels/ag-20240428_184432/models/WeightedEnsemble_L2_FULL will be removed.
TabularPredictor saved. To load, use: predictor = TabularPredictor.load(&quot;AutogluonModels/ag-20240428_184432&quot;)
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="80">
<div class="sourceCode" id="cb61"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>predictor_new_hpo <span class="op">=</span> TabularPredictor(label<span class="op">=</span><span class="st">&#39;count&#39;</span>, problem_type<span class="op">=</span><span class="st">&#39;regression&#39;</span>, eval_metric<span class="op">=</span><span class="st">&#39;root_mean_squared_error&#39;</span>,</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>                                    learner_kwargs<span class="op">=</span>{<span class="st">&#39;ignored_columns&#39;</span>: ignored_columns}).fit(</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>                                                                         train_data<span class="op">=</span>train, </span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>                                                                         time_limit<span class="op">=</span>time_limit,</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>                                                                         presets<span class="op">=</span>presets, </span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>                                                                         hyperparameters<span class="op">=</span>hyperparameters, </span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>                                                                         hyperparameter_tune_kwargs<span class="op">=</span>hyperparameter_tune_kwargs,</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>                                                                         refit_full<span class="op">=</span><span class="st">&#39;best&#39;</span>)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>No path specified. Models will be saved in: &quot;AutogluonModels/ag-20240428_184919&quot;
Presets specified: [&#39;optimize_for_deployment&#39;]
Warning: hyperparameter tuning is currently experimental and may cause the process to hang.
Beginning AutoGluon training ... Time limit = 600s
AutoGluon will save models to &quot;AutogluonModels/ag-20240428_184919&quot;
=================== System Info ===================
AutoGluon Version:  1.1.0
Python Version:     3.11.5
Operating System:   Darwin
Platform Machine:   arm64
Platform Version:   Darwin Kernel Version 23.3.0: Wed Dec 20 21:30:44 PST 2023; root:xnu-10002.81.5~7/RELEASE_ARM64_T6000
CPU Count:          8
Memory Avail:       4.80 GB / 16.00 GB (30.0%)
Disk Space Avail:   43.64 GB / 460.43 GB (9.5%)
===================================================
Train Data Rows:    10886
Train Data Columns: 15
Label Column:       count
Problem Type:       regression
Preprocessing data ...
Using Feature Generators to preprocess the data ...
Dropping user-specified ignored columns: [&#39;casual&#39;, &#39;registered&#39;]
Fitting AutoMLPipelineFeatureGenerator...
	Available Memory:                    4884.19 MB
	Train Data (Original)  Memory Usage: 0.77 MB (0.0% of available memory)
	Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.
	Stage 1 Generators:
		Fitting AsTypeFeatureGenerator...
			Note: Converting 3 features to boolean dtype as they only contain 2 unique values.
	Stage 2 Generators:
		Fitting FillNaFeatureGenerator...
	Stage 3 Generators:
		Fitting IdentityFeatureGenerator...
		Fitting CategoryFeatureGenerator...
			Fitting CategoryMemoryMinimizeFeatureGenerator...
		Fitting DatetimeFeatureGenerator...
	Stage 4 Generators:
		Fitting DropUniqueFeatureGenerator...
	Stage 5 Generators:
		Fitting DropDuplicatesFeatureGenerator...
	Types of features in original data (raw dtype, special dtypes):
		(&#39;category&#39;, []) : 2 | [&#39;season&#39;, &#39;weather&#39;]
		(&#39;datetime&#39;, []) : 1 | [&#39;datetime&#39;]
		(&#39;float&#39;, [])    : 3 | [&#39;temp&#39;, &#39;atemp&#39;, &#39;windspeed&#39;]
		(&#39;int&#39;, [])      : 7 | [&#39;holiday&#39;, &#39;workingday&#39;, &#39;humidity&#39;, &#39;year&#39;, &#39;month&#39;, ...]
	Types of features in processed data (raw dtype, special dtypes):
		(&#39;category&#39;, [])             : 2 | [&#39;season&#39;, &#39;weather&#39;]
		(&#39;float&#39;, [])                : 3 | [&#39;temp&#39;, &#39;atemp&#39;, &#39;windspeed&#39;]
		(&#39;int&#39;, [])                  : 4 | [&#39;humidity&#39;, &#39;month&#39;, &#39;day&#39;, &#39;hour&#39;]
		(&#39;int&#39;, [&#39;bool&#39;])            : 3 | [&#39;holiday&#39;, &#39;workingday&#39;, &#39;year&#39;]
		(&#39;int&#39;, [&#39;datetime_as_int&#39;]) : 3 | [&#39;datetime&#39;, &#39;datetime.year&#39;, &#39;datetime.dayofweek&#39;]
	0.2s = Fit runtime
	13 features in original data used to generate 15 features in processed data.
	Train Data (Processed) Memory Usage: 0.76 MB (0.0% of available memory)
Data preprocessing and feature engineering runtime = 0.19s ...
AutoGluon will gauge predictive performance using evaluation metric: &#39;root_mean_squared_error&#39;
	This metric&#39;s sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
	To change this, specify the eval_metric parameter of Predictor()
Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 8708, Val Rows: 2178
User-specified model hyperparameters to be fit:
{
	&#39;GBM&#39;: [{&#39;extra_trees&#39;: True, &#39;num_boost_round&#39;: Int: lower=100, upper=500, &#39;num_leaves&#39;: Int: lower=25, upper=64, &#39;ag_args&#39;: {&#39;name_suffix&#39;: &#39;XT&#39;}}, {}, &#39;GBMLarge&#39;],
	&#39;NN_TORCH&#39;: {&#39;num_epochs&#39;: 5, &#39;learning_rate&#39;: Real: lower=0.0001, upper=0.01, &#39;activation&#39;: Categorical[&#39;relu&#39;, &#39;softrelu&#39;, &#39;tanh&#39;], &#39;dropout_prob&#39;: Real: lower=0.0, upper=0.5},
}
Fitting 4 L1 models ...
Hyperparameter tuning model: LightGBMXT ... Tuning model for up to 134.96s of the 599.81s of remaining time.
</code></pre>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb63"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;df9d9953b7f04209a38858aee3b00e90&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output stream stderr">
<pre><code>Fitted model: LightGBMXT/T1 ...
	-78.818	 = Validation score   (-root_mean_squared_error)
	0.85s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T2 ...
	-46.2068	 = Validation score   (-root_mean_squared_error)
	2.04s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T3 ...
	-47.7462	 = Validation score   (-root_mean_squared_error)
	3.49s	 = Training   runtime
	0.02s	 = Validation runtime
Fitted model: LightGBMXT/T4 ...
	-40.4993	 = Validation score   (-root_mean_squared_error)
	3.81s	 = Training   runtime
	0.02s	 = Validation runtime
Fitted model: LightGBMXT/T5 ...
	-63.01	 = Validation score   (-root_mean_squared_error)
	2.59s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T6 ...
	-105.7793	 = Validation score   (-root_mean_squared_error)
	2.04s	 = Training   runtime
	0.02s	 = Validation runtime
Fitted model: LightGBMXT/T7 ...
	-45.387	 = Validation score   (-root_mean_squared_error)
	1.95s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T8 ...
	-49.3335	 = Validation score   (-root_mean_squared_error)
	5.76s	 = Training   runtime
	0.03s	 = Validation runtime
Fitted model: LightGBMXT/T9 ...
	-125.1796	 = Validation score   (-root_mean_squared_error)
	0.97s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T10 ...
	-73.2246	 = Validation score   (-root_mean_squared_error)
	4.05s	 = Training   runtime
	0.02s	 = Validation runtime
Fitted model: LightGBMXT/T11 ...
	-100.4798	 = Validation score   (-root_mean_squared_error)
	4.95s	 = Training   runtime
	0.03s	 = Validation runtime
Fitted model: LightGBMXT/T12 ...
	-83.8827	 = Validation score   (-root_mean_squared_error)
	2.23s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T13 ...
	-63.1415	 = Validation score   (-root_mean_squared_error)
	1.44s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T14 ...
	-110.3516	 = Validation score   (-root_mean_squared_error)
	1.1s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T15 ...
	-93.1544	 = Validation score   (-root_mean_squared_error)
	1.38s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T16 ...
	-113.0309	 = Validation score   (-root_mean_squared_error)
	1.52s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T17 ...
	-91.5956	 = Validation score   (-root_mean_squared_error)
	2.52s	 = Training   runtime
	0.02s	 = Validation runtime
Fitted model: LightGBMXT/T18 ...
	-69.3348	 = Validation score   (-root_mean_squared_error)
	2.52s	 = Training   runtime
	0.02s	 = Validation runtime
Fitted model: LightGBMXT/T19 ...
	-94.4336	 = Validation score   (-root_mean_squared_error)
	1.07s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBMXT/T20 ...
	-45.519	 = Validation score   (-root_mean_squared_error)
	2.51s	 = Training   runtime
	0.02s	 = Validation runtime
Hyperparameter tuning model: LightGBM ... Tuning model for up to 134.96s of the 550.24s of remaining time.
</code></pre>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb65"><pre
class="sourceCode json"><code class="sourceCode json"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;cb8fd56c713745c981947c0d3686fe3c&quot;</span><span class="fu">,</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output stream stdout">
<pre><code>[1000]	valid_set&#39;s rmse: 36.7309
[1000]	valid_set&#39;s rmse: 36.6478
[1000]	valid_set&#39;s rmse: 37.9359
[2000]	valid_set&#39;s rmse: 36.5501
[3000]	valid_set&#39;s rmse: 36.1871
[4000]	valid_set&#39;s rmse: 36.0677
[5000]	valid_set&#39;s rmse: 36.0518
[6000]	valid_set&#39;s rmse: 36.0917
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>	Ran out of time, early stopping on iteration 918. Best iteration is:
	[915]	valid_set&#39;s rmse: 35.5783
	Stopping HPO to satisfy time limit...
Fitted model: LightGBM/T1 ...
	-36.5883	 = Validation score   (-root_mean_squared_error)
	10.26s	 = Training   runtime
	0.04s	 = Validation runtime
Fitted model: LightGBM/T2 ...
	-35.5328	 = Validation score   (-root_mean_squared_error)
	9.91s	 = Training   runtime
	0.01s	 = Validation runtime
Fitted model: LightGBM/T3 ...
	-36.5418	 = Validation score   (-root_mean_squared_error)
	10.01s	 = Training   runtime
	0.04s	 = Validation runtime
Fitted model: LightGBM/T4 ...
	-36.0477	 = Validation score   (-root_mean_squared_error)
	73.98s	 = Training   runtime
	0.46s	 = Validation runtime
Fitted model: LightGBM/T5 ...
	-36.3146	 = Validation score   (-root_mean_squared_error)
	8.04s	 = Training   runtime
	0.03s	 = Validation runtime
Fitted model: LightGBM/T6 ...
	-35.5783	 = Validation score   (-root_mean_squared_error)
	16.78s	 = Training   runtime
	0.06s	 = Validation runtime
Hyperparameter tuning model: NeuralNetTorch ... Tuning model for up to 134.96s of the 419.9s of remaining time.
Warning: Exception caused NeuralNetTorch to fail during hyperparameter tuning... Skipping this model.
Traceback (most recent call last):
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py&quot;, line 2237, in _train_single_full
    hpo_models, hpo_results = model.hyperparameter_tune(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 1474, in hyperparameter_tune
    return self._hyperparameter_tune(hpo_executor=hpo_executor, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py&quot;, line 1537, in _hyperparameter_tune
    hpo_executor.execute(
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/hpo/executors.py&quot;, line 408, in execute
    analysis = run(
               ^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/autogluon/core/hpo/ray_hpo.py&quot;, line 267, in run
    tuner = tune.Tuner(
            ^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/ray/tune/tuner.py&quot;, line 166, in __init__
    self._local_tuner = TunerInternal(**kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/ray/tune/impl/tuner_internal.py&quot;, line 139, in __init__
    storage = StorageContext(
              ^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/ray/train/_internal/storage.py&quot;, line 441, in __init__
    self.storage_filesystem, self.storage_fs_path = get_fs_and_path(
                                                    ^^^^^^^^^^^^^^^^
  File &quot;/opt/homebrew/lib/python3.11/site-packages/ray/train/_internal/storage.py&quot;, line 306, in get_fs_and_path
    return pyarrow.fs.FileSystem.from_uri(storage_path)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;pyarrow/_fs.pyx&quot;, line 471, in pyarrow._fs.FileSystem.from_uri
  File &quot;pyarrow/error.pxi&quot;, line 154, in pyarrow.lib.pyarrow_internal_check_status
  File &quot;pyarrow/error.pxi&quot;, line 91, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: URI has empty scheme: &#39;AutogluonModels/ag-20240428_184919/models&#39;
URI has empty scheme: &#39;AutogluonModels/ag-20240428_184919/models&#39;
Fitting model: LightGBMLarge ... Training model for up to 134.96s of the 419.88s of remaining time.
	-34.8778	 = Validation score   (-root_mean_squared_error)
	20.13s	 = Training   runtime
	0.03s	 = Validation runtime
Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 398.59s of remaining time.
	Ensemble Weights: {&#39;LightGBMLarge&#39;: 0.565, &#39;LightGBM/T5&#39;: 0.217, &#39;LightGBM/T2&#39;: 0.13, &#39;LightGBM/T6&#39;: 0.087}
	-34.5097	 = Validation score   (-root_mean_squared_error)
	0.0s	 = Training   runtime
	0.0s	 = Validation runtime
AutoGluon training complete, total runtime = 201.43s ... Best model: &quot;WeightedEnsemble_L2&quot;
Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`
Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...
	Models trained in this way will have the suffix &quot;_FULL&quot; and have NaN validation score.
	This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.
	To learn more, refer to the `.refit_full` method docstring which explains how &quot;_FULL&quot; models differ from normal models.
Fitting 1 L1 models ...
Fitting model: LightGBM/T2_FULL ...
	5.67s	 = Training   runtime
Fitting 1 L1 models ...
Fitting model: LightGBM/T5_FULL ...
	4.88s	 = Training   runtime
Fitting 1 L1 models ...
Fitting model: LightGBM/T6_FULL ...
	16.27s	 = Training   runtime
Fitting 1 L1 models ...
Fitting model: LightGBMLarge_FULL ...
	14.03s	 = Training   runtime
Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...
	Ensemble Weights: {&#39;LightGBMLarge&#39;: 0.565, &#39;LightGBM/T5&#39;: 0.217, &#39;LightGBM/T2&#39;: 0.13, &#39;LightGBM/T6&#39;: 0.087}
	0.0s	 = Training   runtime
Refit complete, total runtime = 41.38s ... Best model: &quot;WeightedEnsemble_L2&quot;
Deleting model LightGBMXT/T1. All files under AutogluonModels/ag-20240428_184919/models/LightGBMXT/T1 will be removed.
Deleting model LightGBMXT/T2. All files under AutogluonModels/ag-20240428_184919/models/LightGBMXT/T2 will be removed.
Deleting model LightGBMXT/T3. All files under AutogluonModels/ag-20240428_184919/models/LightGBMXT/T3 will be removed.
Deleting model LightGBMXT/T4. All files under AutogluonModels/ag-20240428_184919/models/LightGBMXT/T4 will be removed.
Deleting model LightGBMXT/T5. All files under AutogluonModels/ag-20240428_184919/models/LightGBMXT/T5 will be removed.
Deleting model LightGBMXT/T6. All files under AutogluonModels/ag-20240428_184919/models/LightGBMXT/T6 will be removed.
Deleting model LightGBMXT/T7. All files under AutogluonModels/ag-20240428_184919/models/LightGBMXT/T7 will be removed.
Deleting model LightGBMXT/T8. All files under AutogluonModels/ag-20240428_184919/models/LightGBMXT/T8 will be removed.
Deleting model LightGBMXT/T9. All files under AutogluonModels/ag-20240428_184919/models/LightGBMXT/T9 will be removed.
Deleting model LightGBMXT/T10. All files under AutogluonModels/ag-20240428_184919/models/LightGBMXT/T10 will be removed.
Deleting model LightGBMXT/T11. All files under AutogluonModels/ag-20240428_184919/models/LightGBMXT/T11 will be removed.
Deleting model LightGBMXT/T12. All files under AutogluonModels/ag-20240428_184919/models/LightGBMXT/T12 will be removed.
Deleting model LightGBMXT/T13. All files under AutogluonModels/ag-20240428_184919/models/LightGBMXT/T13 will be removed.
Deleting model LightGBMXT/T14. All files under AutogluonModels/ag-20240428_184919/models/LightGBMXT/T14 will be removed.
Deleting model LightGBMXT/T15. All files under AutogluonModels/ag-20240428_184919/models/LightGBMXT/T15 will be removed.
Deleting model LightGBMXT/T16. All files under AutogluonModels/ag-20240428_184919/models/LightGBMXT/T16 will be removed.
Deleting model LightGBMXT/T17. All files under AutogluonModels/ag-20240428_184919/models/LightGBMXT/T17 will be removed.
Deleting model LightGBMXT/T18. All files under AutogluonModels/ag-20240428_184919/models/LightGBMXT/T18 will be removed.
Deleting model LightGBMXT/T19. All files under AutogluonModels/ag-20240428_184919/models/LightGBMXT/T19 will be removed.
Deleting model LightGBMXT/T20. All files under AutogluonModels/ag-20240428_184919/models/LightGBMXT/T20 will be removed.
Deleting model LightGBM/T1. All files under AutogluonModels/ag-20240428_184919/models/LightGBM/T1 will be removed.
Deleting model LightGBM/T3. All files under AutogluonModels/ag-20240428_184919/models/LightGBM/T3 will be removed.
Deleting model LightGBM/T4. All files under AutogluonModels/ag-20240428_184919/models/LightGBM/T4 will be removed.
Deleting model LightGBM/T2_FULL. All files under AutogluonModels/ag-20240428_184919/models/LightGBM/T2_FULL will be removed.
Deleting model LightGBM/T5_FULL. All files under AutogluonModels/ag-20240428_184919/models/LightGBM/T5_FULL will be removed.
Deleting model LightGBM/T6_FULL. All files under AutogluonModels/ag-20240428_184919/models/LightGBM/T6_FULL will be removed.
Deleting model LightGBMLarge_FULL. All files under AutogluonModels/ag-20240428_184919/models/LightGBMLarge_FULL will be removed.
Deleting model WeightedEnsemble_L2_FULL. All files under AutogluonModels/ag-20240428_184919/models/WeightedEnsemble_L2_FULL will be removed.
TabularPredictor saved. To load, use: predictor = TabularPredictor.load(&quot;AutogluonModels/ag-20240428_184919&quot;)
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="81">
<div class="sourceCode" id="cb68"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>predictor_new_hpo.fit_summary()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>*** Summary of fit() ***
Estimated performance of each model:
                 model  score_val              eval_metric  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order
0  WeightedEnsemble_L2 -34.509698  root_mean_squared_error       0.125621  54.858335                0.000184           0.004449            2       True          5
1        LightGBMLarge -34.877774  root_mean_squared_error       0.027223  20.126667                0.027223          20.126667            1       True          4
2          LightGBM/T2 -35.532811  root_mean_squared_error       0.014142   9.905786                0.014142           9.905786            1       True          1
3          LightGBM/T6 -35.578290  root_mean_squared_error       0.055792  16.779174                0.055792          16.779174            1       True          3
4          LightGBM/T5 -36.314643  root_mean_squared_error       0.028280   8.042259                0.028280           8.042259            1       True          2
Number of models trained: 5
Types of models trained:
{&#39;WeightedEnsembleModel&#39;, &#39;LGBModel&#39;}
Bagging used: False 
Multi-layer stack-ensembling used: False 
Feature Metadata (Processed):
(raw dtype, special dtypes):
(&#39;category&#39;, [])             : 2 | [&#39;season&#39;, &#39;weather&#39;]
(&#39;float&#39;, [])                : 3 | [&#39;temp&#39;, &#39;atemp&#39;, &#39;windspeed&#39;]
(&#39;int&#39;, [])                  : 4 | [&#39;humidity&#39;, &#39;month&#39;, &#39;day&#39;, &#39;hour&#39;]
(&#39;int&#39;, [&#39;bool&#39;])            : 3 | [&#39;holiday&#39;, &#39;workingday&#39;, &#39;year&#39;]
(&#39;int&#39;, [&#39;datetime_as_int&#39;]) : 3 | [&#39;datetime&#39;, &#39;datetime.year&#39;, &#39;datetime.dayofweek&#39;]
*** End of fit() summary ***
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>/opt/homebrew/lib/python3.11/site-packages/autogluon/core/utils/plots.py:169: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: &quot;pip install bokeh==2.0.1&quot;
  warnings.warn(&#39;AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: &quot;pip install bokeh==2.0.1&quot;&#39;)
</code></pre>
</div>
<div class="output execute_result" data-execution_count="81">
<pre><code>{&#39;model_types&#39;: {&#39;LightGBM/T2&#39;: &#39;LGBModel&#39;,
  &#39;LightGBM/T5&#39;: &#39;LGBModel&#39;,
  &#39;LightGBM/T6&#39;: &#39;LGBModel&#39;,
  &#39;LightGBMLarge&#39;: &#39;LGBModel&#39;,
  &#39;WeightedEnsemble_L2&#39;: &#39;WeightedEnsembleModel&#39;},
 &#39;model_performance&#39;: {&#39;LightGBM/T2&#39;: -35.532811026722946,
  &#39;LightGBM/T5&#39;: -36.3146433554767,
  &#39;LightGBM/T6&#39;: -35.578290310120636,
  &#39;LightGBMLarge&#39;: -34.877773655726564,
  &#39;WeightedEnsemble_L2&#39;: -34.509698229854536},
 &#39;model_best&#39;: &#39;WeightedEnsemble_L2&#39;,
 &#39;model_paths&#39;: {&#39;LightGBM/T2&#39;: [&#39;LightGBM&#39;, &#39;T2&#39;],
  &#39;LightGBM/T5&#39;: [&#39;LightGBM&#39;, &#39;T5&#39;],
  &#39;LightGBM/T6&#39;: [&#39;LightGBM&#39;, &#39;T6&#39;],
  &#39;LightGBMLarge&#39;: [&#39;LightGBMLarge&#39;],
  &#39;WeightedEnsemble_L2&#39;: [&#39;WeightedEnsemble_L2&#39;]},
 &#39;model_fit_times&#39;: {&#39;LightGBM/T2&#39;: 9.90578579902649,
  &#39;LightGBM/T5&#39;: 8.042258977890015,
  &#39;LightGBM/T6&#39;: 16.779173851013184,
  &#39;LightGBMLarge&#39;: 20.126667022705078,
  &#39;WeightedEnsemble_L2&#39;: 0.004449129104614258},
 &#39;model_pred_times&#39;: {&#39;LightGBM/T2&#39;: 0.014142036437988281,
  &#39;LightGBM/T5&#39;: 0.028280019760131836,
  &#39;LightGBM/T6&#39;: 0.05579209327697754,
  &#39;LightGBMLarge&#39;: 0.027222871780395508,
  &#39;WeightedEnsemble_L2&#39;: 0.00018405914306640625},
 &#39;num_bag_folds&#39;: 0,
 &#39;max_stack_level&#39;: 2,
 &#39;model_hyperparams&#39;: {&#39;LightGBM/T2&#39;: {&#39;learning_rate&#39;: 0.06994332504138302,
   &#39;feature_fraction&#39;: 0.8872033759818312,
   &#39;min_data_in_leaf&#39;: 5,
   &#39;num_leaves&#39;: 83},
  &#39;LightGBM/T5&#39;: {&#39;learning_rate&#39;: 0.1000260297171191,
   &#39;feature_fraction&#39;: 0.8694162793303375,
   &#39;min_data_in_leaf&#39;: 48,
   &#39;num_leaves&#39;: 53},
  &#39;LightGBM/T6&#39;: {&#39;learning_rate&#39;: 0.017357968430552234,
   &#39;feature_fraction&#39;: 0.9590196908843444,
   &#39;min_data_in_leaf&#39;: 22,
   &#39;num_leaves&#39;: 96},
  &#39;LightGBMLarge&#39;: {&#39;learning_rate&#39;: 0.03,
   &#39;num_leaves&#39;: 128,
   &#39;feature_fraction&#39;: 0.9,
   &#39;min_data_in_leaf&#39;: 5},
  &#39;WeightedEnsemble_L2&#39;: {&#39;use_orig_features&#39;: False,
   &#39;max_base_models&#39;: 25,
   &#39;max_base_models_per_type&#39;: 5,
   &#39;save_bag_folds&#39;: True}},
 &#39;leaderboard&#39;:                  model  score_val              eval_metric  pred_time_val  \
 0  WeightedEnsemble_L2 -34.509698  root_mean_squared_error       0.125621   
 1        LightGBMLarge -34.877774  root_mean_squared_error       0.027223   
 2          LightGBM/T2 -35.532811  root_mean_squared_error       0.014142   
 3          LightGBM/T6 -35.578290  root_mean_squared_error       0.055792   
 4          LightGBM/T5 -36.314643  root_mean_squared_error       0.028280   
 
     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  \
 0  54.858335                0.000184           0.004449            2   
 1  20.126667                0.027223          20.126667            1   
 2   9.905786                0.014142           9.905786            1   
 3  16.779174                0.055792          16.779174            1   
 4   8.042259                0.028280           8.042259            1   
 
    can_infer  fit_order  
 0       True          5  
 1       True          4  
 2       True          1  
 3       True          3  
 4       True          2  }</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="82">
<div class="sourceCode" id="cb72"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>predictions_new_hpo <span class="op">=</span> predictor_new_hpo.predict(test)</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>predictions_new_hpo.head()  <span class="co"># Display the first few rows of the predictions</span></span></code></pre></div>
<div class="output execute_result" data-execution_count="82">
<pre><code>0    11.424698
1     0.810113
2    -2.084970
3     2.369029
4     2.268670
Name: count, dtype: float32</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="83">
<div class="sourceCode" id="cb74"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># describe the predictions</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>predictions_new_hpo.describe()</span></code></pre></div>
<div class="output execute_result" data-execution_count="83">
<pre><code>count    6493.000000
mean      194.114304
std       174.989914
min        -6.561350
25%        48.320908
50%       152.738434
75%       286.819977
max       908.924866
Name: count, dtype: float64</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="85">
<div class="sourceCode" id="cb76"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Remember to set all negative values to zero</span></span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>predictions_new_hpo[predictions_new_hpo <span class="op">&lt;</span> <span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>pred_pos_count <span class="op">=</span> (predictions_new_hpo <span class="op">&lt;</span> <span class="dv">0</span>).<span class="bu">sum</span>()</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>pred_neg_count <span class="op">=</span> (predictions_new_hpo <span class="op">==</span> <span class="dv">1</span>).<span class="bu">sum</span>()</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
</div>
<div class="cell code" data-execution_count="86">
<div class="sourceCode" id="cb77"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Same thing as train and test dataset</span></span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>submission_new_hpo <span class="op">=</span> pd.read_csv(<span class="st">&#39;sampleSubmission.csv&#39;</span>, parse_dates <span class="op">=</span> [<span class="st">&#39;datetime&#39;</span>])</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a>submission_new_hpo.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="86">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>datetime</th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2011-01-20 00:00:00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2011-01-20 01:00:00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2011-01-20 02:00:00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2011-01-20 03:00:00</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2011-01-20 04:00:00</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell code" data-execution_count="87">
<div class="sourceCode" id="cb78"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Same submitting predictions</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>submission_new_hpo[<span class="st">&quot;count&quot;</span>] <span class="op">=</span> predictions_new_hpo</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>submission_new_hpo.to_csv(<span class="st">&quot;submission_new_hpo.csv&quot;</span>, index<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="88">
<div class="sourceCode" id="cb79"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>kaggle competitions submit <span class="op">-</span>c bike<span class="op">-</span>sharing<span class="op">-</span>demand <span class="op">-</span>f submission_new_hpo.csv <span class="op">-</span>m <span class="st">&quot;new features with hyperparameters&quot;</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>100%|█████████████████████████████████████████| 188k/188k [00:00&lt;00:00, 211kB/s]
Successfully submitted to Bike Sharing Demand</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="89">
<div class="sourceCode" id="cb81"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>kaggle competitions submissions <span class="op">-</span>c bike<span class="op">-</span>sharing<span class="op">-</span>demand <span class="op">|</span> tail <span class="op">-</span>n <span class="op">+</span><span class="dv">1</span> <span class="op">|</span> head <span class="op">-</span>n <span class="dv">6</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>fileName                     date                 description                        status    publicScore  privateScore  
---------------------------  -------------------  ---------------------------------  --------  -----------  ------------  
submission_new_hpo.csv       2024-04-28 18:57:37  new features with hyperparameters  complete  0.52604      0.52604       
submission_new_features.csv  2024-04-28 18:23:06  new features                       complete  0.85666      0.85666       
submission.csv               2024-04-28 18:04:20  first raw submission               complete  1.83131      1.83131       
</code></pre>
</div>
</div>
<section id="new-score-of-052604" class="cell markdown">
<h4>New Score of <code>0.52604</code></h4>
</section>
<section id="step-7-write-a-report" class="cell markdown">
<h2>Step 7: Write a Report</h2>
<h3 id="refer-to-the-markdown-file-for-the-full-report">Refer to the
markdown file for the full report</h3>
<h3 id="creating-plots-and-table-for-report">Creating plots and table
for report</h3>
</section>
<div class="cell code" data-execution_count="90"
data-jupyter="{&quot;source_hidden&quot;:true}">
<div class="sourceCode" id="cb83"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Taking the top model score from each training run and creating a line plot to show improvement</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="co"># You can create these in the notebook and save them to PNG or use some other tool (e.g. google sheets, excel)</span></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> pd.DataFrame(</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;model&quot;</span>: [<span class="st">&quot;initial&quot;</span>, <span class="st">&quot;add_features&quot;</span>, <span class="st">&quot;hpo&quot;</span>],</span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;score&quot;</span>: [<span class="fl">50.051484</span>, <span class="fl">28.163684</span>, <span class="fl">34.509698</span>]</span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a>).plot(x<span class="op">=</span><span class="st">&quot;model&quot;</span>, y<span class="op">=</span><span class="st">&quot;score&quot;</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>)).get_figure()</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>fig.savefig(<span class="st">&#39;model_train_score.png&#39;</span>)</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_0be6c1bfb8f540fb92135113ec53c6ab/2ddf09dffc0db88637fadc341f17a8eec4890ed6.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="91">
<div class="sourceCode" id="cb84"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Take the 3 kaggle scores and creating a line plot to show improvement</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> pd.DataFrame(</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;test_eval&quot;</span>: [<span class="st">&quot;initial&quot;</span>, <span class="st">&quot;add_features&quot;</span>, <span class="st">&quot;hpo&quot;</span>],</span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;score&quot;</span>: [<span class="fl">1.83131</span>, <span class="fl">0.85666</span>, <span class="fl">0.52604</span>]</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>).plot(x<span class="op">=</span><span class="st">&quot;test_eval&quot;</span>, y<span class="op">=</span><span class="st">&quot;score&quot;</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>)).get_figure()</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>fig.savefig(<span class="st">&#39;model_test_score.png&#39;</span>)</span></code></pre></div>
<div class="output display_data">
<p><img
src="vertopal_0be6c1bfb8f540fb92135113ec53c6ab/d54fc7d83b356c8afc9f4b12922775760c3b3f3e.png" /></p>
</div>
</div>
<section id="hyperparameter-table" class="cell markdown">
<h3>Hyperparameter table</h3>
</section>
<div class="cell code" data-execution_count="92">
<div class="sourceCode" id="cb85"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The 3 hyperparameters we tuned with the kaggle score as the result</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>pd.DataFrame({</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;model&quot;</span>: [<span class="st">&quot;initial_model&quot;</span>, <span class="st">&quot;add_features_model&quot;</span>, <span class="st">&quot;hpo_model&quot;</span>],</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;hpo1&quot;</span>: [<span class="st">&#39;default_vals&#39;</span>, <span class="st">&#39;default_vals&#39;</span>, <span class="st">&#39;GBM: num_leaves: lower=26, upper=66&#39;</span>],</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;hpo2&quot;</span>: [<span class="st">&#39;default_vals&#39;</span>, <span class="st">&#39;default_vals&#39;</span>, <span class="st">&#39;NN: dropout_prob: 0.0, 0.5&#39;</span>],</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;hpo3&quot;</span>: [<span class="st">&#39;default_vals&#39;</span>, <span class="st">&#39;default_vals&#39;</span>, <span class="st">&#39;GBM: num_boost_round: 100&#39;</span>],</span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;score&quot;</span>: [<span class="fl">1.83131</span>, <span class="fl">0.85666</span>, <span class="fl">0.52604</span>]</span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true" tabindex="-1"></a>})</span></code></pre></div>
<div class="output execute_result" data-execution_count="92">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>model</th>
      <th>hpo1</th>
      <th>hpo2</th>
      <th>hpo3</th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>initial_model</td>
      <td>default_vals</td>
      <td>default_vals</td>
      <td>default_vals</td>
      <td>1.83131</td>
    </tr>
    <tr>
      <th>1</th>
      <td>add_features_model</td>
      <td>default_vals</td>
      <td>default_vals</td>
      <td>default_vals</td>
      <td>0.85666</td>
    </tr>
    <tr>
      <th>2</th>
      <td>hpo_model</td>
      <td>GBM: num_leaves: lower=26, upper=66</td>
      <td>NN: dropout_prob: 0.0, 0.5</td>
      <td>GBM: num_boost_round: 100</td>
      <td>0.52604</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell code" data-execution_count="113">
<div class="sourceCode" id="cb86"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>jupyter nbconvert <span class="op">--</span>to html project<span class="op">-</span>template.ipynb</span></code></pre></div>
<div class="output error" data-ename="SyntaxError"
data-evalue="invalid syntax (1478045972.py, line 1)">
<pre><code>  Cell In[113], line 1
    jupyter nbconvert --to html project-template.ipynb
            ^
SyntaxError: invalid syntax

</code></pre>
</div>
</div>
<section id="conclusion" class="cell markdown">
<h3>Conclusion:</h3>
<p>The top-ranked model was the (add features) model named
WeightedEnsemble_L3, with a validation RMSE score of
<strong>34.509698</strong> and the best Kaggle score of
<strong>0.52604</strong> (on test dataset).</p>
</section>
</body>
</html>
